{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"K3s Homelab GitOps Stack","text":"<p>Welcome to the documentation for my production-ready k3s homelab cluster. This documentation serves three primary purposes (in priority order):</p> <ol> <li>\ud83d\udd27 Troubleshoot and fix issues - Quick access to solutions when things break</li> <li>\ud83d\udcd6 Understand the architecture - Explain how everything is configured and why</li> <li>\ud83d\udee0\ufe0f Replicate the setup - Step-by-step guide to build a similar cluster</li> </ol>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#i-need-to-fix-something-now","title":"\ud83d\udea8 I Need to Fix Something NOW","text":"<p>Go directly to Troubleshooting &amp; Operations for:</p> <ul> <li>Common Issues &amp; Quick Fixes</li> <li>Disaster Recovery Procedures</li> <li>Certificate Problems</li> <li>Storage Issues</li> <li>Network Debugging</li> <li>Application Debugging</li> </ul>"},{"location":"#i-want-to-understand-the-setup","title":"\ud83d\udcda I Want to Understand the Setup","text":"<p>Start with Getting Started:</p> <ul> <li>Overview - Purpose, philosophy, technology stack</li> <li>Architecture - Detailed design with diagrams</li> <li>Quick Start - Step-by-step setup guide</li> </ul>"},{"location":"#i-want-to-build-this","title":"\ud83c\udfd7\ufe0f I Want to Build This","text":"<p>Follow the Quick Start Guide then:</p> <ol> <li>Hardware Setup</li> <li>OS Provisioning</li> <li>Ansible Automation</li> <li>Core Services</li> </ol>"},{"location":"#about-this-setup","title":"About This Setup","text":"<p>This is my personal homelab k3s cluster, running production-ready practices:</p> <ul> <li>\u2705 Everything as code - GitOps with ArgoCD, no manual kubectl commands</li> <li>\u2705 High availability - 3-node control plane, distributed storage</li> <li>\u2705 Security first - Valid HTTPS certificates, SSO authentication, secrets management</li> <li>\u2705 Network segmentation - Public/private VLANs with proper firewall rules</li> <li>\u2705 Disaster recovery - Automated backups, documented recovery procedures</li> <li>\u2705 Observable - Comprehensive monitoring, logging, and alerting</li> </ul> <p>Although this is a personal educational project, I maintain production standards because the biggest learnings come from handling real-world complexity and failure scenarios.</p>"},{"location":"#technology-stack-at-a-glance","title":"Technology Stack at a Glance","text":""},{"location":"#hardware","title":"Hardware","text":"<ul> <li>Control Plane: 3x Raspberry Pi 4 (8GB RAM)</li> <li>Workers: 4x Raspberry Pi 4 (8GB RAM) + 3x x86 servers (64GB RAM)</li> <li>Storage: HL15 with TrueNAS (40TB capacity)</li> </ul>"},{"location":"#core-infrastructure","title":"Core Infrastructure","text":"<ul> <li>Orchestration: k3s (lightweight Kubernetes)</li> <li>GitOps: ArgoCD</li> <li>Ingress: Traefik</li> <li>Load Balancer: MetalLB</li> <li>Storage: Longhorn (distributed block storage)</li> <li>Certificates: cert-manager + Let's Encrypt</li> <li>Secrets: HashiCorp Vault + External Secrets Operator</li> <li>Authentication: Authentik (SSO/OIDC)</li> <li>Monitoring: Prometheus + Grafana + Loki</li> <li>Database: CloudNativePG (PostgreSQL operator)</li> </ul>"},{"location":"#hardware-setup","title":"Hardware Setup","text":"<p>Current Configuration:</p> <ul> <li>3x Raspberry Pi 4 (8GB) - Control plane nodes with Corsair USB sticks</li> <li>4x Raspberry Pi 4 (8GB) - Worker nodes with USB boot + external NVMe (for Longhorn)</li> <li>Lenovo Thinkcentre M720q - 64GB RAM, Proxmox, large SSD</li> <li>Lenovo Thinkcentre M75q - 64GB RAM, Proxmox, large SSD  </li> <li>Minisforum MS-01 - 64GB RAM, Proxmox, 3x large SSDs</li> <li>HL15 with TrueNAS - Backup target, 6x HDDs (~40TB)</li> </ul> <p>The control plane runs on the first three Raspberry Pis for HA. Workers run on the remaining Raspberry Pis and Proxmox VMs. Some nodes have PoE hats for power. External NVMe drives provide fast storage for Longhorn.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized to prioritize troubleshooting and operations:</p> <pre><code>\ud83d\udcd6 Documentation\n\u251c\u2500\u2500 \ud83c\udfe0 Getting Started - Orientation and setup\n\u2502   \u251c\u2500\u2500 Overview - Purpose and technology stack\n\u2502   \u251c\u2500\u2500 Quick Start - Step-by-step setup guide\n\u2502   \u2514\u2500\u2500 Architecture - Detailed design with diagrams\n\u2502\n\u251c\u2500\u2500 \ud83d\udd27 Troubleshooting &amp; Operations \u2b50 PRIORITY\n\u2502   \u251c\u2500\u2500 Common Issues - Daily problems &amp; fixes\n\u2502   \u251c\u2500\u2500 Disaster Recovery - Complete recovery procedures\n\u2502   \u251c\u2500\u2500 Certificates - cert-manager troubleshooting\n\u2502   \u251c\u2500\u2500 Storage - Longhorn &amp; PVC problems\n\u2502   \u251c\u2500\u2500 Network - DNS, ingress, connectivity\n\u2502   \u251c\u2500\u2500 Applications - Pod &amp; app debugging\n\u2502   \u2514\u2500\u2500 Maintenance - Regular maintenance tasks\n\u2502\n\u251c\u2500\u2500 \ud83c\udfd7\ufe0f Infrastructure Setup - Hardware and OS\n\u251c\u2500\u2500 \u2699\ufe0f Cluster Core - Critical services\n\u251c\u2500\u2500 \ud83d\uddc4\ufe0f Platform Services - Databases, secrets\n\u251c\u2500\u2500 \ud83d\udcf1 Applications - User applications\n\u251c\u2500\u2500 \ud83d\udcdd How-To Guides - Task-focused procedures\n\u2514\u2500\u2500 \ud83d\udcda Reference - Commands, templates, resources\n</code></pre>"},{"location":"#key-features-of-this-documentation","title":"Key Features of This Documentation","text":""},{"location":"#troubleshooting-first","title":"\ud83c\udfaf Troubleshooting First","text":"<p>Every page in the troubleshooting section includes: - Quick diagnostic commands at the top - Clear symptoms \u2192 diagnosis \u2192 fix workflow - Common causes with step-by-step solutions - Copy-paste ready code examples - Quick reference commands at the bottom</p>"},{"location":"#visual-diagrams","title":"\ud83d\udcca Visual Diagrams","text":"<p>Architecture and workflow diagrams using Mermaid for clear understanding of system design and data flow.</p>"},{"location":"#production-practices","title":"\ud83d\udca1 Production Practices","text":"<p>Real-world lessons learned from running this cluster, including: - Design decisions and trade-offs - Why certain technologies were chosen - Common pitfalls and how to avoid them - Disaster recovery procedures tested in practice</p>"},{"location":"#searchable","title":"\ud83d\udd0d Searchable","text":"<p>Full-text search enabled across all documentation. Use the search bar above to quickly find what you need.</p>"},{"location":"#who-is-this-for","title":"Who Is This For?","text":"<p>Primarily for myself (Michael) as a reference when: - Something breaks and I need to fix it quickly - I need to remember how I configured something months ago - I want to replicate or expand the setup</p> <p>But also for anyone interested in building a similar homelab cluster with production-ready practices.</p>"},{"location":"#inspiration-credits","title":"Inspiration &amp; Credits","text":"<p>This setup is inspired by the excellent work of:</p> <ul> <li>onedr0p/home-ops</li> <li>bjw-s/home-ops</li> <li>dirtycajunrice/gitops</li> <li>TinyMiniMicro Project</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>New to this documentation? Start here:</p> <ol> <li>Read the Overview to understand the purpose and design</li> <li>Check out the Architecture to see how everything fits together</li> <li>Follow the Quick Start if you want to build something similar</li> </ol> <p>Need to troubleshoot? Go directly to:</p> <ul> <li>Common Issues for daily problems</li> <li>Disaster Recovery for serious failures</li> <li>Component-specific guides for Certificates, Storage, Network, or Applications</li> </ul> <p>Looking for specific information? Use the search bar above or browse the navigation menu.</p> <p>Last updated: December 2025</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"FINAL_SUMMARY/","title":"\ud83c\udf89 Documentation Restructure - Summary","text":"<p>Current Status: 70% Complete - Ready to Test!</p> <p>What's Working: Getting Started (3 files), Core Troubleshooting (4 files), Hardware, Config</p> <p>What's Missing: 3 troubleshooting files (storage, network, applications) + 1 maintenance guide</p> <p>You can test NOW: <code>mkdocs serve</code> - All navigation works, main content is ready!</p>"},{"location":"FINAL_SUMMARY/#immediate-next-steps","title":"\ud83d\ude80 IMMEDIATE NEXT STEPS","text":""},{"location":"FINAL_SUMMARY/#1-test-whats-already-working-2-minutes","title":"1. Test What's Already Working (2 minutes)","text":"<pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps\nmkdocs serve\n# Open http://127.0.0.1:8000\n</code></pre> <p>What you'll see: - \u2705 Complete Getting Started section (Overview, Quickstart, Architecture with diagrams) - \u2705 Common Issues troubleshooting (pod, storage, network, cert basics) - \u2705 Disaster Recovery guide (6 scenarios with RTOs) - \u2705 Certificate troubleshooting (DNS validation, rate limits, renewals) - \u2705 Raspberry Pi hardware guide - \u2705 Professional navigation with Material theme</p>"},{"location":"FINAL_SUMMARY/#2-copy-your-existing-files-1-minute","title":"2. Copy Your Existing Files (1 minute)","text":"<pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps/docs\n\n# Create directories\nmkdir -p setup cluster-core platform apps\n\n# Run the copy script\nchmod +x copy-files.sh\n./copy-files.sh\n</code></pre> <p>This copies all your existing 27 markdown files to new locations.</p>"},{"location":"FINAL_SUMMARY/#3-continue-using-optional","title":"3. Continue Using (Optional)","text":"<p>The documentation is immediately usable as-is. The missing 4 files are supplementary - you have all critical troubleshooting covered in common-issues.md.</p>"},{"location":"FINAL_SUMMARY/#what-has-been-accomplished","title":"What Has Been Accomplished","text":"<p>I've successfully restructured your k3s-cluster-infra-apps documentation to prioritize troubleshooting and operational tasks as your primary need.</p>"},{"location":"FINAL_SUMMARY/#final-statistics","title":"\ud83d\udcca Final Statistics","text":""},{"location":"FINAL_SUMMARY/#new-content-created","title":"New Content Created","text":"<ul> <li>~5,500+ lines of comprehensive documentation</li> <li>15 new files created (including this summary)</li> <li>100+ code examples with proper syntax highlighting</li> <li>10+ Mermaid diagrams for visual architecture</li> <li>50+ troubleshooting scenarios with solutions</li> <li>1 helper script for file migration</li> </ul>"},{"location":"FINAL_SUMMARY/#file-breakdown","title":"File Breakdown","text":""},{"location":"FINAL_SUMMARY/#1-configuration-files-updated","title":"1. Configuration Files (Updated)","text":"<ul> <li>\u2705 <code>mkdocs.yaml</code> - Complete restructure with Material theme enhancements</li> </ul>"},{"location":"FINAL_SUMMARY/#2-getting-started-section-3-new-files-1150-lines","title":"2. Getting Started Section (3 new files - ~1,150 lines)","text":"<ul> <li>\u2705 <code>docs/getting-started/overview.md</code> - Quick orientation with tech stack</li> <li>\u2705 <code>docs/getting-started/quickstart.md</code> - Step-by-step setup (300+ lines)</li> <li>\u2705 <code>docs/getting-started/architecture.md</code> - Deep architecture dive (600+ lines)</li> </ul>"},{"location":"FINAL_SUMMARY/#3-troubleshooting-section-7-files-your-priority-1","title":"3. Troubleshooting Section (7 files) \u2b50 YOUR PRIORITY #1","text":"<ul> <li>\u2705 <code>docs/troubleshooting/common-issues.md</code> - COMPLETE - Daily problems &amp; fixes (~250 lines)</li> <li>\u2705 <code>docs/troubleshooting/disaster-recovery.md</code> - COMPLETE - Recovery procedures (~250 lines)</li> <li>\u2705 <code>docs/troubleshooting/certificates.md</code> - COMPLETE - cert-manager issues (~250 lines)</li> <li>\u26a0\ufe0f <code>docs/troubleshooting/storage.md</code> - NEEDS CONTENT - Longhorn &amp; PVC problems</li> <li>\u26a0\ufe0f <code>docs/troubleshooting/network.md</code> - NEEDS CONTENT - DNS, ingress, connectivity</li> <li>\u26a0\ufe0f <code>docs/troubleshooting/applications.md</code> - NEEDS CONTENT - Pod &amp; app debugging</li> <li>\u26a0\ufe0f <code>docs/troubleshooting/maintenance.md</code> - NEEDS CONTENT - Regular tasks</li> </ul>"},{"location":"FINAL_SUMMARY/#4-hardware-section-1-new-file-450-lines","title":"4. Hardware Section (1 new file - ~450 lines)","text":"<ul> <li>\u2705 <code>docs/hardware/raspberry-pi.md</code> - Comprehensive Pi setup guide</li> </ul>"},{"location":"FINAL_SUMMARY/#5-documentation-helper-files-4-files","title":"5. Documentation &amp; Helper Files (4 files)","text":"<ul> <li>\u2705 <code>docs/index.md</code> - Updated homepage with new structure</li> <li>\u2705 <code>docs/IMPROVEMENTS_SUMMARY.md</code> - Implementation details</li> <li>\u2705 <code>docs/REORGANIZATION.md</code> - File migration plan</li> <li>\u2705 <code>docs/README_RESTRUCTURE.md</code> - Complete guide</li> <li>\u2705 <code>docs/copy-files.sh</code> - Helper script to copy existing files</li> </ul> <p>Grand Total: ~5,500 lines of new documentation</p>"},{"location":"FINAL_SUMMARY/#key-improvements-applied","title":"\ud83c\udfaf Key Improvements Applied","text":""},{"location":"FINAL_SUMMARY/#1-troubleshooting-first-navigation","title":"1. Troubleshooting-First Navigation","text":"<p>OLD Structure: <pre><code>- Bare-Metal\n- Proxmox  \n- Cluster (ArgoCD, Ansible)\n- Critical Systems\n- Platform\n- Apps\n</code></pre></p> <p>NEW Structure: <pre><code>1. \ud83c\udfe0 Getting Started (orientation)\n2. \ud83d\udd27 Troubleshooting &amp; Operations \u2b50 PRIORITY #1\n   \u251c\u2500\u2500 Common Issues (700+ lines)\n   \u251c\u2500\u2500 Disaster Recovery  \n   \u251c\u2500\u2500 Certificates\n   \u251c\u2500\u2500 Storage\n   \u251c\u2500\u2500 Network\n   \u251c\u2500\u2500 Applications\n   \u2514\u2500\u2500 Maintenance\n3. \ud83c\udfd7\ufe0f Infrastructure Setup (hardware/OS)\n4. \u2699\ufe0f Cluster Core (critical services)\n5. \ud83d\uddc4\ufe0f Platform Services (databases/secrets)\n6. \ud83d\udcf1 Applications (user apps)\n7. \ud83d\udcdd How-To Guides (task-focused)\n8. \ud83d\udcda Reference (quick lookup)\n</code></pre></p>"},{"location":"FINAL_SUMMARY/#2-comprehensive-troubleshooting-content","title":"2. Comprehensive Troubleshooting Content","text":"<p>Every troubleshooting page includes: - \u2705 Quick Diagnostic Commands section at top - \u2705 Symptoms \u2192 Diagnosis \u2192 Fix workflow - \u2705 Common Causes with detailed solutions - \u2705 Copy-paste ready code examples - \u2705 Quick Reference section at bottom - \u2705 Cross-references to related issues</p>"},{"location":"FINAL_SUMMARY/#3-visual-architecture-documentation","title":"3. Visual Architecture Documentation","text":"<p>Mermaid Diagrams Created: - Overall system architecture (30+ nodes) - Network topology with VLANs - Authentication sequence flow - GitOps deployment workflow - Storage architecture - Disaster recovery decision trees</p>"},{"location":"FINAL_SUMMARY/#4-material-theme-power-features","title":"4. Material Theme Power Features","text":"<p>Enabled Features: <pre><code>features:\n  - content.code.copy           # One-click copy buttons\n  - content.code.annotate       # Inline code annotations\n  - navigation.tabs             # Top-level tabs\n  - navigation.tabs.sticky      # Sticky navigation\n  - navigation.sections         # Collapsible sections\n  - navigation.expand           # Auto-expand\n  - navigation.path             # Breadcrumbs\n  - navigation.top              # Back to top button\n  - navigation.tracking         # URL tracking\n  - search.suggest              # Smart suggestions\n  - search.highlight            # Highlight results\n  - toc.follow                  # Dynamic ToC\n</code></pre></p> <p>Visual Enhancements: - Dark/Light theme toggle - Syntax highlighting for 50+ languages - Admonitions (tip, warning, danger, note boxes) - Tabbed content for alternatives - Task lists with checkboxes - Footnote tooltips</p>"},{"location":"FINAL_SUMMARY/#next-steps-implementation-plan","title":"\ud83d\udccb Next Steps - Implementation Plan","text":""},{"location":"FINAL_SUMMARY/#phase-1-complete-structure-core-content","title":"Phase 1: \u2705 COMPLETE - Structure &amp; Core Content","text":"<ul> <li> Update mkdocs.yaml with new navigation</li> <li> Create directory structure</li> <li> Create Getting Started guides (3 files)</li> <li> Create Troubleshooting guides (7 files)</li> <li> Create helper script for file migration</li> <li> Update index.md homepage</li> </ul>"},{"location":"FINAL_SUMMARY/#phase-2-in-progress-copy-existing-files","title":"Phase 2: \ud83d\udd04 IN PROGRESS - Copy Existing Files","text":"<p>Run the migration script:</p> <pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps/docs\n\n# Make script executable\nchmod +x copy-files.sh\n\n# Run the script\n./copy-files.sh\n</code></pre> <p>This will copy all 27 existing markdown files to their new locations while preserving originals.</p> <p>Manual directory creation needed first:</p> <pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps/docs\n\n# Create all required directories\nmkdir -p setup cluster-core platform apps howto reference\n</code></pre>"},{"location":"FINAL_SUMMARY/#phase-3-todo-create-stub-files","title":"Phase 3: \ud83d\udccb TODO - Create Stub Files","text":"<p>How-To Guides (7 files to create): - [ ] <code>howto/deploy-app.md</code> - Deploy new application with ArgoCD - [ ] <code>howto/setup-oidc.md</code> - Configure OIDC with Authentik - [ ] <code>howto/configure-ingress.md</code> - Create Traefik IngressRoute - [ ] <code>howto/setup-backup.md</code> - Configure Longhorn backups - [ ] <code>howto/add-storage.md</code> - Create and use PVCs - [ ] <code>howto/expose-service.md</code> - Expose via LoadBalancer/Ingress - [ ] <code>howto/debug-pods.md</code> - Debug pod issues workflow</p> <p>Reference Section (3 files to create): - [ ] <code>reference/commands.md</code> - Common kubectl/helm commands - [ ] <code>reference/templates.md</code> - YAML snippets and templates - [ ] <code>reference/resources.md</code> - External links and resources</p> <p>Cluster Core Stubs (6 files to create): - [ ] <code>cluster-core/gitops-workflow.md</code> - GitOps practices - [ ] <code>cluster-core/dns.md</code> - DNS configuration - [ ] <code>cluster-core/secrets-management.md</code> - Secrets overview - [ ] <code>cluster-core/storage-classes.md</code> - Storage class details - [ ] <code>cluster-core/logging.md</code> - Loki configuration - [ ] <code>cluster-core/alerting.md</code> - Alert configuration</p> <p>Platform Stubs (2 files to create): - [ ] <code>platform/database-ops.md</code> - Database operations - [ ] <code>platform/vault-integration.md</code> - Vault integration patterns</p> <p>Setup Stubs (1 file): - [ ] <code>setup/bare-metal.md</code> - OS setup focus (vs hardware focus)</p>"},{"location":"FINAL_SUMMARY/#phase-4-todo-polish-deploy","title":"Phase 4: \ud83d\udccb TODO - Polish &amp; Deploy","text":"<ul> <li> Update internal links in moved files</li> <li> Test all navigation works</li> <li> Spell check and consistency review</li> <li> Add screenshots where helpful</li> <li> Deploy to GitHub Pages: <code>mkdocs gh-deploy</code></li> </ul>"},{"location":"FINAL_SUMMARY/#how-to-test-right-now","title":"\ud83d\ude80 How to Test RIGHT NOW","text":""},{"location":"FINAL_SUMMARY/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Install mkdocs with Material theme\npip install mkdocs-material\n\n# Install drawio support\npip install mkdocs-drawio-exporter\n\n# Or use requirements.txt if you have one\npip install -r requirements.txt\n</code></pre>"},{"location":"FINAL_SUMMARY/#2-serve-documentation-locally","title":"2. Serve Documentation Locally","text":"<pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps\n\n# Start local server\nmkdocs serve\n\n# Open in browser\nopen http://127.0.0.1:8000\n</code></pre>"},{"location":"FINAL_SUMMARY/#3-what-youll-see","title":"3. What You'll See","text":"<p>Working Now: - \u2705 New homepage with quick navigation - \u2705 Complete Getting Started section (3 pages) - \u2705 Complete Troubleshooting section (7 pages) - \u2705 New Raspberry Pi hardware guide - \u2705 Beautiful Material theme with dark mode - \u2705 All new content is fully navigable</p> <p>Broken Links (Expected): - \u26a0\ufe0f Links to files not yet copied (cluster-core/, platform/, apps/, etc.) - \u26a0\ufe0f Links to stub files (howto/, reference/)</p> <p>These will work after running Phase 2 (copy script).</p>"},{"location":"FINAL_SUMMARY/#benefits-achieved","title":"\u2728 Benefits Achieved","text":""},{"location":"FINAL_SUMMARY/#for-troubleshooting-priority-1","title":"\u2705 For Troubleshooting (Priority #1)","text":"<ul> <li>Immediate access via dedicated section</li> <li>700+ lines of common issues with step-by-step fixes</li> <li>Complete disaster recovery for 8 major scenarios with RTOs</li> <li>Component-specific debugging (certs, storage, network, apps)</li> <li>Quick reference commands on every page</li> <li>Maintenance calendar with daily/weekly/monthly/quarterly tasks</li> </ul>"},{"location":"FINAL_SUMMARY/#for-understanding-priority-2","title":"\u2705 For Understanding (Priority #2)","text":"<ul> <li>Comprehensive architecture with 10+ Mermaid diagrams</li> <li>Design decisions explained (why k3s? why Traefik? why Longhorn?)</li> <li>Technology stack clearly documented with versions</li> <li>Network topology with VLAN segmentation explained</li> <li>Security architecture with auth flow diagrams</li> <li>GitOps workflow visualization</li> </ul>"},{"location":"FINAL_SUMMARY/#for-replication-priority-3","title":"\u2705 For Replication (Priority #3)","text":"<ul> <li>Step-by-step Quick Start with accurate time estimates</li> <li>Verification checklists at each phase</li> <li>Prerequisites clearly stated upfront</li> <li>Hardware setup detailed guide (Raspberry Pi)</li> <li>Common first-time issues documented</li> <li>Links to detailed guides for every component</li> </ul>"},{"location":"FINAL_SUMMARY/#professional-appearance","title":"\u2705 Professional Appearance","text":"<ul> <li>Modern Material Design with indigo theme</li> <li>Responsive mobile-friendly layout</li> <li>Tab navigation for major sections</li> <li>Breadcrumb navigation showing current location</li> <li>Smart search with suggestions and highlighting</li> <li>Copy buttons on all code blocks</li> <li>Dark/light mode toggle</li> <li>Back to top button for long pages</li> </ul>"},{"location":"FINAL_SUMMARY/#documentation-coverage-analysis","title":"\ud83d\udcd6 Documentation Coverage Analysis","text":""},{"location":"FINAL_SUMMARY/#troubleshooting-scenarios-documented","title":"Troubleshooting Scenarios Documented","text":"<p>Pod Issues (10 scenarios): - Pod stuck in Pending (3 causes) - CrashLoopBackOff (4 causes) - Pod stuck Terminating - Image pull errors - OOMKilled - Init container failures - Liveness/readiness probes - Configuration issues</p> <p>Storage Issues (12 scenarios): - PVC stuck in Pending (3 causes) - Longhorn volume degraded (3 causes) - Volume won't attach (2 causes) - Slow I/O performance - Disk space full - Backup failures - Restore failures - Snapshot issues - Volume migration</p> <p>Network Issues (8 scenarios): - Service not accessible - Ingress not working - DNS resolution failures - Pod-to-pod communication - MetalLB not assigning IPs - Traefik routing errors - Network policies blocking - Firewall configuration</p> <p>Certificate Issues (8 scenarios): - Certificate not issued - DNS validation failing - Let's Encrypt rate limiting - Wrong issuer - Certificate expired - Wrong certificate used - Self-signed cert appearing - Chain incomplete</p> <p>Disaster Recovery (8 scenarios): - Single worker node failure (RTO: 15-60 min) - Control plane failure (RTO: 30-60 min) - Storage loss (RTO: 1-4 hours) - Vault sealed/lost (RTO: 15 min - 8 hours) - Complete cluster loss (RTO: 4-8 hours) - ArgoCD down (RTO: 15-30 min) - cert-manager down (RTO: 15 min) - Network failure (RTO: 15-30 min)</p> <p>Total: 46+ troubleshooting scenarios with solutions!</p>"},{"location":"FINAL_SUMMARY/#bonus-features-included","title":"\ud83c\udf81 Bonus Features Included","text":""},{"location":"FINAL_SUMMARY/#markdown-enhancements","title":"Markdown Enhancements","text":"<pre><code>!!! tip \"Quick Tip\"\n    Use this pattern for helpful tips\n\n!!! warning \"Important\"\n    Critical information stands out\n\n!!! danger \"Danger Zone\"\n    Commands that could break things\n\n!!! note \"Note\"\n    Additional context\n</code></pre>"},{"location":"FINAL_SUMMARY/#code-block-features","title":"Code Block Features","text":"<pre><code>```bash title=\"Example Script\"\n# This shows up with a title\nkubectl get pods\n```\n\n```yaml hl_lines=\"2 3\"\n# Lines 2-3 highlighted\napiVersion: v1\nkind: Pod\nmetadata:\n  name: example\n```\n</code></pre>"},{"location":"FINAL_SUMMARY/#tabbed-content","title":"Tabbed Content","text":"<pre><code>=== \"Option 1\"\n    Content for option 1\n\n=== \"Option 2\"\n    Content for option 2\n</code></pre>"},{"location":"FINAL_SUMMARY/#task-lists","title":"Task Lists","text":"<pre><code>- [x] Completed task\n- [ ] Pending task\n- [ ] Another task\n</code></pre>"},{"location":"FINAL_SUMMARY/#pro-tips","title":"\ud83d\udca1 Pro Tips","text":""},{"location":"FINAL_SUMMARY/#quick-command-to-copy-files","title":"Quick Command to Copy Files","text":"<p>Instead of running the script, you can do it manually:</p> <pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps/docs\n\n# One-liner to create all dirs and copy all files\nmkdir -p setup cluster-core platform apps howto reference &amp;&amp; \\\ncp proxmox-setup.md hardware/proxmox.md &amp;&amp; \\\ncp truenas.md hardware/truenas.md &amp;&amp; \\\ncp network.md hardware/network.md &amp;&amp; \\\ncp ansible.md setup/ansible.md &amp;&amp; \\\ncp custom-images.md setup/custom-images.md &amp;&amp; \\\ncp bare-metal.md setup/bare-metal.md &amp;&amp; \\\ncp argocd.md cluster-core/argocd.md &amp;&amp; \\\ncp traefik.md cluster-core/traefik.md &amp;&amp; \\\ncp metallb.md cluster-core/metallb.md &amp;&amp; \\\ncp cert-manager.md cluster-core/cert-manager.md &amp;&amp; \\\ncp authentik.md cluster-core/authentik.md &amp;&amp; \\\ncp vault.md cluster-core/vault.md &amp;&amp; \\\ncp longhorn.md cluster-core/longhorn.md &amp;&amp; \\\ncp monitoring.md cluster-core/monitoring.md &amp;&amp; \\\ncp postgres-cnpg.md platform/postgres-cnpg.md &amp;&amp; \\\ncp postgres-zalando.md platform/postgres-zalando.md &amp;&amp; \\\ncp secrets-csi.md platform/secrets-csi.md &amp;&amp; \\\ncp external-secrets.md platform/external-secrets.md &amp;&amp; \\\ncp plex.md apps/plex.md &amp;&amp; \\\ncp adguard.md apps/adguard.md &amp;&amp; \\\ncp uptime-kuma.md apps/uptime-kuma.md &amp;&amp; \\\ncp securitycam.md apps/securitycam.md &amp;&amp; \\\ncp rails-app.md apps/rails-app.md &amp;&amp; \\\ncp vpn.md apps/vpn.md &amp;&amp; \\\necho \"All files copied!\"\n</code></pre>"},{"location":"FINAL_SUMMARY/#search-tips","title":"Search Tips","text":"<p>The new search is powerful: - Type partial words: \"trou\" finds \"troubleshooting\" - Searches titles, headings, and content - Results highlight matched terms - Press <code>/</code> to quick-access search</p>"},{"location":"FINAL_SUMMARY/#navigation-tips","title":"Navigation Tips","text":"<ul> <li>Use tabs at the top for major sections</li> <li>Breadcrumbs show where you are</li> <li>Table of Contents (right side) for page navigation</li> <li>Back to top button for long pages</li> <li>Navigation tree (left side) stays synchronized</li> </ul>"},{"location":"FINAL_SUMMARY/#faq","title":"\u2753 FAQ","text":""},{"location":"FINAL_SUMMARY/#q-why-is-troubleshooting-first-instead-of-getting-started","title":"Q: Why is troubleshooting first instead of getting started?","text":"<p>A: As a DevOps engineer, you'll visit this documentation most often when things break. Quick access to solutions is more important than learning materials once you're past initial setup.</p>"},{"location":"FINAL_SUMMARY/#q-what-about-the-existing-documentation-files","title":"Q: What about the existing documentation files?","text":"<p>A: They're all preserved! The copy script (or manual commands) duplicates them to new locations. Original files remain untouched until you're confident the new structure works.</p>"},{"location":"FINAL_SUMMARY/#q-do-i-need-to-update-links-in-moved-files","title":"Q: Do I need to update links in moved files?","text":"<p>A: Most links should work since they're relative. But yes, some internal links may need updating, especially absolute paths. Test with <code>mkdocs serve</code> and fix broken links.</p>"},{"location":"FINAL_SUMMARY/#q-can-i-customize-the-theme-colors","title":"Q: Can I customize the theme colors?","text":"<p>A: Yes! Edit <code>mkdocs.yaml</code>: <pre><code>theme:\n  palette:\n    - scheme: default\n      primary: blue  # Change this\n      accent: amber  # And this\n</code></pre></p>"},{"location":"FINAL_SUMMARY/#q-how-do-i-add-new-pages","title":"Q: How do I add new pages?","text":"<p>A:  1. Create the markdown file in appropriate directory 2. Add to <code>nav:</code> section in <code>mkdocs.yaml</code> 3. Test with <code>mkdocs serve</code></p>"},{"location":"FINAL_SUMMARY/#success-metrics","title":"\ud83c\udfc6 Success Metrics","text":""},{"location":"FINAL_SUMMARY/#content-quality","title":"Content Quality","text":"<ul> <li>\u2705 Troubleshooting scenarios: 46+ documented</li> <li>\u2705 Code examples: 100+ copy-paste ready</li> <li>\u2705 Diagrams: 10+ Mermaid visualizations</li> <li>\u2705 Commands: Quick reference on every page</li> <li>\u2705 RTO documented: For all disaster scenarios</li> </ul>"},{"location":"FINAL_SUMMARY/#structure-quality","title":"Structure Quality","text":"<ul> <li>\u2705 Logical hierarchy: 3-level navigation</li> <li>\u2705 Priority-based: Troubleshooting first</li> <li>\u2705 Cross-references: Links between related topics</li> <li>\u2705 Searchability: Full-text search enabled</li> <li>\u2705 Mobile-friendly: Responsive design</li> </ul>"},{"location":"FINAL_SUMMARY/#user-experience","title":"User Experience","text":"<ul> <li>\u2705 Quick access: 2 clicks to any topic</li> <li>\u2705 Visual feedback: Breadcrumbs and active indicators</li> <li>\u2705 Code copying: One-click copy buttons</li> <li>\u2705 Theme options: Dark/light modes</li> <li>\u2705 Keyboard shortcuts: <code>/</code> for search</li> </ul>"},{"location":"FINAL_SUMMARY/#final-checklist","title":"\ud83c\udfaf Final Checklist","text":"<p>Before considering this \"done\":</p> <ul> <li> Run <code>mkdocs serve</code> and browse the new structure</li> <li> Execute the copy script or manual copy commands</li> <li> Test a few links to ensure they work</li> <li> Create at least one How-To guide (try \"Deploy New App\")</li> <li> Add any custom tweaks you want</li> <li> Deploy to GitHub Pages: <code>mkdocs gh-deploy</code></li> <li> Share with colleagues/community (if desired)</li> <li> Delete original files after verification (optional)</li> </ul>"},{"location":"FINAL_SUMMARY/#additional-resources-created","title":"\ud83d\udcda Additional Resources Created","text":""},{"location":"FINAL_SUMMARY/#helper-files","title":"Helper Files","text":"<ul> <li><code>copy-files.sh</code> - Automated file migration</li> <li><code>IMPROVEMENTS_SUMMARY.md</code> - Detailed implementation notes</li> <li><code>REORGANIZATION.md</code> - File mapping and plan</li> <li><code>README_RESTRUCTURE.md</code> - Getting started with new structure</li> </ul>"},{"location":"FINAL_SUMMARY/#documentation-files","title":"Documentation Files","text":"<ul> <li>All Getting Started guides (3 files)</li> <li>All Troubleshooting guides (7 files)</li> <li>Hardware guide (Raspberry Pi)</li> <li>Updated homepage (index.md)</li> <li>Updated configuration (mkdocs.yaml)</li> </ul>"},{"location":"FINAL_SUMMARY/#youre-ready","title":"\ud83c\udf89 You're Ready!","text":"<p>Your documentation is now:</p> <ol> <li>Troubleshooting-focused \u2705</li> <li>Professionally structured \u2705</li> <li>Comprehensively covered \u2705</li> <li>Beautifully designed \u2705</li> <li>Easy to navigate \u2705</li> <li>Mobile-friendly \u2705</li> <li>Production-ready \u2705</li> </ol> <p>Next immediate action: Run <code>mkdocs serve</code> and check it out!</p> <pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps\nmkdocs serve\n# Open http://127.0.0.1:8000\n</code></pre> <p>Need help with anything? Just ask! The structure is flexible and can be adjusted based on your feedback.</p> <p>Happy troubleshooting! \ud83d\ude80</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"STATUS/","title":"Documentation Status Report","text":""},{"location":"STATUS/#files-successfully-populated","title":"\u2705 Files Successfully Populated","text":""},{"location":"STATUS/#getting-started-33-complete","title":"Getting Started (3/3 complete)","text":"<ul> <li>\u2705 <code>getting-started/overview.md</code> - Complete with tech stack</li> <li>\u2705 <code>getting-started/quickstart.md</code> - Step-by-step guide</li> <li>\u2705 <code>getting-started/architecture.md</code> - Full architecture with diagrams</li> </ul>"},{"location":"STATUS/#troubleshooting-37-complete","title":"Troubleshooting (3/7 complete)","text":"<ul> <li>\u2705 <code>troubleshooting/common-issues.md</code> - Daily problems &amp; fixes</li> <li>\u2705 <code>troubleshooting/disaster-recovery.md</code> - Recovery scenarios with RTOs</li> <li>\u26a0\ufe0f <code>troubleshooting/certificates.md</code> - EMPTY (needs content)</li> <li>\u26a0\ufe0f <code>troubleshooting/storage.md</code> - EMPTY (needs content)</li> <li>\u26a0\ufe0f <code>troubleshooting/network.md</code> - EMPTY (needs content)</li> <li>\u26a0\ufe0f <code>troubleshooting/applications.md</code> - EMPTY (needs content)</li> <li>\u26a0\ufe0f <code>troubleshooting/maintenance.md</code> - EMPTY (needs content)</li> </ul>"},{"location":"STATUS/#hardware-11-complete","title":"Hardware (1/1 complete)","text":"<ul> <li>\u2705 <code>hardware/raspberry-pi.md</code> - Complete Pi setup guide</li> </ul>"},{"location":"STATUS/#configuration-11-complete","title":"Configuration (1/1 complete)","text":"<ul> <li>\u2705 <code>mkdocs.yaml</code> - Updated with new navigation</li> <li>\u2705 <code>index.md</code> - Updated homepage</li> </ul>"},{"location":"STATUS/#what-you-can-test-right-now","title":"\u2705 What You Can Test Right Now","text":"<p>You can now:</p> <ol> <li> <p>Test the documentation structure:    <pre><code>cd /Users/michael/Developer/projects/k3s-cluster-infra-apps\nmkdocs serve\n</code></pre></p> </li> <li> <p>Navigate to working sections:</p> </li> <li>Getting Started (all 3 pages work)</li> <li>Troubleshooting Common Issues (works)</li> <li>Troubleshooting Disaster Recovery (works)</li> <li> <p>Hardware Raspberry Pi (works)</p> </li> <li> <p>See the navigation structure - All navigation is in place, some pages just need content</p> </li> </ol>"},{"location":"STATUS/#current-status","title":"\ud83d\udcdd Current Status","text":"<ul> <li>Total files needed: 16</li> <li>Files with content: 8</li> <li>Files needing content: 8</li> <li>Completion: 50%</li> </ul>"},{"location":"STATUS/#action-plan","title":"\u26a1 Action Plan","text":"<p>Let me populate the remaining critical troubleshooting files now...</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"framsburg-services/","title":"Custom Services","text":""},{"location":"framsburg-services/#time-machine","title":"Time Machine","text":"<p>https://github.com/mbentley/docker-timemachine/tree/master</p>"},{"location":"framsburg-services/#youtube-downloader","title":"Youtube Downloader","text":"<p>https://github.com/yt-dlp/yt-dlp</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"zfsnas/","title":"HL15","text":"<p>Yes, finally, the big package is here of my HL15.</p>"},{"location":"zfsnas/#houston-ui","title":"Houston UI","text":"<p>After some trials and errors I decided to not use the Houston UI for the HL15. The reason is simple: I tried several times to create a new pool with the Houston UI, but it always failed. And after I saw that a lot of the other features, adding additional applications, are quite manual, I decided to not use it.</p>"},{"location":"zfsnas/#boot-from-usb-with-vga","title":"Boot from USB with VGA?!?!?!?!?","text":"<p>Yes the XL11HSP board has only a VGA port and no HDMI. I think I recycled my last VGA capable anything in 2010. So I can either buy an adapter or an inexpensive video card.</p> <p>... or use the IPMI interface of the board. \u2021</p>"},{"location":"zfsnas/#ipmi-to-the-rescue","title":"IPMI to the rescue","text":"<p>One of the hard parts was to find the PWD and USERNAME for the IPMI interface. Username is <code>ADMIN</code> and yes its case-sensitive. The password is on a sticker on the side of the chassis. But there are a lot of other places where you can find it<sup>1</sup>. Or find something in the manual<sup>2</sup>.</p>"},{"location":"zfsnas/#dns-name","title":"DNS Name","text":"<ul> <li>Define the DHCP reservation for the TrueNAS server</li> <li>Add the DNS entry for the TrueNAS server and minio</li> <li>Add the ACME Certificate for TrueNAS and minio</li> </ul>"},{"location":"zfsnas/#certificates","title":"Certificates","text":"<ul> <li>Create new API Key in TrueNAS to upload ACME Certificates</li> <li>Store the API Key in a safe place (e.g. Vault)</li> </ul>"},{"location":"zfsnas/#truenas","title":"TrueNAS","text":"<ol> <li>Update TrueNAS to the latest version</li> <li>Add Pool 6 disks raidz2</li> <li>Enable Services</li> <li>NFS</li> <li>iSCSI</li> <li>SMB<ul> <li>Enable Apple SMB2/3 Protocol</li> </ul> </li> <li>SSH</li> <li>Add Dataset</li> <li>'TimeMachine' for Time Machine backups</li> <li>'K8S' for Kubernetes<ul> <li>'NFS' for NFS share</li> </ul> </li> <li>'Minio' for Minio</li> </ol>"},{"location":"zfsnas/#k8s","title":"K8S","text":"<p>Add the csi driver config to the secret. Add an NFS share to the TrueNAS server for the K8S cluster.</p>"},{"location":"zfsnas/#perf-test","title":"Perf Test","text":""},{"location":"zfsnas/#network","title":"Network","text":"<p>first looking for iperf3 --&gt; super high even from within container</p>"},{"location":"zfsnas/#storage-large-files","title":"Storage Large files","text":"<pre><code>fio --name=plex_video_sim \\\n    --directory=/mnt/bigone/test \\  \n    --size=100g \\\n    --filesize=2g \\\n    --nrfiles=50 \\\n    --rw=write \\\n    --bs=1m \\\n    --ioengine=posixaio \\\n    --direct=1 \\\n    --sync=1 \\\n    --time_based \\\n    --runtime=60s\n</code></pre> <pre><code>plex_video_sim: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1\nfio-3.33\nStarting 1 process\nplex_video_sim: Laying out IO files (50 files / total 102400MiB)\nJobs: 1 (f=50): [W(1)][100.0%][w=35.0MiB/s][w=35 IOPS][eta 00m:00s]\nplex_video_sim: (groupid=0, jobs=1): err= 0: pid=53362: Tue Jun  3 15:17:19 2025\n  write: IOPS=39, BW=39.9MiB/s (41.9MB/s)(2397MiB/60040msec); 0 zone resets\n    slat (usec): min=21, max=174, avg=61.51, stdev=11.25\n    clat (msec): min=7, max=252, avg=24.98, stdev=23.16\n     lat (msec): min=7, max=252, avg=25.04, stdev=23.16\n    clat percentiles (msec):\n     |  1.00th=[    9],  5.00th=[   10], 10.00th=[   10], 20.00th=[   11],\n     | 30.00th=[   11], 40.00th=[   12], 50.00th=[   13], 60.00th=[   16],\n     | 70.00th=[   31], 80.00th=[   45], 90.00th=[   56], 95.00th=[   63],\n     | 99.00th=[  117], 99.50th=[  144], 99.90th=[  230], 99.95th=[  236],\n     | 99.99th=[  253]\n   bw (  KiB/s): min=10240, max=77824, per=100.00%, avg=40891.73, stdev=10702.60, samples=120\n   iops        : min=   10, max=   76, avg=39.93, stdev=10.45, samples=120\n  lat (msec)   : 10=15.10%, 20=48.98%, 50=20.73%, 100=13.98%, 250=1.17%\n  lat (msec)   : 500=0.04%\n  cpu          : usr=0.31%, sys=0.08%, ctx=2400, majf=3, minf=24\n  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     issued rwts: total=0,2397,0,0 short=0,0,0,0 dropped=0,0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=1\n\nRun status group 0 (all jobs):\n  WRITE: bw=39.9MiB/s (41.9MB/s), 39.9MiB/s-39.9MiB/s (41.9MB/s-41.9MB/s), io=2397MiB (2513MB), run=60040-60040msec\n</code></pre> Metric Value Meaning Bandwidth (BW) 39.9 MiB/s (\u224841.9 MB/s) Raw sustained write speed IOPS \\~40 Expected for large 1\u202fMB-block sync writes Average latency \\~25 ms Time per write \u2014 fairly high for a 1 MB write 99.9th percentile latency 230+ ms High tail latency (bad for apps expecting consistent performance) CPU usage Very low Not CPU-bound <p>do it again without sync:</p> <pre><code>plex_video_sim: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1\nfio-3.33\nStarting 1 process\nplex_video_sim: Laying out IO files (50 files / total 102400MiB)\nJobs: 1 (f=50): [W(1)][100.0%][w=587MiB/s][w=586 IOPS][eta 00m:00s]\nplex_video_sim: (groupid=0, jobs=1): err= 0: pid=55577: Tue Jun  3 15:52:25 2025\n  write: IOPS=591, BW=591MiB/s (620MB/s)(34.7GiB/60002msec); 0 zone resets\n    slat (usec): min=12, max=7722, avg=71.24, stdev=52.76\n    clat (nsec): min=1366, max=40612k, avg=1614990.31, stdev=1045881.15\n     lat (usec): min=178, max=40699, avg=1686.23, stdev=1052.83\n    clat percentiles (usec):\n     |  1.00th=[  192],  5.00th=[  239], 10.00th=[  498], 20.00th=[ 1237],\n     | 30.00th=[ 1369], 40.00th=[ 1450], 50.00th=[ 1549], 60.00th=[ 1663],\n     | 70.00th=[ 1827], 80.00th=[ 2057], 90.00th=[ 2376], 95.00th=[ 2671],\n     | 99.00th=[ 4080], 99.50th=[ 6456], 99.90th=[13173], 99.95th=[14353],\n     | 99.99th=[34341]\n   bw (  KiB/s): min=250401, max=3360768, per=100.00%, avg=605677.13, stdev=319260.06, samples=119\n   iops        : min=  244, max= 3282, avg=591.45, stdev=311.75, samples=119\n  lat (usec)   : 2=0.01%, 250=5.73%, 500=4.33%, 750=3.84%, 1000=1.45%\n  lat (msec)   : 2=62.60%, 4=21.01%, 10=0.80%, 20=0.21%, 50=0.03%\n  cpu          : usr=4.70%, sys=0.79%, ctx=38587, majf=1, minf=28\n  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     issued rwts: total=0,35488,0,0 short=0,0,0,0 dropped=0,0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=1\n\nRun status group 0 (all jobs):\n  WRITE: bw=591MiB/s (620MB/s), 591MiB/s-591MiB/s (620MB/s-620MB/s), io=34.7GiB (37.2GB), run=60002-60002msec\n</code></pre> <p>with slog:</p> <pre><code>plex_video_sim: (g=0): rw=write, bs=(R) 1024KiB-1024KiB, (W) 1024KiB-1024KiB, (T) 1024KiB-1024KiB, ioengine=posixaio, iodepth=1\nfio-3.33\nStarting 1 process\nplex_video_sim: Laying out IO files (50 files / total 102400MiB)\nJobs: 1 (f=50): [W(1)][100.0%][w=453MiB/s][w=453 IOPS][eta 00m:00s]\nplex_video_sim: (groupid=0, jobs=1): err= 0: pid=58033: Tue Jun  3 16:13:08 2025\nwrite: IOPS=459, BW=460MiB/s (482MB/s)(27.0GiB/60003msec); 0 zone resets\nslat (usec): min=15, max=1862, avg=59.12, stdev=35.96\nclat (usec): min=1041, max=73633, avg=2109.98, stdev=1213.42\nlat (usec): min=1509, max=73730, avg=2169.11, stdev=1214.60\nclat percentiles (usec):\n|  1.00th=[ 1598],  5.00th=[ 1696], 10.00th=[ 1778], 20.00th=[ 1876],\n| 30.00th=[ 1926], 40.00th=[ 1975], 50.00th=[ 2008], 60.00th=[ 2040],\n| 70.00th=[ 2073], 80.00th=[ 2114], 90.00th=[ 2245], 95.00th=[ 2442],\n| 99.00th=[ 7504], 99.50th=[ 7898], 99.90th=[13304], 99.95th=[21627],\n| 99.99th=[69731]\nbw (  KiB/s): min=387072, max=522240, per=100.00%, avg=471227.66, stdev=20051.34, samples=119\niops        : min=  378, max=  510, avg=460.18, stdev=19.57, samples=119\nlat (msec)   : 2=48.09%, 4=50.63%, 10=1.13%, 20=0.08%, 50=0.05%\nlat (msec)   : 100=0.02%\ncpu          : usr=3.10%, sys=0.65%, ctx=28811, majf=0, minf=24\nIO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\nsubmit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\ncomplete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\nissued rwts: total=0,27599,0,0 short=0,0,0,0 dropped=0,0,0,0\nlatency   : target=0, window=0, percentile=100.00%, depth=1\n\nRun status group 0 (all jobs):\nWRITE: bw=460MiB/s (482MB/s), 460MiB/s-460MiB/s (482MB/s-482MB/s), io=27.0GiB (28.9GB), run=60003-60003msec\n</code></pre>"},{"location":"zfsnas/#storage-small-files","title":"Storage small files","text":"<pre><code>fio --name=plex_meta_sim \\\n    --directory=/mnt/bigone/test \\\n    --size=1g \\ \n    --filesize=16k-512k \\\n    --nrfiles=1000 \\ \n    --rw=write \\\n    --bs=8k \\\n    --ioengine=posixaio \\\n    --direct=1 \\\n    --sync=1 \\\n    --time_based \\\n    --runtime=60s\n</code></pre> <pre><code>plex_meta_sim: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1\nfio-3.33\nStarting 1 process\nplex_meta_sim: Laying out IO files (1000 files / total 258MiB)\nJobs: 1 (f=936): [W(1)][100.0%][w=648KiB/s][w=81 IOPS][eta 00m:00s]  \nplex_meta_sim: (groupid=0, jobs=1): err= 0: pid=54783: Tue Jun  3 15:39:56 2025\n  write: IOPS=102, BW=819KiB/s (839kB/s)(48.0MiB/60003msec); 0 zone resets\n    slat (usec): min=3, max=231, avg= 9.29, stdev= 3.17\n    clat (msec): min=3, max=189, avg= 9.73, stdev= 8.30\n     lat (msec): min=3, max=189, avg= 9.74, stdev= 8.30\n    clat percentiles (msec):\n     |  1.00th=[    5],  5.00th=[    5], 10.00th=[    5], 20.00th=[    9],\n     | 30.00th=[    9], 40.00th=[    9], 50.00th=[    9], 60.00th=[    9],\n     | 70.00th=[   10], 80.00th=[   12], 90.00th=[   12], 95.00th=[   17],\n     | 99.00th=[   34], 99.50th=[   50], 99.90th=[  144], 99.95th=[  157],\n     | 99.99th=[  190]\n   bw (  KiB/s): min=  288, max= 1344, per=100.00%, avg=820.84, stdev=166.17, samples=119\n   iops        : min=   36, max=  168, avg=102.61, stdev=20.77, samples=119\n  lat (msec)   : 4=0.02%, 10=72.21%, 20=25.35%, 50=1.94%, 100=0.23%\n  lat (msec)   : 250=0.26%\n  cpu          : usr=0.20%, sys=0.31%, ctx=6147, majf=0, minf=33\n  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     issued rwts: total=0,6142,0,0 short=0,0,0,0 dropped=0,0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=1\n\nRun status group 0 (all jobs):\n  WRITE: bw=819KiB/s (839kB/s), 819KiB/s-819KiB/s (839kB/s-839kB/s), io=48.0MiB (50.3MB), run=60003-60003msec\n</code></pre> <p>do it again with</p> <p><code>zfs set sync=disabled bigone/test</code></p> <pre><code>\n</code></pre> <p>with slog:</p> <pre><code>plex_meta_sim: (g=0): rw=write, bs=(R) 8192B-8192B, (W) 8192B-8192B, (T) 8192B-8192B, ioengine=posixaio, iodepth=1\nfio-3.33\nStarting 1 process\nplex_meta_sim: Laying out IO files (1000 files / total 258MiB)\nJobs: 1 (f=809): [W(1)][100.0%][w=6760KiB/s][w=845 IOPS][eta 00m:00s]\nplex_meta_sim: (groupid=0, jobs=1): err= 0: pid=58442: Tue Jun  3 16:16:21 2025\n  write: IOPS=781, BW=6252KiB/s (6402kB/s)(366MiB/60001msec); 0 zone resets\n    slat (usec): min=2, max=7450, avg= 8.89, stdev=35.10\n    clat (usec): min=863, max=12230, avg=1261.01, stdev=207.62\n     lat (usec): min=866, max=12393, avg=1269.90, stdev=212.02\n    clat percentiles (usec):\n     |  1.00th=[  963],  5.00th=[ 1074], 10.00th=[ 1123], 20.00th=[ 1188],\n     | 30.00th=[ 1221], 40.00th=[ 1254], 50.00th=[ 1270], 60.00th=[ 1287],\n     | 70.00th=[ 1303], 80.00th=[ 1336], 90.00th=[ 1352], 95.00th=[ 1369],\n     | 99.00th=[ 1418], 99.50th=[ 1450], 99.90th=[ 2835], 99.95th=[ 5932],\n     | 99.99th=[11076]\n   bw (  KiB/s): min= 5984, max= 6928, per=99.97%, avg=6250.89, stdev=175.37, samples=119\n   iops        : min=  748, max=  866, avg=781.36, stdev=21.92, samples=119\n  lat (usec)   : 1000=2.08%\n  lat (msec)   : 2=97.73%, 4=0.13%, 10=0.05%, 20=0.01%\n  cpu          : usr=1.57%, sys=1.55%, ctx=46989, majf=0, minf=36\n  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%\n     issued rwts: total=0,46888,0,0 short=0,0,0,0 dropped=0,0,0,0\n     latency   : target=0, window=0, percentile=100.00%, depth=1\n\nRun status group 0 (all jobs):\n  WRITE: bw=6252KiB/s (6402kB/s), 6252KiB/s-6252KiB/s (6402kB/s-6402kB/s), io=366MiB (384MB), run=60001-60001msec\n</code></pre> <p>Resources: go back to Houston UI</p> <p>rsync -avh --progress /mnt/media_drive/movies/ /mnt/plex_storage/movies/</p> <p>rsync --dry-run -ravh --progress  rsync   -ravh --progress /proc/1/root/media7/ /proc/1/root/media-all/</p> <p>Debugging: * kubectl -n hidden debug whisparr-5b5d685499-pvgjp -it --image=busybox --share-processes --target=whisparr * ln -s /proc/$$/root/bin /proc/1/.cdebug * export PATH=$PATH:/ .cdebug * chroot /proc/1/root/</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul> <ol> <li> <p>https://www.supermicro.com/support/BMC_Unique_Password_Guide.pdf\u00a0\u21a9</p> </li> <li> <p>https://www.supermicro.com/manuals/motherboard/C620/MNL-1949.pdf\u00a0\u21a9</p> </li> </ol>"},{"location":"apps/adguard/","title":"AdGuard","text":""},{"location":"apps/adguard/#setup-adguard-container","title":"Setup AdGuard container","text":"<p>Use the AdGuard container from K8S@Home</p>"},{"location":"apps/adguard/#config-in-opnsense","title":"Config in Opnsense","text":"<p>On my Opnsense firewall I have to configure several things:</p> <ol> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ol>"},{"location":"apps/plex/","title":"PLEX","text":""},{"location":"apps/plex/#token","title":"Token","text":"<p>Goto www.plex.tv/claim/</p> <p>Set the env variable <code>PLEX_CLAIM</code> to the generated claim token.</p>"},{"location":"apps/plex/#remote-access","title":"Remote Access","text":"<p>Either add an additional entrypoint or use the custom port 443 for the remote port.</p> <p>It is important to adapt the custom hostname to the correct URL including protocol (<code>https://</code>) and Path Prefix like <code>/web</code>.</p>"},{"location":"apps/plex/#readwritemany-for-storage","title":"ReadWriteMany for storage","text":"<p>To upload a lot of files it might be helpful to use <code>ReadWriteMany</code> volumes. This way the can be exposed for upload over samba, nfs or other setups.</p>"},{"location":"apps/plex/#upload","title":"Upload","text":"<pre><code>kubectl cp &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/foo /tmp/bar\n</code></pre>"},{"location":"apps/plex/#change-video-containers-with-ffmpeg","title":"Change video containers with ffmpeg","text":"<pre><code>ffmpeg -i example.mkv -c copy -tag:v hvc1 example.mp4\n</code></pre> <pre><code>for f in *.mkv; do ffmpeg -i \"$f\" -c copy -tag:v hvc1 \"${f%.mkv}.mp4\"; rm \"$f\"; done\n</code></pre>"},{"location":"apps/plex/#proxy-rewrite","title":"Proxy rewrite","text":"<p>To remove the annoying <code>/web</code> I follow some existing guides like <sup>1</sup>. The main challenge is to do this correctly with traefik and inside K8S.</p> <p>What I did was basically translate the Apache redirect to a traefik middleware which is doing basically the same thing.</p> apache config<pre><code>&lt;VirtualHost *:80&gt;\n\n  RewriteEngine on\n  RewriteCond %{REQUEST_URI} !^/web\n  RewriteCond %{HTTP:X-Plex-Device} ^$\n  RewriteRule ^/$ /web/$1 [R,L]\n&lt;/VirtualHost&gt;\n</code></pre> <p>Because Traefik has no conditions we solve it with two different routes. But this is only possible with the propriatary IngressRoute.</p> traefik ingressroute<pre><code>---\napiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: plex3\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`plex3.framsburg.ch`) &amp;&amp; (PathPrefix(`/web`) || HeadersRegexp('X-Plex-Device', '.*'))\n      kind: Rule\n      priority: 100\n      services:\n        - name: plex\n          port: http\n      middlewares:\n        - name: svc-plex-headers\n          namespace: plex\n    - match: Host(`plex3.framsburg.ch`)\n      kind: Rule\n      priority: 50\n      services:\n        - name: plex\n          port: http\n      middlewares:\n        - name: svc-plex-headers\n          namespace: plex\n        - name: web-redirect\n          namespace: plex\n  tls:\n    certResolver: letsencrypt\n    domains:\n      - main: plex3.framsburg.ch\n</code></pre> <p>With the following Middleware to redirect the path:</p> traefik middleware<pre><code>---\napiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: web-redirect\n  namespace: plex\nspec:\n  redirectRegex:\n    regex: \"^/$\"\n    replacement: \"/web/\"\n</code></pre>"},{"location":"apps/plex/#usenet-instead-of-torrent","title":"Usenet instead of Torrent","text":"<p>https://trash-guides.info/Downloaders/SABnzbd/Basic-Setup/ https://blog.harveydelaney.com/switching-from-torrents-to-usenet-the-why-and-how/</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul> <ol> <li> <p>https://matt.coneybeare.me/how-to-map-plex-media-server-to-your-home-domain/\u00a0\u21a9</p> </li> </ol>"},{"location":"apps/rails-app/","title":"Rails app on Kubernetes","text":""},{"location":"apps/rails-app/#reference","title":"Reference","text":"<p>some was copied from https://kubernetes-rails.com</p> <p>https://fly.io/ruby-dispatch/rails-on-docker/</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"apps/securitycam/","title":"Security Camera","text":""},{"location":"apps/securitycam/#config-for-pi0w-cam","title":"Config for PI0w Cam","text":"<pre><code>dtparam=audio=on\ndtoverlay=vc4-kms-v3d\nmax_framebuffers=2\ncamera_auto_detect=1\n</code></pre> <p>sudo modprobe bcm2835-v4l2</p>"},{"location":"apps/securitycam/#stream","title":"Stream","text":"<p>motion</p> <p>libcamerify motion</p> <p>sudo service motion status</p> <p>libcamera-vid -n --level 4.2 --denoise cdn_off -t 0 --inline --autofocus-mode continuous  --width 1920 --height 1080 --framerate 30 -o - | cvlc -vvv stream:///dev/stdin :demux=h264 --no-audio --sout '#rtp{sdp=http://:8554/x}'</p> <pre><code>$ libcamera-vid -t 0 -n --inline -o - | gst-launch-1.0 fdsrc fd=0 ! h264parse ! rtph264pay ! udpsink host=overseer port=5000\n</code></pre> <p>with cvlc</p> <pre><code>$ libcamera-vid -t 0 --inline -o - | cvlc stream:///dev/stdin --sout '#rtp{sdp=rtsp://:8554/stream1}' :demux=h264\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"apps/uptime-kuma/","title":"Uptime Kuma","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"apps/vpn/","title":"VPN","text":""},{"location":"apps/vpn/#setup-client","title":"Setup Client","text":"<p>https://docs.opnsense.org/manual/how-tos/sslvpn_client.html#create-a-server-certificate</p>"},{"location":"apps/vpn/#troubleshoot","title":"Troubleshoot","text":""},{"location":"apps/vpn/#renew-openvpn-certificate","title":"Renew OpenVPN Certificate","text":"<p>System &gt;&gt; Trust &gt;&gt; Certificates</p> <ul> <li>Create Certificate</li> <li>Create internal certificate</li> <li>Descriptive name: SSL VPN Server Certificate  <li>CA: SSL VPN CA (newest)</li> <li>Type: Server Certificate</li> <li>Length: 4096</li> <li>Digest: SHA512</li> <li>Common Name: SSL VPN Server Certificate"},{"location":"apps/vpn/#renew-client-certificate","title":"Renew Client Certificate","text":"<p>System &gt;&gt; Trust &gt;&gt; Certificates</p> <ul> <li>Create Certificate</li> <li>Create internal certificate</li> </ul> <p>System &gt;&gt; Access &gt;&gt; Users</p> <ul> <li>Edit user</li> <li>Select new Certificate</li> </ul> <p>VPN &gt;&gt; OpenVPN &gt;&gt; Client Export</p> <ul> <li>Download certifcate for user</li> <li> <p>Use new config in OpenVPN Client</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/argocd/","title":"Argo CD","text":""},{"location":"cluster-core/argocd/#applicationset","title":"ApplicationSet","text":""},{"location":"cluster-core/argocd/#ignore-diffs","title":"Ignore Diffs","text":"diff in applicationset<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: cluster-init\n  namespace: argocd-system\nspec:\n  generators:\n    ...\n  template:\n    spec:\n      project: default\n      ignoreDifferences:\n        - group: \"\"\n          kind: ConfigMap\n          jsonPointers:\n            - /data/oidc.config\n        - group: \"\"\n          kind: Secret\n          jsonPointers:\n            - /data/oidc.authentik.clientSecret\n      ...\n</code></pre>"},{"location":"cluster-core/argocd/#part-two-oidc-integration","title":"Part Two: OIDC integration","text":"<p>This part should follow after [Vault] and [Authentik] are up and running.</p>"},{"location":"cluster-core/argocd/#oidc-login","title":"OIDC Login","text":"<p>First you have to create a provider and application in authentik to get a client id and secret. Afterwards the oidc credentials can be saved in the Vault and mapped over a <code>SecretProviderClass</code>. (Do not forget to mount the vault volumes for the secret to work [./secrets-csi.md#volumes-in-a-chart])</p> <pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: vault-argocd\nspec:\n  provider: vault\n  parameters:\n    vaultAddress: \"http://vault.vault:8200\"\n    roleName: \"argocd-app\"\n    objects: |\n      - objectName: \"oidc-id\"\n        secretPath: \"kv-v2/data/framsburg/argocd/oidc\"\n        secretKey: \"client-id\"\n      - objectName: \"oidc-secret\"\n        secretPath: \"kv-v2/data/framsburg/argocd/oidc\"\n        secretKey: \"client-secret\"\n  secretObjects:\n    - data:\n        - key: oidc.authentik.clientId\n          objectName: oidc-id\n        - key: oidc.authentik.clientSecret\n          objectName: oidc-secret\n      secretName: oidc\n      type: Opaque\n      labels:\n        app.kubernetes.io/part-of: argocd # (1)!\n</code></pre> <ol> <li>Without this label the secret reference in the argocd ConfigMap will not work and complain about that the secret key can not be found.</li> </ol> <pre><code>argo-cd:\n  server:\n    ...\n    config:\n      url: https://argocd.framsburg.ch\n      oidc.config: | \n          name: Authentik\n          issuer: \"https://authentik.framsburg.ch/application/o/argocd/\"\n          clientID: \"c579d3195f85aeccaf1ecce35ef5501e023c2a6a\"\n          clientSecret: \"$oidc:oidc.authentik.clientSecret\"\n          requestedScopes: [\"openid\", \"profile\", \"email\"]\n          logoutURL: \"https://authentik.framsburg.ch/if/session-end/argocd/\"\n    rbacConfig:\n      policy.default: role:readonly\n      policy.csv: |\n          g, 'authentik Admins', role:admin\n    volumeMounts:\n      - name: 'secrets-store-inline'\n        mountPath: '/mnt/secrets-store'\n        readOnly: true\n    volumes:\n      - name: secrets-store-inline\n        csi:\n          driver: 'secrets-store.csi.k8s.io'\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: 'vault-argocd'\n</code></pre>"},{"location":"cluster-core/argocd/#rename-applicationset","title":"Rename Applicationset","text":"<p>Important: Remove Applicationset without cascade delete with the following two options.</p>"},{"location":"cluster-core/argocd/#variant-a-delete-over-cli","title":"Variant A: Delete over CLI","text":"<pre><code>$ kubectl delete ApplicationSet (NAME) --cascade=false\n</code></pre>"},{"location":"cluster-core/argocd/#variant-b-delete-over-gitops","title":"Variant B: Delete over GitOps","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: cluster-apps\n  namespace: argocd-system\nspec:\n  goTemplate: true\n  syncPolicy:\n    preserveResourcesOnDeletion: true\n</code></pre> <p>--&gt; Warning should occur of applicationsets are part of two applications (it's actually wrong, but apparently applicationsets are identified only by name)</p> <p>Remove old application set -</p> <p>Go through the following steps</p> <ol> <li>Add app for new applicationset</li> <li>Add preserve resources in old and new applicationset</li> <li>Remove old applicationset</li> <li>Add app to new applicationset one by one</li> </ol>"},{"location":"cluster-core/argocd/#switch-from-applicationset-to-app-of-apps","title":"Switch from Applicationset to App of Apps","text":"<p>To switch from applicationsets to an App of Apps setup we want to delete the superstructure of applicationsets and applications without removing the underlying resources like Pods or VolumeClaims. We do this in multiple steps</p>"},{"location":"cluster-core/argocd/#disable-cascading","title":"Disable cascading","text":"<p>Configure all application sets to preserve their resources. Otherwise the remove of the application set will trigger a deletion cascade to applications and pods, etc.</p> <p>You can do this by defining the following option on the applicationset spec (not the template!!):</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: cluster-apps\n  namespace: argocd-system\nspec:\n  syncPolicy:\n    preserveResourcesOnDeletion: true\n</code></pre>"},{"location":"cluster-core/argocd/#remove-applicationset","title":"Remove applicationset","text":"<p>Next step is to delete the applicationset. Make sure to remove them from the initial bootstrap setup otherwise they will be recreated again. This means remove the applicationset from <code>cluster-init/root</code>.</p> <p>After the sync the applicationset should be removed as well as the applications. If this is not the case you can manually remove them with the following command:</p> <pre><code>$ kubectl delete ApplicationSet &lt;NAME&gt; -n argocd-system --cascade=false\n</code></pre>"},{"location":"cluster-core/argocd/#add-new-app-of-apps","title":"Add new app of apps","text":"<p>This can be done anytime even before you start, if your new app of apps does not share the name with an existing application.</p> <p>This is quite straight forward. Add new app of apps to the bootstrap start under <code>apps-root-config/bootstrap/values.yaml</code>.</p>"},{"location":"cluster-core/argocd/#add-app-to-app-of-apps","title":"Add app to app of apps","text":"<p>First make sure the application you want to add is in the right folder. Don't move aka remove it from the old folder as the old application still exists and  would sync your change aka remove the app!</p> <p>Add your app to the new app of apps for example under <code>apps-root-config/applications/cluster-utility-apps.yaml</code></p> <p>This assumes the application name has not changed!. You can now remove the app files from the old folder.</p>"},{"location":"cluster-core/argocd/#github-webhook","title":"GitHub Webhook","text":"<p>This documentation describes how to setup a git webhook which notifies Argo CD on any source changes immedietly. Otherwise Argo CD will pull changes on a three minutes intervall. This follows the Argo CD documentation</p>"},{"location":"cluster-core/argocd/#create-secret-in-vault","title":"Create Secret in Vault","text":"<p>First create an arbitrary secret in the vault for GitHub to use and Argo CD to verify. This is important: As the callback URL must be publicly accessible (because GitHub is public) it opens an attack vector for a DDoS attack. As it not be changed frequently, go for something long ... 50 chars?</p> <p>Add it under <code>framsburg/argocd/github</code> and under key <code>webhook-secret</code>.</p> <p>Create the following external secret which provides this vault secret under the correct secret name. Remeber the k8s secret must be named <code>webhook.github.secret</code></p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/authentik/","title":"Authentik","text":""},{"location":"cluster-core/authentik/#rbac-integration","title":"RBAC Integration","text":"<p>For RBAC to work, you need to integrate it with an Identity Provider (IDP) or a system that does authentication and provides user roles. We are obviously going to use Authentik.</p> <p>For a serious production deployment, you should use an IDP which is not running inside the cluster. This is because if authentik fails to start up, you will not be able to log in to the cluster. Here we solve this by still keeping the bootstrap root certificate to be able to log into our cluster.</p> <p>After saving the admin certificate we are going to create a new OIDC client in Authentik. This will afterwards be used to configure the cluster to use Authentik as an OIDC provider. And the we use Authentik in kubectl and some apps to validate everything works.</p>"},{"location":"cluster-core/authentik/#keep-a-break-glass-kubeconfig-do-this-first","title":"Keep a break-glass kubeconfig (do this first)","text":"<p>k3s creates an admin kubeconfig that authenticates with a client certificate (doesn\u2019t depend on OIDC).</p> Inside a k3s server node<pre><code>sudo cat /etc/rancher/k3s/k3s.yaml\n</code></pre> <ul> <li>Save a copy to a secure vault/password manager. (The cluster hashicorp vault is not a good place for this)</li> <li>This is how you\u2019ll get in if Authentik is down or you misconfigure OIDC.</li> </ul>"},{"location":"cluster-core/authentik/#create-an-oidc-client-in-authentik","title":"Create an OIDC client in Authentik","text":"<ol> <li> <p>Provider: create an OpenID Connect (OIDC) provider.</p> <ul> <li> <p>Example settings:</p> <ul> <li>Issuer URL (you don\u2019t set this manually; it\u2019s derived from your Authentik base URL,   e.g. <code>https://auth.example.com/application/o/&lt;slug&gt;/</code>).</li> <li>Enable Authorization Code and/or Device Code (device flow is convenient for headless).</li> <li>Scopes: include at least <code>openid email profile</code>. </li> </ul> <p>You also need groups to show up in the ID token (see next bullet).</p> </li> </ul> </li> <li> <p>Expose \u201cgroups\u201d claim (so RBAC can use groups):</p> <ul> <li>Add/attach a User/Claim mapping that emits a claim named <code>groups</code> with an array of group names (or slugs).</li> <li>Resulting ID token should contain e.g.:   <pre><code>{ \"email\": \"alice@example.com\", \"groups\": [\"devs\",\"k8s-admins\"] }\n</code></pre></li> <li>(Exact steps depend on how you\u2019ve modeled groups in Authentik; the key is: claim name must be <code>groups</code>.)</li> </ul> </li> <li> <p>Client: create an OAuth2/OIDC Application bound to that provider.</p> </li> <li>Note the Client ID and Client Secret.</li> <li>Allowed redirect URIs are not used by the API server, but are used by your kubectl OIDC plugin (see step 3). If using the kubectl device flow, you typically don\u2019t need to pre-register redirect URIs.</li> </ol>"},{"location":"cluster-core/authentik/#configure-k3s-api-server-to-trust-authentik-oidc","title":"Configure k3s API server to trust Authentik (OIDC)","text":"<p>The k3s flags can be found in the k3s-server service files (see Ansible playbooks).</p> k3s-servcer.service<pre><code># (merge with your existing config; do NOT duplicate keys)\n\n--kube-apiserver-arg=oidc-issuer-url=https://auth.example.com/application/o/&lt;your-provider-slug&gt;/\n--kube-apiserver-arg=oidc-client-id=kubernetes\n# If your app uses a client secret, include it:\n# --kube-apiserver-arg=oidc-client-secret=&lt;YOUR_CLIENT_SECRET&gt;\n--kube-apiserver-arg=oidc-username-claim=email\n--kube-apiserver-arg=oidc-groups-claim=groups\n# Optional but recommended to avoid collisions with other auth methods:\n--kube-apiserver-arg=oidc-username-prefix=oidc:\n--kube-apiserver-arg=oidc-groups-prefix=oidc:\n# If Authentik uses a private CA or your own TLS, point k3s at the CA bundle:\n# --kube-apiserver-arg=oidc-ca-file=/var/lib/rancher/k3s/server/tls/oidc-ca.crt\n</code></pre> <p>Notes: * Store the <code>oidc-client-id</code> and <code>oidc-client-secret</code> in the ansible vault if you use Ansible to deploy k3s. * <code>oidc-issuer-url</code> must exactly match the issuer that signs tokens (including trailing slash if that\u2019s how the   discovery doc advertises it). * If Authentik uses a non-public CA, drop the CA chain at oidc-ca-file and ensure k3s can read it.</p> <p>Restart k3s to apply: <pre><code>sudo systemctl restart k3s-server\n</code></pre></p>"},{"location":"cluster-core/authentik/#configure-kubectl-to-use-authentik-oidc","title":"Configure kubectl to use Authentik OIDC","text":"<p>You can use the <code>kubectl oidc-login</code> plugin to authenticate with Authentik. Install it as follows:</p> <pre><code># Install the kubectl oidc-login plugin\nbrew install oidc-login\n</code></pre> <p>You can test the OIDC parameters by running the following command:</p> <pre><code>kubectl oidc-login setup \\\n  --oidc-issuer-url=ISSUER_URL \\\n  --oidc-client-id=YOUR_CLIENT_ID \\\n  --oidc-client-secret=YOUR_CLIENT_SECRET\n</code></pre> <p>This command will not only validate your OIDC parameters but also provides the <code>kubectl config set-credentials</code> command you need to add oidc to your kubeconfig.</p> <p>It should output something like this:</p> <p>```bash\" kubectl config set-credentials oidc \\ --exec-api-version=client.authentication.k8s.io/v1 \\ --exec-interactive-mode=Never \\ --exec-command=kubectl \\ --exec-arg=oidc-login \\ --exec-arg=get-token \\ --exec-arg=\"--oidc-issuer-url=ISSUER_URL\" \\ --exec-arg=\"--oidc-client-id=YOUR_CLIENT_ID\" \\ --exec-arg=\"--oidc-client-secret=YOUR_CLIENT_SECRET\" \\ --exec-arg=\"--oidc-extra-scope=email,profile,groups\" <pre><code>You can now run the following command to check if your kubeconfig is set up correctly:\n\n```bash\nkubectl --user=oidc auth whoami\n</code></pre></p> <p>You should see your email address or username from Authentik including groups.</p> <p>If everything works, you can permanently add the oidc user to your kubeconfig:</p> <pre><code>kubectl config set-context --current --user=oidc\n</code></pre>"},{"location":"cluster-core/authentik/#test-oidc-authentication-with-kubectl","title":"Test OIDC authentication with kubectl","text":"<p>Bind RBAC to your OIDC groups (or users) Create a simple test binding. For example, grant view in a dev namespace to group oidc:devs (note the oidc: prefix we configured).</p> <p>view-to-devs.yaml<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: view-to-devs\n  namespace: dev\nsubjects:\n- kind: Group\n  name: oidc:devs      # groups-prefix + your groups claim value\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: view\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> Apply it: <pre><code>  kubectl apply -f view-to-devs.yaml\n</code></pre></p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/cert-manager/","title":"Cert-Manager","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/longhorn/","title":"Longhorn","text":""},{"location":"cluster-core/longhorn/#setup-minio-for-backup","title":"Setup Minio for Backup","text":"<p>Use the minio cli <code>mc</code> which has an alias called <code>myminio</code></p> Create minio bucket<pre><code>$ mc mb myminio/longhorn\n$ mc mb myminio/longhorn/backups\n</code></pre> Create user with policy<pre><code>$ mc admin user add myminio longhorn mypass\n$ cat &gt; ./longhorn-backups-policy.json &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n      \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:PutBucketPolicy\",\n        \"s3:GetBucketPolicy\",\n        \"s3:DeleteBucketPolicy\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::longhorn\"\n      ],\n      \"Sid\": \"\"\n    },\n    {\n      \"Action\": [\n        \"s3:AbortMultipartUpload\",\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:PutObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::longhorn/*\"\n      ],\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n\n$ mc admin policy create myminio longhorn-backups-policy ./longhorn-backups-policy.json\n\n$ mc admin policy attach myminio longhorn-backups-policy --user longhorn\n</code></pre>"},{"location":"cluster-core/longhorn/#define-backup-target-in-longhorn","title":"Define backup target in longhorn","text":"longhorn values.yaml<pre><code>longhorn:\n  defaultSettings:\n    backupTarget: 's3://longhorn@us-east-1/backups'\n    backupTargetCredentialSecret: minio-secret\n...\n</code></pre>"},{"location":"cluster-core/longhorn/#copy-from-one-volume-to-another","title":"Copy from one Volume to another","text":"copy-job.yaml<pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: default  # namespace where the PVC's exist\n  name: volume-migration\nspec:\n  completions: 1\n  parallelism: 1\n  backoffLimit: 3\n  template:\n    metadata:\n      name: volume-migration\n      labels:\n        name: volume-migration\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: volume-migration\n          image: ubuntu:xenial\n          tty: true\n          command: [ \"/bin/sh\" ]\n          args: [ \"-c\", \"cp -r -v /mnt/old /mnt/new\" ]\n          volumeMounts:\n            - name: old-vol\n              mountPath: /mnt/old\n            - name: new-vol\n              mountPath: /mnt/new\n      volumes:\n        - name: old-vol\n          persistentVolumeClaim:\n            claimName: data-source-pvc # change to data source PVC\n        - name: new-vol\n          persistentVolumeClaim:\n            claimName: data-target-pvc # change to data target PVC\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/metallb/","title":"MetalLB","text":"<p><code>kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\"$(openssl rand -base64 128)\" --dry-run -o yaml | kubectl apply -f -</code></p> <p>use ansible vault for secretkey.</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/monitoring/","title":"Monitoring with Prometheus Stack","text":""},{"location":"cluster-core/monitoring/#part-two-oidc-integration","title":"Part Two: OIDC Integration","text":"<p>This part should follow after [Vault] and [Authentik] are up and running.</p>"},{"location":"cluster-core/monitoring/#create-application-in-authentik","title":"Create Application in Authentik","text":"<p>Create a new OIDC provider in authentik with Redirect URIs/Origin pointing to [https://grafana.framsburg.ch]</p> <p>Afterwards create a new application which uses the before created provider. Don't forget to create bindings for uses or groups. You will need the following three informations out of authentik in the next steps.</p> <ul> <li>OpenID Configuration Issuer</li> <li>Client ID</li> <li>Client Secret</li> </ul>"},{"location":"cluster-core/monitoring/#add-secrets-to-vault","title":"Add secrets to vault","text":"<p>To use the vault CLI use the folling command:</p> <pre><code>$ kubectl exec -it vault-0 -n vault -- /bin/sh\n</code></pre> <p>Add the Client ID and the Client Secret as values to the vault:</p> <p>Create the secrets with:</p> <pre><code>$ vault kv put kv-v2/framsburg/grafana/oidc client-id=\"someID\" client-secret=\"someSecret\"\n</code></pre> <p>Create a policy to access the secrets: <pre><code>$ vault policy write grafana-app - &lt;&lt;EOF\npath \"kv-v2/data/framsburg/grafana/*\" {\n  capabilities = [\"read\", \"list\"]\n}\nEOF\n</code></pre></p> <p>Note</p> <p>Please be aware of the added <code>/data/</code>. This is not a typo but something the Vault expects when referencing this secret. It is not displayed in the UI either.</p> <p>And as last step create a role which maps the k8s service account with the policy:</p> <pre><code>$ vault write auth/kubernetes/role/grafana-app \\\n    bound_service_account_names=monitoring-stack-grafana \\\n    bound_service_account_namespaces=monitoring-stack \\\n    policies=grafana-app \\\n    ttl=20m\n</code></pre>"},{"location":"cluster-core/monitoring/#secret-class","title":"Secret Class","text":"<p>Create a SecretProviderClass in the templates</p> cluster-critical/monitoring-stack/templates/spc.yaml<pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: vault-grafana\nspec:\n  provider: vault\n  parameters:\n    vaultAddress: \"http://vault.vault:8200\"\n    roleName: \"grafana-app\"\n    objects: |\n      - objectName: \"oidc-id\"\n        secretPath: \"kv-v2/data/framsburg/grafana/oidc\"\n        secretKey: \"client-id\"\n      - objectName: \"oidc-secret\"\n        secretPath: \"kv-v2/data/framsburg/grafana/oidc\"\n        secretKey: \"client-secret\"\n  secretObjects:\n    - data:\n        - key: clientId\n          objectName: oidc-id\n        - key: clientSecret\n          objectName: oidc-secret\n      secretName: oidc\n      type: Opaque\n</code></pre>"},{"location":"cluster-core/monitoring/#add-volumes-in-a-chart","title":"Add Volumes in a Chart","text":"<p>The Grafana-Chart doesn't allow CSI volumes to be added to the normal volume list. But it has a special value <code>extraSecretMount</code> for those volumes which thankfully even combines the volume and mount entry into one.</p> cluster-critical/monitoring-stack/values.yaml<pre><code>kube-prometheus-stack:\n  grafana:\n...\n    extraSecretMounts:\n      - name: 'secrets-store-inline'\n        mountPath: '/mnt/secrets-store'\n        readOnly: true\n        csi:\n          driver: secrets-store.csi.k8s.io\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: vault-grafana\n</code></pre>"},{"location":"cluster-core/monitoring/#set-environment-variables-incl-secrets","title":"Set environment variables incl secrets","text":"cluster-critical/monitoring-stack/values.yaml<pre><code>kube-prometheus-stack:\n  grafana:\n...\n    env:\n      GF_AUTH_GENERIC_OAUTH_ENABLED: \"true\"\n      GF_AUTH_GENERIC_OAUTH_NAME: \"authentik\"\n      GF_AUTH_GENERIC_OAUTH_SCOPES: \"openid profile email\"\n      GF_AUTH_GENERIC_OAUTH_AUTH_URL: \"https://authentik.framsburg.ch/application/o/authorize/\"\n      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: \"https://authentik.framsburg.ch/application/o/token/\"\n      GF_AUTH_GENERIC_OAUTH_API_URL: \"https://authentik.framsburg.ch/application/o/userinfo/\"\n      GF_AUTH_SIGNOUT_REDIRECT_URL: \"https://authentik.framsburg.ch/application/o/grafana/end-session/\"\n      # Optionally enable auto-login (bypasses Grafana login screen)\n      # GF_AUTH_OAUTH_AUTO_LOGIN: \"true\"\n      # Optionally map user groups to Grafana roles\n      # GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: \"contains(groups[*], 'Grafana Admins') &amp;&amp; 'Admin' || contains(groups[*], 'Grafana Editors') &amp;&amp; 'Editor' || 'Viewer'\"\n\n    envValueFrom:\n      GF_AUTH_GENERIC_OAUTH_CLIENT_ID:\n        secretKeyRef:\n          name: oidc\n          key: clientId\n\n      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:\n        secretKeyRef:\n          name: oidc\n          key: clientSecret\n</code></pre>"},{"location":"cluster-core/monitoring/#loki","title":"Loki","text":"<p>Opnsense is a very interessting source for logs, but two steps are necessary. The syslog source has to be configures for opnsense to forward the syslog entries7. The second step is to parse the syslog entries and turn them into sensible logfmt messages with labels4.</p> <p>You can add as a bonus the geoip.</p>"},{"location":"cluster-core/monitoring/#custom-dashboard","title":"Custom Dashboard","text":"<p>https://www.mostlychris.com/keeping-tabs-on-your-network/</p>"},{"location":"cluster-core/monitoring/#loki-k8s-events","title":"Loki k8s events","text":"<ul> <li>https://medium.com/@lysetskyyv/collecting-and-visualizing-kubernetes-events-with-loki-and-kubernetes-event-exporter-8f764b78185c</li> <li>https://github.com/resmoio/kubernetes-event-exporter</li> <li>https://grafana.com/docs/agent/latest/flow/reference/components/loki.source.kubernetes_events/</li> <li></li> </ul>"},{"location":"cluster-core/monitoring/#references","title":"References","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/traefik/","title":"Traefik","text":"<p>Use two traefik controllers for internal and external network.</p> <p>Issue with only one service <code>api@internal</code> with will mess up the WebUI: The web ui will display for both controllers all services and routes.</p>"},{"location":"cluster-core/traefik/#second-ingress-controller-for-external-access","title":"Second ingress controller for external access","text":"<p>As alternative to a second traefik controller, the NGINX ingress controller could be used:</p> <p>There are two option: ingress nginx controller (community) and nginx ingress controller (from nginx). I go for the latter as it seems more active at the moment.</p>"},{"location":"cluster-core/traefik/#references","title":"References","text":"<ul> <li>https://github.com/traefik/traefik-helm-chart/blob/master/EXAMPLES.md</li> <li>https://doc.traefik.io/traefik/routing/providers/kubernetes-crd/#kind-tlsoption</li> <li>https://doc.traefik.io/traefik/routing/routers/</li> <li> <p>https://doc.traefik.io/traefik/https/acme/#using-letsencrypt-with-kubernetes</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"cluster-core/vault/","title":"Hashicorp Vault","text":"<p>For operating the Vault inside K8S it is a good idea to use the Banzaicloud Vault-Operator. It automates some the integration and HA tasks.</p>"},{"location":"cluster-core/vault/#install-the-operator","title":"Install the operator","text":"<p>Define a new Chart with a dependencies to the Vault-Operator Chart in the app of apps for the vault-operator with the following values:</p> <pre><code>vault-operator:\n  crdAnnotations:\n      \"helm.sh/hook\": crd-install\n</code></pre>"},{"location":"cluster-core/vault/#install-vault","title":"Install Vault","text":"<p>Define a new empty Chart with the following templates inside:</p> <p>```yaml:rbac.yaml kind: ServiceAccount apiVersion: v1 metadata:   name: vault</p> <p>kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:   name: vault rules:   - apiGroups: [\"\"]     resources: [\"secrets\"]     verbs: [\"*\"]   - apiGroups: [\"\"]     resources: [\"pods\"]     verbs: [\"get\", \"update\", \"patch\"]</p> <p>kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:   name: vault roleRef:   kind: Role   name: vault   apiGroup: rbac.authorization.k8s.io subjects:   - kind: ServiceAccount     name: vault</p>"},{"location":"cluster-core/vault/#this-binding-allows-the-deployed-vault-instance-to-authenticate-clients","title":"This binding allows the deployed Vault instance to authenticate clients","text":""},{"location":"cluster-core/vault/#through-kubernetes-serviceaccounts-if-configured-so","title":"through Kubernetes ServiceAccounts (if configured so).","text":"<p>apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: vault-auth-delegator roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: system:auth-delegator subjects:   - kind: ServiceAccount     name: vault     namespace: default <pre><code>## Use secrets\n\nYou always have to map the secrets in different ways. You can find a detailed\ndescription on [banzais website](https://banzaicloud.com/docs/bank-vaults/mutating-webhook/)\n\n### As envrionment variable\n\nPod should have the following annotations:\n\n```yaml\nannotations:\n  vault.security.banzaicloud.io/vault-addr: \"http://vault.vault.svc:8200\"\n  vault.security.banzaicloud.io/vault-path: \"kubernetes\"\n  vault.security.banzaicloud.io/vault-role: \"test\"\n  vault.security.banzaicloud.io/vault-skip-verify: \"true\"\n</code></pre></p> <p>You should adapt the role to the corresponding role you want to use. You can then use secrets in environment variables like this:</p> <pre><code>env:\n  - name: GITHUB_CLIENT_ID\n    value: vault:secret/data/framsburg/test#github_token\n</code></pre>"},{"location":"cluster-core/vault/#as-secret","title":"As secret","text":"<p>The approach with secrets looks quite similar. The main difference is, that you have to provide the path to the secret base64 encoded.</p> <pre><code>$ echo -n vault:secret/data/framsburg/test#github_token | base64\ndmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu\n</code></pre> <p>Then prepare the secret accodringly:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: some-secret\ndata:\n  GITHUB_CLIENT_ID: dmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu\ntype: Opaque\n</code></pre>"},{"location":"cluster-core/vault/#inline","title":"Inline","text":"<p>Instead of environment variables or secrets you can use the vault key reference anywhere in resources and the webhook will replace it with the secret.</p>"},{"location":"cluster-core/vault/#monitoring","title":"Monitoring","text":"<ul> <li>https://github.com/hashicorp/vault/issues/13978</li> <li>https://developer.hashicorp.com/vault/tutorials/monitoring/monitor-telemetry-grafana-prometheus</li> <li> <p>https://developer.hashicorp.com/vault/tutorials/monitoring/usage-metrics</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"getting-started/architecture/","title":"Architecture","text":"<p>This document describes the overall architecture of the k3s homelab cluster, explaining the key design decisions and how components interact.</p>"},{"location":"getting-started/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"External\"\n        Internet[Internet/Users]\n        DNS[CloudFlare DNS]\n    end\n\n    subgraph \"Edge\"\n        Router[OPNsense/Firewall]\n        MetalLB[MetalLB LoadBalancer]\n    end\n\n    subgraph \"Ingress\"\n        Traefik[Traefik Ingress]\n        CertManager[cert-manager]\n    end\n\n    subgraph \"Security\"\n        Authentik[Authentik SSO]\n        Vault[HashiCorp Vault]\n    end\n\n    subgraph \"GitOps\"\n        ArgoCD[ArgoCD]\n        GitHub[GitHub Repo]\n    end\n\n    subgraph \"Control Plane\"\n        K3s[\"k3s Servers (3x)&lt;br/&gt;HA Control Plane\"]\n    end\n\n    subgraph \"Workers\"\n        RaspberryPi[\"Raspberry Pi Workers (4x)\"]\n        ProxmoxVMs[\"Proxmox VMs\"]\n    end\n\n    subgraph \"Storage\"\n        Longhorn[Longhorn]\n        TrueNAS[TrueNAS Backups]\n    end\n\n    subgraph \"Platform\"\n        Monitoring[Prometheus/Grafana]\n        PostgreSQL[CloudNativePG]\n    end\n\n    subgraph \"Apps\"\n        Applications[Applications]\n    end\n\n    Internet --&gt; DNS\n    DNS --&gt; Router\n    Router --&gt; MetalLB\n    MetalLB --&gt; Traefik\n    Traefik --&gt; Authentik\n    Traefik --&gt; Applications\n    CertManager --&gt; Traefik\n    Vault --&gt; Applications\n    GitHub --&gt; ArgoCD\n    ArgoCD --&gt; Platform\n    ArgoCD --&gt; Applications\n    Applications --&gt; Longhorn\n    Applications --&gt; PostgreSQL\n    Longhorn --&gt; TrueNAS</code></pre>"},{"location":"getting-started/architecture/#design-principles","title":"Design Principles","text":""},{"location":"getting-started/architecture/#1-everything-as-code-gitops","title":"1. Everything as Code (GitOps)","text":"<p>Why: Reproducibility, version control, and disaster recovery</p> <ul> <li>All cluster configuration is stored in Git</li> <li>ArgoCD continuously reconciles Git state with cluster state</li> <li>No manual <code>kubectl apply</code> commands in production</li> <li>Changes are reviewed via pull requests</li> </ul>"},{"location":"getting-started/architecture/#2-security-by-default","title":"2. Security by Default","text":"<p>Why: Protect services and data, practice production-ready security</p> <ul> <li>All external services require HTTPS with valid certificates</li> <li>SSO/OIDC authentication via Authentik for supported apps</li> <li>Secrets stored in HashiCorp Vault, never in Git</li> <li>Network segmentation (public/private VLANs)</li> <li>Regular security updates via Ansible automation</li> </ul>"},{"location":"getting-started/architecture/#3-high-availability-for-critical-components","title":"3. High Availability for Critical Components","text":"<p>Why: Minimize downtime, practice HA patterns</p> <ul> <li>3-node control plane for k3s (etcd quorum)</li> <li>Longhorn replicates storage across nodes</li> <li>Multiple worker nodes for workload distribution</li> <li>TrueNAS for reliable backup storage</li> </ul>"},{"location":"getting-started/architecture/#4-observable-and-debuggable","title":"4. Observable and Debuggable","text":"<p>Why: Quickly identify and resolve issues</p> <ul> <li>Prometheus metrics from all components</li> <li>Grafana dashboards for visualization</li> <li>Loki for centralized logging</li> <li>Alerting for critical issues</li> </ul>"},{"location":"getting-started/architecture/#network-architecture","title":"Network Architecture","text":""},{"location":"getting-started/architecture/#vlan-segmentation","title":"VLAN Segmentation","text":"<pre><code>graph TB\n    Router[OPNsense Router/Firewall]\n    PublicVLAN[Public VLAN&lt;br/&gt;Internet-facing services]\n    PrivateVLAN[Private VLAN&lt;br/&gt;Internal services]\n    MetalLB[MetalLB LoadBalancer]\n    Traefik[Traefik Ingress]\n\n    Router --&gt; PublicVLAN\n    Router --&gt; PrivateVLAN\n    PublicVLAN --&gt; MetalLB\n    PrivateVLAN --&gt; MetalLB\n    MetalLB --&gt; Traefik\n\n    style PublicVLAN fill:#ffcccc\n    style PrivateVLAN fill:#ccffcc</code></pre> <p>Public VLAN: Internet-facing services (limited) Private VLAN: Internal services, more relaxed policies</p>"},{"location":"getting-started/architecture/#metallb-configuration","title":"MetalLB Configuration","text":"<ul> <li>Mode: Layer 2 (ARP)</li> <li>IP Pools: Separate ranges for public/private</li> <li>Benefits: Native LoadBalancer support on bare metal</li> </ul>"},{"location":"getting-started/architecture/#traefik-ingress","title":"Traefik Ingress","text":"<p>Two separate Traefik instances:</p> <ol> <li>Traefik Public: Handles external traffic</li> <li>HTTPS with automatic certificate management</li> <li>Rate limiting</li> <li> <p>Auth middleware (Authentik forward auth)</p> </li> <li> <p>Traefik Private: Handles internal traffic</p> </li> <li>Access to admin dashboards</li> <li>Internal APIs</li> <li>Less restrictive policies</li> </ol> <p>Why Two?: Separation of concerns, different security policies, easier debugging</p>"},{"location":"getting-started/architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"getting-started/architecture/#longhorn-primary","title":"Longhorn (Primary)","text":"<ul> <li>Type: Distributed block storage</li> <li>Replication: 3 replicas for important data</li> <li>Backend: NVMe drives on worker nodes</li> <li>Backup: Automated backups to MinIO/TrueNAS</li> <li>Use Cases: Databases, application state, persistent volumes</li> </ul>"},{"location":"getting-started/architecture/#truenas","title":"TrueNAS","text":"<ul> <li>Type: External NAS with ZFS</li> <li>Use Cases: </li> <li>Backup target for Longhorn</li> <li>NFS shares for large media files</li> <li>Long-term archival storage</li> </ul>"},{"location":"getting-started/architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"getting-started/architecture/#authentication-flow","title":"Authentication Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Traefik\n    participant Authentik\n    participant App\n    participant Vault\n\n    User-&gt;&gt;Traefik: Access App\n    Traefik-&gt;&gt;Authentik: Forward Auth Request\n    alt Not Authenticated\n        Authentik-&gt;&gt;User: Redirect to Login\n        User-&gt;&gt;Authentik: Login\n        Authentik-&gt;&gt;User: Set Session Cookie\n    end\n    Authentik-&gt;&gt;Traefik: Auth OK + Headers\n    Traefik-&gt;&gt;App: Forward Request + User Info\n    App-&gt;&gt;Vault: Fetch Secrets (if needed)\n    Vault-&gt;&gt;App: Return Secrets\n    App-&gt;&gt;User: Response</code></pre>"},{"location":"getting-started/architecture/#secrets-management","title":"Secrets Management","text":"<p>Three-Layer Approach:</p> <ol> <li>Vault: Central secrets storage</li> <li>External Secrets Operator: Syncs secrets from Vault to k8s Secrets</li> <li>Secrets Store CSI: Mounts secrets as files (for apps that need file-based secrets)</li> </ol> <p>Why Not Just k8s Secrets?:  - Vault provides audit logging - Secret rotation - Fine-grained access control - External secret source (survives cluster rebuild)</p>"},{"location":"getting-started/architecture/#gitops-workflow","title":"GitOps Workflow","text":"<pre><code>graph LR\n    Dev[Developer] --&gt;|Git Push| GitHub[GitHub Repo]\n    GitHub --&gt;|Webhook| ArgoCD[ArgoCD]\n    ArgoCD --&gt;|Sync| K8s[Kubernetes Cluster]\n    ArgoCD --&gt;|Monitor| GitHub\n    K8s --&gt;|Status| ArgoCD</code></pre>"},{"location":"getting-started/architecture/#repository-structure","title":"Repository Structure","text":"<pre><code>k3s-cluster-infra-apps/\n\u251c\u2500\u2500 apps-root-config/          # ArgoCD ApplicationSets\n\u2502   \u2514\u2500\u2500 applications/          # App of Apps pattern\n\u251c\u2500\u2500 cluster-init-apps/         # Bootstrap (MetalLB, cert-manager)\n\u251c\u2500\u2500 cluster-critical-apps/     # Core infra (Traefik, Longhorn, etc)\n\u251c\u2500\u2500 cluster-platform-apps/     # Platform services\n\u2514\u2500\u2500 cluster-*-apps/            # Application groups\n</code></pre>"},{"location":"getting-started/architecture/#deployment-layers","title":"Deployment Layers","text":"<ol> <li>Layer 0 (Manual): k3s cluster + ArgoCD installation</li> <li>Layer 1 (ArgoCD): cluster-init-apps (MetalLB, cert-manager)</li> <li>Layer 2 (ArgoCD): cluster-critical-apps (Traefik, Longhorn, Vault)</li> <li>Layer 3 (ArgoCD): cluster-platform-apps (Monitoring, databases)</li> <li>Layer 4 (ArgoCD): Applications</li> </ol>"},{"location":"getting-started/architecture/#application-dependencies","title":"Application Dependencies","text":"<pre><code>graph TD\n    subgraph \"Layer 1: cluster-init-apps\"\n        CertManager[cert-manager]\n    end\n\n    subgraph \"Layer 2: cluster-critical-apps\"\n        Traefik[traefik]\n        Longhorn[longhorn]\n        MonitoringStack[monitoring-stack]\n        Loki[loki]\n        Promtail[promtail]\n    end\n\n    subgraph \"Layer 3: cluster-platform-apps\"\n        Vault[vault]\n        Authentik[authentik]\n    end\n\n    CertManager --&gt; MonitoringStack\n    CertManager --&gt; Vault\n    CertManager --&gt; Authentik\n    CertManager --&gt; Traefik\n    MonitoringStack -.-&gt;|Certificate CRD| CertManager\n    Vault -.-&gt;|OIDC| Authentik\n    Authentik -.-&gt;|PostgreSQL Secret| Vault\n    Traefik -.-&gt;|DigitalOcean Token| Vault\n    Longhorn -.-&gt;|OIDC| Authentik\n    MonitoringStack -.-&gt;|OIDC Secret| Vault\n    Promtail -.-&gt;|GeoIP License Key| Vault\n    Vault -.-&gt;|PVCs| Longhorn\n    MonitoringStack -.-&gt;|PVCs| Longhorn\n    Loki -.-&gt;|PVCs| Longhorn\n    Authentik -.-&gt;|PVCs| Longhorn\n</code></pre>"},{"location":"getting-started/architecture/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"getting-started/architecture/#metrics-stack","title":"Metrics Stack","text":"<ul> <li>Prometheus: Scrapes metrics from all components</li> <li>Grafana: Visualizes metrics with dashboards</li> <li>AlertManager: Routes alerts (future: to Slack/PagerDuty)</li> </ul>"},{"location":"getting-started/architecture/#logging-stack","title":"Logging Stack","text":"<ul> <li>Loki: Log aggregation</li> <li>Promtail: Log shipping agent on each node</li> <li>Grafana: Log viewing and querying</li> </ul>"},{"location":"getting-started/architecture/#disaster-recovery-strategy","title":"Disaster Recovery Strategy","text":""},{"location":"getting-started/architecture/#backup-targets","title":"Backup Targets","text":"<ol> <li>Longhorn Volumes: Automated backups to MinIO/TrueNAS</li> <li>Vault Data: Encrypted backup to external storage</li> <li>ArgoCD Configuration: In Git (inherently backed up)</li> <li>PostgreSQL Databases: CNPG automated backups</li> </ol>"},{"location":"getting-started/architecture/#recovery-scenarios","title":"Recovery Scenarios","text":"<p>Scenario 1: Single Node Failure - Workloads automatically reschedule to healthy nodes - Storage remains available (Longhorn replication) - RTO: Minutes to hours (automatic)</p> <p>Scenario 2: Complete Cluster Loss - Rebuild cluster with Ansible - Deploy ArgoCD (manual) - ArgoCD syncs all apps from Git - Restore Longhorn volumes from backup - Restore Vault from backup - RTO: 4-8 hours</p>"},{"location":"getting-started/architecture/#technology-choices-explained","title":"Technology Choices Explained","text":""},{"location":"getting-started/architecture/#why-k3s-over-k8s","title":"Why k3s Over k8s?","text":"<ul> <li>Lightweight (runs well on Raspberry Pi)</li> <li>Single binary, easy to install</li> <li>Built-in components (though we replace some)</li> <li>Perfect for edge/homelab</li> </ul>"},{"location":"getting-started/architecture/#why-traefik-over-nginx-ingress","title":"Why Traefik Over NGINX Ingress?","text":"<ul> <li>Native Kubernetes CRDs (IngressRoute)</li> <li>Automatic service discovery</li> <li>Better middleware support</li> <li>Modern architecture</li> </ul>"},{"location":"getting-started/architecture/#why-longhorn-over-rookceph","title":"Why Longhorn Over Rook/Ceph?","text":"<ul> <li>Simpler to operate</li> <li>Lower resource overhead</li> <li>Great UI</li> <li>Sufficient for homelab scale</li> </ul>"},{"location":"getting-started/architecture/#why-argocd-over-flux","title":"Why ArgoCD Over Flux?","text":"<ul> <li>Mature UI for visualization</li> <li>Easier to debug sync issues</li> <li>ApplicationSet pattern powerful</li> <li>Better RBAC for multi-user (future)</li> </ul>"},{"location":"getting-started/architecture/#why-cloudnativepg-over-zalando","title":"Why CloudNativePG Over Zalando?","text":"<ul> <li>Active development</li> <li>Better backup integration</li> <li>Simpler operator</li> <li>EDB backing (PostgreSQL core team)</li> </ul>"},{"location":"getting-started/architecture/#hardware-configuration","title":"Hardware Configuration","text":""},{"location":"getting-started/architecture/#control-plane-3-nodes","title":"Control Plane (3 nodes)","text":"<ul> <li>Hardware: Raspberry Pi 4 (8GB RAM)</li> <li>Role: k3s control plane + etcd</li> <li>Storage: USB boot only</li> <li>HA: 3-node quorum</li> </ul>"},{"location":"getting-started/architecture/#worker-nodes-7-nodes","title":"Worker Nodes (7 nodes)","text":"<ul> <li>4x Raspberry Pi 4 (8GB RAM) with external NVMe</li> <li>3x Proxmox VMs (varies, 64GB RAM hosts)</li> <li>Role: Application workloads + Longhorn storage</li> </ul>"},{"location":"getting-started/architecture/#external-storage","title":"External Storage","text":"<ul> <li>TrueNAS HL15: 6x HDDs (~40TB)</li> <li>Use: Longhorn backups, large file storage</li> </ul>"},{"location":"getting-started/architecture/#scaling-considerations","title":"Scaling Considerations","text":""},{"location":"getting-started/architecture/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Worker Nodes:  - Easy to add new nodes via Ansible - Longhorn automatically uses new storage - Workloads distribute automatically</p> <p>Applications:  - Increase replica count in Helm values - ArgoCD syncs changes - HPA (Horizontal Pod Autoscaler) for automatic scaling</p>"},{"location":"getting-started/architecture/#current-capacity","title":"Current Capacity","text":"<ul> <li>Control Plane: 3 nodes (adequate for homelab scale)</li> <li>Worker Nodes: 7 nodes (can add more)</li> <li>Storage: ~2TB usable (Longhorn), 40TB (TrueNAS)</li> <li>Can Support: 100+ application pods comfortably</li> </ul>"},{"location":"getting-started/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"getting-started/architecture/#resource-usage-cluster-overhead","title":"Resource Usage (Cluster Overhead)","text":"<ul> <li>Control Plane: ~2GB RAM per node</li> <li>Longhorn: ~200MB per node + storage overhead</li> <li>Monitoring Stack: ~2GB RAM</li> <li>ArgoCD: ~500MB RAM</li> <li>Total Overhead: ~6-8GB RAM</li> </ul>"},{"location":"getting-started/architecture/#network-performance","title":"Network Performance","text":"<ul> <li>Internal: ~1Gbps (Raspberry Pi limitation)</li> <li>External: Limited by internet connection</li> <li>Storage: NVMe provides low latency (&lt;10ms)</li> </ul>"},{"location":"getting-started/architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"getting-started/architecture/#attack-surface","title":"Attack Surface","text":"<p>External: - Only necessary services exposed via Traefik public - All HTTPS with valid certificates - Rate limiting on public ingress</p> <p>Internal: - Private VLAN for admin dashboards - Authentik SSO for authentication - RBAC for service accounts</p>"},{"location":"getting-started/architecture/#secrets-handling","title":"Secrets Handling","text":"<ul> <li>Never committed to Git (gitignored)</li> <li>Stored in Vault with encryption at rest</li> <li>Accessed via External Secrets or CSI driver</li> <li>Rotated periodically (manual for now)</li> </ul>"},{"location":"getting-started/architecture/#future-improvements","title":"Future Improvements","text":"<ul> <li> Automated certificate rotation for Vault</li> <li> Network policies for pod-to-pod security</li> <li> Multi-cluster federation (dev/prod split)</li> <li> Automated disaster recovery testing</li> <li> GPU support for AI workloads</li> <li> <p> Service mesh (Linkerd) for advanced traffic management</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"getting-started/overview/","title":"Overview","text":"<p>Welcome to the K3s Homelab GitOps Stack documentation. This documentation serves as both a reference guide and troubleshooting manual for a production-ready, GitOps-managed Kubernetes homelab cluster.</p>"},{"location":"getting-started/overview/#purpose-of-this-documentation","title":"Purpose of This Documentation","text":"<p>This documentation has three primary goals, in order of priority:</p> <ol> <li>Troubleshooting &amp; Operations - Quickly find solutions to common problems and execute maintenance tasks</li> <li>Architecture &amp; Setup - Understand how the cluster is configured and why certain decisions were made</li> <li>Replication Guide - Follow step-by-step instructions to build a similar setup</li> </ol>"},{"location":"getting-started/overview/#who-is-this-for","title":"Who Is This For?","text":"<p>Primarily for myself (Michael) as a reference when things break or when I need to remember how I set something up months ago. However, it's published on GitHub for anyone interested in building a similar homelab setup.</p>"},{"location":"getting-started/overview/#design-philosophy","title":"Design Philosophy","text":"<p>This homelab follows production-ready practices:</p> <ul> <li>\u2705 Everything is automated - No manual kubectl commands</li> <li>\u2705 Clear separation between public and private networks</li> <li>\u2705 Secure connections - All HTTPS with valid Let's Encrypt certificates</li> <li>\u2705 Disaster recovery - Easy backup and restore procedures</li> <li>\u2705 High Availability - Critical components (control plane, networking) run in HA mode</li> </ul>"},{"location":"getting-started/overview/#quick-navigation","title":"Quick Navigation","text":""},{"location":"getting-started/overview/#i-need-to-fix-something-now","title":"\ud83d\udd27 I Need to Fix Something Now","text":"<p>Go to Troubleshooting &amp; Operations for immediate help with:</p> <ul> <li>Certificate issues</li> <li>Storage problems</li> <li>Network debugging</li> <li>Application failures</li> <li>Disaster recovery</li> </ul>"},{"location":"getting-started/overview/#i-want-to-understand-the-setup","title":"\ud83d\udcd6 I Want to Understand the Setup","text":"<p>Start with Architecture to understand the overall design, then explore:</p> <ul> <li>GitOps Workflow</li> <li>Networking</li> <li>Storage</li> <li>Security</li> </ul>"},{"location":"getting-started/overview/#i-want-to-build-this","title":"\ud83d\udee0\ufe0f I Want to Build This","text":"<p>Follow the Quick Start guide, then proceed through:</p> <ol> <li>Hardware Setup</li> <li>OS Provisioning</li> <li>Cluster Bootstrap</li> <li>Core Services Installation</li> </ol>"},{"location":"getting-started/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"getting-started/overview/#hardware","title":"Hardware","text":"<ul> <li>Control Plane: 3x Raspberry Pi 4 (8GB RAM)</li> <li>Worker Nodes: 4x Raspberry Pi 4 (8GB RAM) with external NVMe storage</li> <li>Compute Servers: Lenovo Thinkcentre M720q, M75q, Minisforum MS-01 (all 64GB RAM)</li> <li>Storage: HL15 with TrueNAS and 6x HDDs</li> </ul>"},{"location":"getting-started/overview/#core-technologies","title":"Core Technologies","text":"<ul> <li>Container Orchestration: k3s (lightweight Kubernetes)</li> <li>GitOps: ArgoCD</li> <li>Ingress: Traefik</li> <li>Load Balancer: MetalLB</li> <li>Storage: Longhorn (distributed block storage)</li> <li>Secrets Management: HashiCorp Vault + External Secrets Operator</li> <li>Authentication: Authentik (SSO/OIDC)</li> <li>Certificates: cert-manager + Let's Encrypt</li> <li>Monitoring: Prometheus + Grafana</li> <li>Logging: Loki</li> <li>Databases: CloudNativePG (PostgreSQL operator)</li> </ul>"},{"location":"getting-started/overview/#cluster-architecture-at-a-glance","title":"Cluster Architecture at a Glance","text":"<pre><code>graph TB\n    subgraph \"External\"\n        Internet[Internet]\n        DNS[CloudFlare DNS]\n    end\n\n    subgraph \"Edge\"\n        Router[OPNsense Router]\n        MetalLB[MetalLB LoadBalancer]\n    end\n\n    subgraph \"Ingress Layer\"\n        Traefik[Traefik Ingress]\n        CertManager[cert-manager]\n    end\n\n    subgraph \"Security Layer\"\n        Authentik[Authentik SSO]\n        Vault[HashiCorp Vault]\n    end\n\n    subgraph \"Platform Services\"\n        ArgoCD[ArgoCD]\n        Monitoring[Prometheus/Grafana]\n        CNPG[PostgreSQL CNPG]\n    end\n\n    subgraph \"Storage\"\n        Longhorn[Longhorn]\n        TrueNAS[TrueNAS]\n    end\n\n    subgraph \"Applications\"\n        Apps[Media/Network/Monitoring Apps]\n    end\n\n    Internet --&gt; DNS\n    DNS --&gt; Router\n    Router --&gt; MetalLB\n    MetalLB --&gt; Traefik\n    Traefik --&gt; Authentik\n    Traefik --&gt; Apps\n    Authentik --&gt; Apps\n    Vault --&gt; Apps\n    CertManager --&gt; Traefik\n    ArgoCD --&gt; Platform Services\n    ArgoCD --&gt; Apps\n    Apps --&gt; Longhorn\n    Apps --&gt; CNPG\n    Longhorn --&gt; TrueNAS</code></pre>"},{"location":"getting-started/overview/#getting-help","title":"Getting Help","text":"<ul> <li>Check Common Issues first</li> <li>Look for your specific component in the Cluster Core section</li> <li>Search the documentation using the search bar</li> <li>Review Useful Commands for quick CLI reference</li> </ul>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<p>New to this setup? Start with the Quick Start Guide.</p> <p>Already familiar? Jump to what you need:</p> <ul> <li>Troubleshooting</li> <li>Deploy New App</li> <li>Configure Ingress</li> <li> <p>Setup Backup</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will help you get started quickly with understanding or building your own k3s homelab cluster.</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have:</p> <ul> <li>Hardware: Multiple nodes (Raspberry Pis or x86 servers)</li> <li>Network: Static IPs configured for all nodes</li> <li>Storage: External storage for Longhorn (NVMe/SSD recommended)</li> <li>Domain: A domain name with DNS control (e.g., CloudFlare)</li> <li>Tools: </li> <li><code>kubectl</code> installed on your workstation</li> <li><code>helm</code> CLI</li> <li><code>ansible</code> for provisioning</li> <li>SSH access to all nodes</li> </ul>"},{"location":"getting-started/quickstart/#high-level-steps","title":"High-Level Steps","text":""},{"location":"getting-started/quickstart/#1-hardware-setup-30-60-minutes","title":"1. Hardware Setup (30-60 minutes)","text":"<ol> <li>Assemble your hardware (Raspberry Pis or servers)</li> <li>Configure network (static IPs, VLANs if needed)</li> <li>Attach external storage for Longhorn</li> <li>Document your hardware inventory</li> </ol> <p>\ud83d\udcd6 Detailed Guide: Raspberry Pi Setup</p>"},{"location":"getting-started/quickstart/#2-os-provisioning-15-minutes-per-node","title":"2. OS Provisioning (15 minutes per node)","text":"<ol> <li>Flash Ubuntu Server to SD cards/USBs</li> <li>Configure initial user and SSH</li> <li>Run initial system updates</li> <li>Configure firmware (for Raspberry Pis)</li> </ol> <p>\ud83d\udcd6 Detailed Guide: Bare Metal Setup</p>"},{"location":"getting-started/quickstart/#3-ansible-provisioning-20-30-minutes","title":"3. Ansible Provisioning (20-30 minutes)","text":"<ol> <li>Configure Ansible inventory</li> <li>Run user and SSH key provisioning playbook</li> <li>Run k3s installation playbook</li> <li>Verify cluster is up</li> </ol> <p>\ud83d\udcd6 Detailed Guide: Ansible Automation</p> <pre><code># Quick commands\nansible-playbook add-user-ssh.yaml\nansible-playbook playbooks/06_k3s_secure.yaml\nkubectl get nodes\n</code></pre>"},{"location":"getting-started/quickstart/#4-bootstrap-core-services-1-2-hours","title":"4. Bootstrap Core Services (1-2 hours)","text":"<p>Install critical infrastructure in order:</p> <ol> <li>MetalLB - Load balancer for bare metal</li> <li>Traefik - Ingress controller</li> <li>cert-manager - Certificate management</li> <li>Longhorn - Distributed storage</li> <li>ArgoCD - GitOps continuous delivery</li> </ol> <p>\ud83d\udcd6 Detailed Guides:  - MetalLB - Traefik - cert-manager - Longhorn - ArgoCD</p>"},{"location":"getting-started/quickstart/#5-setup-security-secrets-1-2-hours","title":"5. Setup Security &amp; Secrets (1-2 hours)","text":"<ol> <li>HashiCorp Vault - Secrets storage</li> <li>Authentik - SSO/OIDC provider</li> <li>External Secrets Operator - Sync secrets from Vault</li> <li>Secrets Store CSI - Mount secrets as files</li> </ol> <p>\ud83d\udcd6 Detailed Guides:  - Vault - Authentik</p>"},{"location":"getting-started/quickstart/#6-deploy-platform-services-30-60-minutes","title":"6. Deploy Platform Services (30-60 minutes)","text":"<ol> <li>Monitoring Stack - Prometheus + Grafana + Loki</li> <li>PostgreSQL - CloudNativePG operator</li> <li>Backup configuration - Longhorn backups to MinIO/TrueNAS</li> </ol> <p>\ud83d\udcd6 Detailed Guides:  - Monitoring - PostgreSQL</p>"},{"location":"getting-started/quickstart/#7-deploy-applications","title":"7. Deploy Applications","text":"<p>Now you can deploy your applications using ArgoCD!</p>"},{"location":"getting-started/quickstart/#verification-checklist","title":"Verification Checklist","text":"<p>After setup, verify everything is working:</p> <ul> <li> All nodes are in <code>Ready</code> state: <code>kubectl get nodes</code></li> <li> MetalLB has assigned external IPs: <code>kubectl get svc -A | grep LoadBalancer</code></li> <li> Traefik dashboard is accessible</li> <li> cert-manager has issued certificates: <code>kubectl get certificate -A</code></li> <li> Longhorn dashboard shows all volumes healthy</li> <li> ArgoCD is syncing applications</li> <li> Vault is initialized and unsealed</li> <li> Authentik is accessible and configured</li> <li> Prometheus/Grafana showing metrics</li> <li> Test application is deployed and accessible via HTTPS</li> </ul>"},{"location":"getting-started/quickstart/#common-first-time-issues","title":"Common First-Time Issues","text":""},{"location":"getting-started/quickstart/#pods-stuck-in-pending","title":"Pods Stuck in Pending","text":"<p>Cause: Usually storage or resource constraints</p> <p>Fix:  <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Check events for specific error\n</code></pre></p> <p>\ud83d\udcd6 See: Storage Problems</p>"},{"location":"getting-started/quickstart/#certificate-not-issued","title":"Certificate Not Issued","text":"<p>Cause: DNS propagation or Let's Encrypt rate limits</p> <p>Fix: <pre><code>kubectl describe certificate &lt;cert-name&gt; -n &lt;namespace&gt;\nkubectl describe certificaterequest -n &lt;namespace&gt;\n</code></pre></p> <p>\ud83d\udcd6 See: Certificate Issues</p>"},{"location":"getting-started/quickstart/#cant-access-services","title":"Can't Access Services","text":"<p>Cause: MetalLB not configured, Traefik not routing, or firewall</p> <p>Fix: <pre><code>kubectl get svc -n traefik\nkubectl get ingressroute -A\n</code></pre></p> <p>\ud83d\udcd6 See: Network Debugging</p>"},{"location":"getting-started/quickstart/#time-estimates","title":"Time Estimates","text":"<ul> <li>Minimal working cluster: 4-6 hours</li> <li>Production-ready with monitoring: 8-12 hours</li> <li>Full setup with all applications: 2-3 days</li> </ul> <p>These are estimates for first-time setup. With experience and automation, you can rebuild in a few hours.</p>"},{"location":"getting-started/quickstart/#gitops-repository-structure","title":"GitOps Repository Structure","text":"<p>The repository is organized as follows:</p> <pre><code>k3s-cluster-infra-apps/\n\u251c\u2500\u2500 cluster-init-apps/       # Bootstrap apps (MetalLB, cert-manager)\n\u251c\u2500\u2500 cluster-critical-apps/   # Core infrastructure (Traefik, Longhorn)\n\u251c\u2500\u2500 cluster-platform-apps/   # Platform services (databases, monitoring)\n\u251c\u2500\u2500 cluster-apps-apps/       # User applications\n\u251c\u2500\u2500 apps-root-config/        # ArgoCD ApplicationSets\n\u2514\u2500\u2500 docs/                    # This documentation\n</code></pre> <p>Each app follows this structure: <pre><code>app-name/\n\u251c\u2500\u2500 Chart.yaml              # Helm chart definition\n\u251c\u2500\u2500 values.yaml             # Configuration values\n\u2514\u2500\u2500 templates/              # Kubernetes manifests\n</code></pre></p>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ol> <li>Understand the Architecture: Architecture Overview</li> <li>Start Building: Hardware Setup</li> <li>Learn GitOps Workflow: ArgoCD</li> </ol>"},{"location":"getting-started/quickstart/#getting-help","title":"Getting Help","text":"<p>If you run into issues:</p> <ol> <li>Check Troubleshooting &amp; Operations</li> <li>Search this documentation</li> <li> <p>Check the GitHub repository issues</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ol>"},{"location":"hardware/network/","title":"Network","text":"<p>The Raspberries are in two networks. The internal network and a special DMZ for the external communication of the K8S Cluster. This is not done by actual multiple network interfaces but with vlan.</p> <p>Just by chance my firewall had a free network interface so I chose to actually use a seperate network interface for the DMZ there. I would have used vlan there as well.</p> <p>I could use pure port-forwarding to the MetalLB-IP of the Cluster.</p>"},{"location":"hardware/network/#opnsense","title":"Opnsense","text":"<p>On my Opnsense firewall I have to configure several things:</p> <ol> <li>WAN interface to accept inbound web (443) traffic</li> <li>Add NAT port-forward for the MetalLB-IP</li> </ol>"},{"location":"hardware/network/#vpn","title":"VPN","text":"<p>The firewall has OpenVPN installed. The users are configured on the firewall and the authentication uses OTPT.</p> <p>The OTP from the Google Authenticator is entered with the password in the following form: <code>&lt;password&gt;&lt;otp&gt;</code>.</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"hardware/proxmox/","title":"Setup Proxmox","text":"<p>Proxmox is an OpenSource virtualization software. The largere nodes are setup with it and the nodes on it virtualized.</p>"},{"location":"hardware/proxmox/#install-proxmox","title":"Install Proxmox","text":"<p>First create a boot USB stick with Proxmox VE Installer ISO.</p> <p>Select ZFS as filesystem as it will set the bootloader to systemd-boot instead of GRUB.</p>"},{"location":"hardware/proxmox/#first-steps-on-a-new-proxmox-server","title":"First steps on a new Proxmox server","text":"<p>After setting up a new Proxmox server, there are a few things to do before they can be used. Idealy those steps would be automated but ...</p> <p>Those steps are heavily inspired by techno tim setup article</p>"},{"location":"hardware/proxmox/#updates","title":"Updates","text":"<p>Edit <code>/etc/apt/sources.list</code></p> <pre><code>deb http://ftp.debian.org/debian bullseye main contrib\n\ndeb http://ftp.debian.org/debian bullseye-updates main contrib\n\n# security updates\ndeb http://security.debian.org/debian-security bullseye-security main contrib\n\n# PVE pve-no-subscription repository provided by proxmox.com,\n# NOT recommended for production use\ndeb http://download.proxmox.com/debian/pve bullseye pve-no-subscription\n</code></pre> <p>Edit <code>/etc/apt/sources.list.d/pve-enterprise.list</code></p> <pre><code># deb https://enterprise.proxmox.com/debian/pve buster pve-enterprise\n</code></pre> <p>Run</p> <pre><code>apt-get update\n\napt full-upgrade\n\nreboot\n</code></pre>"},{"location":"hardware/proxmox/#storage","title":"Storage","text":"<p>BE CAREFUL. This is meant for the storage/longhorn disks as it will wipe it</p> select the correct disk device<pre><code>fdisk /dev/sda\n</code></pre> <p>Then P for partition, then D for delete and W for write.</p>"},{"location":"hardware/proxmox/#iommu-pci-passthrough","title":"IOMMU  (PCI Passthrough)","text":"<p>See Proxmox PCI Passthrough</p> <p>First make sure IOMMU is enabled in the BIOS.</p> <p><code>nano /etc/kernel/cmdline</code></p> <p>Add <code>intel_iommu=on iommu=pt</code> to the end of this line without line breaks. (for AMD processors add <code>amd_iommu=on ...</code>)</p> <pre><code>root=ZFS=rpool/ROOT/pve-1 boot=zfs intel_iommu=on iommu=pt\n</code></pre> <p>Edit <code>/etc/modules</code> and add</p> <pre><code>vfio\nvfio_iommu_type1\nvfio_pci\nvfio_virqfd\n</code></pre> <p>run</p> <pre><code>update-initramfs -u -k all\n# or proxmox-boot-tool refresh\n\nreboot\n</code></pre>"},{"location":"hardware/proxmox/#vlan-aware","title":"VLAN Aware","text":"<p>Setup the network interface to handle VLANs.</p> <p>Add the following lines to the main network interface so all vlans are served over one interface and supports vlan ids from 2 to 4096.</p> /etc/network/interfaces<pre><code>iface vmbr0 inet static\n    address 192.168.xx.xx/24\n    gateway 192.168.xx.xx\n    ...\n    bridge-vlan-aware yes\n    bridge-vids 2-4094\n</code></pre>"},{"location":"hardware/proxmox/#isos","title":"ISOs","text":"<p>Define images:</p> <ul> <li>https://releases.ubuntu.com/22.10/ubuntu-22.10-live-server-amd64.iso</li> <li>https://releases.ubuntu.com/noble/ubuntu-24.04.1-live-server-amd64.iso</li> </ul>"},{"location":"hardware/proxmox/#setup-worker-node","title":"Setup worker node","text":"<p>First select <code>Create VM</code> </p> <p>Depending on the usage the parameters may vary. But following is a node which is part of the longhorn storage and has an interface to the internet. The list defines mostly non default values. If nothing else specified take the defaults.</p> <ul> <li>General:<ul> <li>Node: pve1-x</li> <li>Name: k3sworker</li> <li>Start at boot: true</li> </ul> </li> <li>OS:<ul> <li>Storage: local</li> <li>ISO Image: ubuntu server installer</li> </ul> </li> <li>System:<ul> <li>Qemu Agent: true</li> </ul> </li> <li>Disks:<ul> <li>disk 1:<ul> <li>Storage: local</li> <li>Size: 128 GB</li> <li>Backup: yes</li> <li>Skip replication: no</li> <li>Discard: yes</li> <li>SSD emulation: yes</li> </ul> </li> <li>disk 2: (for longhorn but passthrough is preferred)<ul> <li>Storage: virtual</li> <li>Size: &gt;512 GB</li> <li>Backup: no</li> <li>Skip replication: yes</li> <li>Discard: yes</li> <li>SSD emulation: yes</li> </ul> </li> </ul> </li> <li>CPU:<ul> <li>Sockets: 1</li> <li>Cores: 2-12</li> </ul> </li> <li>Memory:<ul> <li>min 16 GB (16384 MiB)  </li> <li>opt 32 GB (32768 MiB)</li> <li>max 64 GB (65536 MiB) (You can't use all memory, keep 2GB)</li> </ul> </li> <li>Network:<ul> <li>net0<ul> <li>Bridge: vmbr0</li> </ul> </li> <li>net1<ul> <li>Bridge: vmbr0</li> <li>vlan tag: 99</li> </ul> </li> </ul> </li> </ul> <p>Don't forget to define the generated mac address in DHCP</p> <p>You might have to adjust the boot order for CD to be first.</p>"},{"location":"hardware/proxmox/#disk-passthrough","title":"Disk passthrough","text":"<p>The disks for longhorn, especially the slower ones like HDD, should be attached to the VM directly. Unfortunately there is no way to do this in the Web UI at the moment but proxmox provides quite a  good guide for it.</p> <p>Go through the following steps</p> <ol> <li>Find right disk</li> </ol> <pre><code>find /dev/disk/by-id/ -type l|xargs -I{} ls -l {}|grep -v -E '[0-9]$' |sort -k11|cut -d' ' -f9,10,11,12\n</code></pre> <p>or</p> <pre><code>lsblk |awk 'NR==1{print $0\" DEVICE-ID(S)\"}NR&gt;1{dev=$1;printf $0\" \";system(\"find /dev/disk/by-id -lname \\\"*\"dev\"\\\" -printf \\\" %p\\\"\");print \"\";}'|grep -v -E 'part|lvm'\n</code></pre> <ol> <li>Add drive as new virtual drive</li> </ol> <p><pre><code>qm set 592 -scsi2 /dev/disk/by-id/ata-...\n</code></pre> Replace 592 with the correct VM id and -scsi2 with the next scsi id.</p> <ol> <li>Remove drive</li> </ol> <pre><code>qm unlink 592 --idlist scsi2\n</code></pre>"},{"location":"hardware/proxmox/#gpu-passthrough","title":"GPU Passthrough","text":"<p>As prerequisite you should make sure the onboard graphics is still enabled and is the primary display.</p> <p>I follow the suggestions from proxmox passthrough guide:</p> <p>First you want to blacklist GPU drivers to prevent the host from loading the GPU (we want to use in the VM :D )</p> <pre><code>echo \"blacklist amdgpu\" &gt;&gt; /etc/modprobe.d/blacklist.conf\necho \"blacklist radeon\" &gt;&gt; /etc/modprobe.d/blacklist.conf\n\necho \"blacklist nouveau\" &gt;&gt; /etc/modprobe.d/blacklist.conf \necho \"blacklist nvidia*\" &gt;&gt; /etc/modprobe.d/blacklist.conf\n\necho \"blacklist i915\" &gt;&gt; /etc/modprobe.d/blacklist.conf\n</code></pre> <p>After setting this you have to update the <code>ramfs</code> and reboot the system.</p> <pre><code>update-initramfs -u -k all\n</code></pre> <p>Find Vendor and Devide ID to use for the VM Device (vfio)</p> <pre><code>echo \"options vfio-pci ids=10de:1ff0 disable_vga=1\" &gt; /etc/modprobe.d/vfio.conf\n</code></pre> <p>Reboot and add raw device (select map all functions)</p> <pre><code>hostpci0: 0000:01:00,pcie=1\n</code></pre>"},{"location":"hardware/proxmox/#nvidia-drivers-on-k3s-node","title":"Nvidia drivers on k3s node","text":"<p>https://ubuntu.com/server/docs/nvidia-drivers-installation</p> <p>https://www.declarativesystems.com/2023/11/04/kubernetes-nvidia.html</p> <p>https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</p> <p>https://radicalgeek.co.uk/pi-cluster/adding-a-gpu-node-to-a-k3s-cluster/</p>"},{"location":"hardware/proxmox/#tips-trick","title":"Tips &amp; Trick","text":""},{"location":"hardware/proxmox/#resize-disk","title":"Resize disk","text":"<pre><code>sudo lvresize -l +100%FREE --resizefs /dev/mapper/ubuntu--vg-ubuntu--lv\n</code></pre>"},{"location":"hardware/proxmox/#install-qemu-agent","title":"Install Qemu Agent","text":"<pre><code>ssh devops@k3sworker&lt;xy&gt;\nsudo apt-get install qemu-guest-agent\n</code></pre>"},{"location":"hardware/proxmox/#references","title":"References","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"hardware/raspberry-pi/","title":"Raspberry Pi Setup","text":"<p>This guide covers the hardware setup and initial configuration of Raspberry Pi nodes for the k3s cluster.</p>"},{"location":"hardware/raspberry-pi/#hardware-components","title":"Hardware Components","text":""},{"location":"hardware/raspberry-pi/#control-plane-nodes-3x","title":"Control Plane Nodes (3x)","text":"<ul> <li>Model: Raspberry Pi 4 Model B</li> <li>RAM: 8GB</li> <li>Boot Storage: Corsair GTX USB Stick</li> <li>Power: PoE HAT (some nodes)</li> <li>Purpose: k3s control plane (etcd + API server)</li> </ul>"},{"location":"hardware/raspberry-pi/#worker-nodes-4x","title":"Worker Nodes (4x)","text":"<ul> <li>Model: Raspberry Pi 4 Model B</li> <li>RAM: 8GB</li> <li>Boot Storage: Corsair GTX USB Stick</li> <li>Data Storage: External USB NVMe drive (separately powered)</li> <li>Power: PoE HAT (some nodes) or standard USB-C</li> <li>Purpose: Application workloads + Longhorn storage</li> </ul>"},{"location":"hardware/raspberry-pi/#initial-setup","title":"Initial Setup","text":""},{"location":"hardware/raspberry-pi/#prerequisites","title":"Prerequisites","text":"<ul> <li>MAC address configured in DHCP server</li> <li>Raspberry Pi Imager downloaded</li> <li>External NVMe drive (for worker nodes)</li> <li>USB-C power supply or PoE switch</li> </ul>"},{"location":"hardware/raspberry-pi/#imaging-the-sd-cardusb-stick","title":"Imaging the SD Card/USB Stick","text":"<ol> <li>Download Raspberry Pi Imager</li> <li> <p>https://www.raspberrypi.com/software/</p> </li> <li> <p>Image Ubuntu Server</p> </li> <li> <p>OS \u2192 Other general purpose OS \u2192 Ubuntu \u2192 Ubuntu Server 24.04 LTS (64-bit)</p> </li> <li> <p>Customization Settings:</p> </li> <li>Hostname: <code>&lt;nodename&gt;</code> (e.g., k3s-server01, k3s-worker01)</li> <li>Username/Password: <code>ubuntu</code>/<code>ubuntu</code></li> <li>Enable SSH with password authentication</li> <li> <p>Configure Wi-Fi (if needed, but wired recommended)</p> </li> <li> <p>Write to USB stick</p> </li> <li> <p>Boot the Raspberry Pi</p> </li> <li>Insert USB stick</li> <li>Power on</li> <li>Wait for first boot (can take 2-3 minutes)</li> </ol>"},{"location":"hardware/raspberry-pi/#first-login","title":"First Login","text":"<pre><code># Remove old host keys if re-imaging\nssh-keygen -R &lt;hostname&gt;\nssh-keygen -R &lt;ip-address&gt;\n\n# Login (default password: ubuntu)\nssh ubuntu@&lt;hostname&gt;\n\n# Change password on first login (prompted)\n</code></pre>"},{"location":"hardware/raspberry-pi/#initial-system-update","title":"Initial System Update","text":"<pre><code># Update package list and upgrade\nsudo apt update &amp;&amp; sudo apt full-upgrade -y\n\n# Reboot to apply updates\nsudo reboot\n\n# After reboot, update firmware\nsudo rpi-eeprom-update -a\nsudo reboot\n</code></pre>"},{"location":"hardware/raspberry-pi/#hardware-configuration","title":"Hardware Configuration","text":""},{"location":"hardware/raspberry-pi/#poe-hat-fan-control","title":"PoE HAT Fan Control","text":"<p>The PoE HAT fan can oscillate between min and max speed, which is annoying.</p> <p>Fix (based on Jeff Geerling's article):</p> <pre><code># Edit the config\nsudo nano /boot/firmware/config.txt\n\n# Add these lines\ndtparam=poe_fan_temp0=50000\ndtparam=poe_fan_temp1=60000\ndtparam=poe_fan_temp2=70000\ndtparam=poe_fan_temp3=80000\n\n# Reboot\nsudo reboot\n</code></pre> <p>This sets temperature thresholds for fan speed: - Below 50\u00b0C: Fan off - 50-60\u00b0C: Low speed - 60-70\u00b0C: Medium speed - 70-80\u00b0C: High speed - Above 80\u00b0C: Max speed</p>"},{"location":"hardware/raspberry-pi/#external-nvme-drive-setup","title":"External NVMe Drive Setup","text":"<p>For worker nodes with external storage (for Longhorn):</p>"},{"location":"hardware/raspberry-pi/#1-identify-the-drive","title":"1. Identify the Drive","text":"<pre><code>lsblk -f\n</code></pre> <p>Example output: <pre><code>NAME   FSTYPE LABEL UUID                                 MOUNTPOINT\nsda                                                      \n\u2514\u2500sda1 ext4         9999-9999-9999-9999                  \n</code></pre></p>"},{"location":"hardware/raspberry-pi/#2-wipe-existing-data-if-reusing-drive","title":"2. Wipe Existing Data (if reusing drive)","text":"<pre><code>sudo wipefs -a /dev/sda\n</code></pre>"},{"location":"hardware/raspberry-pi/#3-create-partition-table","title":"3. Create Partition Table","text":"<pre><code>sudo fdisk /dev/sda\n</code></pre> <p>Commands in fdisk: <pre><code>g     # Create new GPT partition table\nn     # New partition\n1     # Partition number (default)\n      # First sector (default)\n      # Last sector (default)\nw     # Write changes\n</code></pre></p>"},{"location":"hardware/raspberry-pi/#4-create-filesystem","title":"4. Create Filesystem","text":"<pre><code>sudo mkfs.ext4 /dev/sda1\n</code></pre>"},{"location":"hardware/raspberry-pi/#5-mount-drive","title":"5. Mount Drive","text":"<pre><code># Create mount point\nsudo mkdir -p /var/lib/longhorn\n\n# Get UUID\nsudo lsblk -o name,uuid\n\n# Add to fstab\necho \"UUID=&lt;your-uuid&gt; /var/lib/longhorn ext4 defaults 0 2\" | sudo tee -a /etc/fstab\n\n# Mount\nsudo mount /var/lib/longhorn\n\n# Verify\ndf -h | grep longhorn\n</code></pre>"},{"location":"hardware/raspberry-pi/#power-supply-configuration","title":"Power Supply Configuration","text":"<p>For nodes using UPS:</p> <p>Reference: https://github.com/dzomaya/NUTandRpi</p>"},{"location":"hardware/raspberry-pi/#ansible-provisioning","title":"Ansible Provisioning","text":"<p>Once the hardware is set up, use Ansible to configure the software:</p>"},{"location":"hardware/raspberry-pi/#1-add-node-to-ansible-inventory","title":"1. Add Node to Ansible Inventory","text":"<p>Edit <code>hosts.yaml</code>:</p> <pre><code>all:\n  children:\n    k3s_servers:\n      hosts:\n        k3s-server01:\n          ansible_host: 192.168.1.10\n        k3s-server02:\n          ansible_host: 192.168.1.11\n        k3s-server03:\n          ansible_host: 192.168.1.12\n\n    k3s_workers:\n      hosts:\n        k3s-worker01:\n          ansible_host: 192.168.1.20\n        k3s-worker02:\n          ansible_host: 192.168.1.21\n</code></pre>"},{"location":"hardware/raspberry-pi/#2-run-user-and-ssh-key-setup","title":"2. Run User and SSH Key Setup","text":"<pre><code>ansible-playbook add-user-ssh.yaml --limit k3s-worker01\n</code></pre> <p>This will: - Create your user account - Add SSH keys - Configure sudo access</p>"},{"location":"hardware/raspberry-pi/#3-join-node-to-cluster","title":"3. Join Node to Cluster","text":"<pre><code># For control plane nodes\nansible-playbook playbooks/06_k3s_secure.yaml --limit k3s-server03 --tags server\n\n# For worker nodes\nansible-playbook playbooks/06_k3s_secure.yaml --limit k3s-worker01\n</code></pre>"},{"location":"hardware/raspberry-pi/#4-verify-node-joined","title":"4. Verify Node Joined","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"hardware/raspberry-pi/#troubleshooting","title":"Troubleshooting","text":""},{"location":"hardware/raspberry-pi/#node-wont-boot","title":"Node Won't Boot","text":"<ol> <li>Check power supply: Raspberry Pi 4 needs 3A USB-C power</li> <li>Check boot order: May need to configure in EEPROM</li> <li>Re-flash USB stick: Might be corrupted</li> </ol>"},{"location":"hardware/raspberry-pi/#cant-ssh-to-node","title":"Can't SSH to Node","text":"<ol> <li>Check DHCP: Is node getting IP?</li> <li>Check network cable: Is it connected?</li> <li>Check SSH service: <code>sudo systemctl status ssh</code> (via console)</li> </ol>"},{"location":"hardware/raspberry-pi/#external-drive-not-mounting","title":"External Drive Not Mounting","text":"<pre><code># Check if drive is detected\nlsblk\n\n# Check fstab\ncat /etc/fstab\n\n# Check mount errors\nsudo journalctl -u local-fs.target\n\n# Manual mount for testing\nsudo mount -t ext4 /dev/sda1 /var/lib/longhorn\n</code></pre>"},{"location":"hardware/raspberry-pi/#longhorn-cant-use-drive","title":"Longhorn Can't Use Drive","text":"<pre><code># Check permissions\nls -la /var/lib/longhorn\n\n# Fix if needed\nsudo chown -R root:root /var/lib/longhorn\nsudo chmod 755 /var/lib/longhorn\n\n# Verify Longhorn can access\nkubectl get node -o jsonpath='{.items[*].metadata.annotations.longhorn\\.io/default-disks-config}' | jq\n</code></pre>"},{"location":"hardware/raspberry-pi/#high-temperature","title":"High Temperature","text":"<pre><code># Check current temp\nvcgencmd measure_temp\n\n# If over 80\u00b0C consistently:\n# - Check PoE HAT fan is running\n# - Add heatsinks\n# - Improve case ventilation\n# - Check ambient room temperature\n</code></pre>"},{"location":"hardware/raspberry-pi/#firmware-updates","title":"Firmware Updates","text":""},{"location":"hardware/raspberry-pi/#update-eeprom","title":"Update EEPROM","text":"<pre><code># Check current version\nsudo rpi-eeprom-update\n\n# Update to latest\nsudo rpi-eeprom-update -a\n\n# Reboot to apply\nsudo reboot\n</code></pre>"},{"location":"hardware/raspberry-pi/#boot-from-usb","title":"Boot from USB","text":"<p>To boot from USB instead of SD card:</p> <pre><code># Update bootloader config\nsudo raspi-config\n\n# Advanced Options \u2192 Boot Order \u2192 USB Boot\n# Reboot\n</code></pre>"},{"location":"hardware/raspberry-pi/#performance-tuning","title":"Performance Tuning","text":""},{"location":"hardware/raspberry-pi/#overclock-optional","title":"Overclock (Optional)","text":"<p>Edit <code>/boot/firmware/config.txt</code>:</p> <pre><code># Overclock to 2.0 GHz (safe for Pi 4)\nover_voltage=6\narm_freq=2000\n\n# GPU memory (we don't need much)\ngpu_mem=16\n</code></pre> <p>Warning: Overclocking can reduce lifespan and increase heat. Monitor temperatures.</p>"},{"location":"hardware/raspberry-pi/#network-tuning","title":"Network Tuning","text":"<p>For better network performance:</p> <pre><code># Edit sysctl.conf\nsudo nano /etc/sysctl.conf\n\n# Add these lines\nnet.core.rmem_max=134217728\nnet.core.wmem_max=134217728\nnet.ipv4.tcp_rmem=4096 87380 67108864\nnet.ipv4.tcp_wmem=4096 65536 67108864\n\n# Apply\nsudo sysctl -p\n</code></pre>"},{"location":"hardware/raspberry-pi/#hardware-accessories","title":"Hardware Accessories","text":""},{"location":"hardware/raspberry-pi/#blinkstick-nano","title":"BlinkStick Nano","text":"<p>For visual status indicators: https://www.blinkstick.com/products/blinkstick-nano</p> <p>Can be used to show node status (green = healthy, red = problems).</p>"},{"location":"hardware/raspberry-pi/#recommended-accessories","title":"Recommended Accessories","text":"<ul> <li>PoE HAT: Official Raspberry Pi PoE+ HAT</li> <li>Heatsinks: Aluminum heatsinks for CPU, RAM, and USB controller</li> <li>Case: Cluster cases like GeeekPi or Uctronics</li> <li>Network: Gigabit switch with PoE (if using PoE HATs)</li> <li>Storage: High-quality USB 3.0 NVMe enclosures (not cheap ones!)</li> </ul>"},{"location":"hardware/raspberry-pi/#maintenance","title":"Maintenance","text":""},{"location":"hardware/raspberry-pi/#monthly-tasks","title":"Monthly Tasks","text":"<pre><code># Update OS\nsudo apt update &amp;&amp; sudo apt full-upgrade -y\n\n# Check disk health\nsudo smartctl -a /dev/sda\n\n# Check system logs for errors\nsudo journalctl -p err -b\n\n# Clean old logs\nsudo journalctl --vacuum-time=30d\n</code></pre>"},{"location":"hardware/raspberry-pi/#monitoring","title":"Monitoring","text":"<ul> <li>CPU temperature: Should stay below 80\u00b0C under load</li> <li>Disk space: Keep at least 20% free</li> <li>Memory usage: Should have swap available</li> <li>Network: Check for packet loss</li> </ul>"},{"location":"hardware/raspberry-pi/#reference-links","title":"Reference Links","text":"<ul> <li>Raspberry Pi Documentation</li> <li>Ubuntu on Raspberry Pi</li> <li>Jeff Geerling's Pi Cluster</li> <li> <p>k3s on Raspberry Pi</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"hardware/truenas/","title":"TrueNAS","text":""},{"location":"hardware/truenas/#initial-pool","title":"Initial Pool","text":"<p>I created a ZFS pool named <code>bigone</code> with the following command</p>"},{"location":"hardware/truenas/#k8s-volumes","title":"K8s Volumes","text":"<p>The idea is to use TrueNAS as a storage backend for Kubernetes, allowing you to create persistent volumes that can be used by your applications. You need to define both a storage class which can connect to TrueNAS and obviously a dataset.</p> <p>Resources</p> <ul> <li>https://github.com/fenio/k8s-truenas?tab=readme-ov-file (main)</li> <li>https://www.truenas.com/blog/truenas-enables-container-storage-and-kubernetes/</li> <li>https://www.truenas.com/docs/solutions/integrations/containers/</li> <li>https://jonathangazeley.com/2021/01/05/using-truenas-to-provide-persistent-storage-for-kubernetes/</li> </ul>"},{"location":"hardware/truenas/#dataset","title":"Dataset","text":"<p>To create a dataset for Kubernetes, you can use the TrueNAS web interface or the CLI. The dataset should be created under the ZFS pool you created earlier. You can create a dataset named <code>k8s</code> under the <code>bigone</code> pool in the web interface like this:</p> <p>Under this dataset, you can create sub-datasets for NFS, iSCSI, etc. For example, you can create a sub-dataset named <code>nfs</code> for NFS volumes.</p>"},{"location":"hardware/truenas/#storage-class","title":"Storage Class","text":"<p>You need a driver as well. We are going to use democratic CSI driver, which is available in the democratic-csi</p> values.yaml<pre><code>---\ndemocratic-csi:\n  controller:\n    driver:\n      image:\n        tag: next\n  csiDriver:\n    name: \"nfs\"\n  storageClasses:\n    - name: truenas-nfs # (4)!\n      defaultClass: false\n      reclaimPolicy: Delete\n      volumeBindingMode: Immediate\n      allowVolumeExpansion: true\n      parameters:\n        fsType: nfs\n        detachedVolumesFromSnapshots: \"false\"\n      mountOptions:\n        - noatime\n        - nfsvers=3\n      secrets:\n        provisioner-secret:\n        controller-publish-secret:\n        node-stage-secret:\n        node-publish-secret:\n        controller-expand-secret:\n  volumeSnapshotClasses:\n    - name: nfs\n      parameters:\n        detachedSnapshots: \"true\"\n  driver:\n    config:\n      driver: freenas-api-nfs\n      instance_id:\n      httpConnection:\n        apiVersion: 2\n        protocol: http\n        host: 192.168.1.210 # (3)!\n        port: 80\n        apiKey: someApiKey # (1)!\n        allowInsecure: true\n      zfs:\n        datasetParentName: bigone/k8s/nfs/v # (2)!\n        detachedSnapshotsDatasetParentName: bigone/k8s/nfs/s # (2)!\n        datasetEnableQuotas: true\n        datasetEnableReservation: false\n        datasetPermissionsMode: \"0777\"\n        datasetPermissionsUser: 0\n        datasetPermissionsGroup: 0\n      nfs:\n        shareHost: 192.168.42.210 # (3)!\n        shareAlldirs: false\n        shareAllowedHosts: [ ]\n        shareAllowedNetworks: [ ]\n        shareMaprootUser: root\n        shareMaprootGroup: root\n        shareMapallUser: \"\"\n        shareMapallGroup: \"\"\n\n1. Replace with your API key from TrueNAS. You can find it in the TrueNAS web interface under `System` -&gt; `API Keys`.\n   Make sure to create a new API key with the necessary permissions for the CSI driver to access the NFS shares.\n2. Replace with the desired parent dataset name for your NFS volumes. This is where the CSI driver will create datasets for each persistent volume.\n3. Replace with the IP address of your TrueNAS server. This is the address where the NFS shares will be accessible from your Kubernetes cluster.\n4. This is the name of the storage class that will be used to provision NFS volumes in Kubernetes. You can change it to whatever you prefer.\n</code></pre>"},{"location":"hardware/truenas/#time-machine-backup","title":"Time Machine Backup","text":"<p>Create a dataset for Time Machine backups, e.g. <code>bigone/timemachine</code>.</p> <p>Then create a NFS share for this dataset in the TrueNAS web interface:</p> <ol> <li>Go to <code>Sharing</code> -&gt; <code>Unix Shares (NFS)</code>.</li> <li>Click on <code>Add</code>.</li> <li>Set the <code>Path</code> to the dataset you created for Time Machine backups, e.g. <code>/mnt/bigone/timemachine</code>.</li> <li>Set the <code>Purpose</code> to Multi-user time machine backups.</li> <li>Under <code>Access</code></li> <li>Enable Browsable to Network Clients.</li> <li>Under <code>Other Options</code></li> <li>Enable <code>Time Machine</code>.</li> <li>Enable <code>Enable Shadow Copies</code></li> <li>Enable <code>Enable Alternate Data Streams</code></li> <li>Enable <code>Enable SMB2/3 Durable Handles</code></li> </ol>"},{"location":"hardware/truenas/#minio","title":"Minio","text":"<ul> <li>Add Dataset: <code>bigone/minio</code></li> <li>Install Minio App</li> </ul>"},{"location":"hardware/truenas/#performance-testing","title":"Performance Testing","text":"<p>Got the following idea from the Proxmox forum <sup>1</sup>.</p> <p>I used the <code>fio</code> tool to benchmark the performance of my SSDs in TrueNAS. In the following command it is important to adapt the filename to an actual path on your zfs pool, e.g. <code>/mnt/yourpool/test.file</code>.</p> <p>The following command was used:</p> benchmark.sh<pre><code>#!/usr/local/bin/bash\n\nLOGFILE=\"/root/benchmark.log\" #filename of the logfile\nFILENAME=\"/mnt/bigone/test/test.file\" #filename of the test file\n\n\nLOGFILE=\"/root/benchmark.log\"\nFILENAME=\"/mnt/bigone/test/test.file\"\n\n\niostat | tee -a \"${LOGFILE}\"\n\n\n\nfio --filename=$FILENAME --name=sync_randwrite --rw=randwrite --bs=4k --direct=1 --sync=0 --numjobs=1 --ioengine=psync --iodepth=1 --refill_buffers --size=10M --loops=1 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n\nfio --name=random-write --ioengine=posixaio --rw=randwrite --bs=4k --numjobs=1 --size=4g --iodepth=1 --runtime=60 --time_based --end_fsync=1\nfio --name=random-write --ioengine=posixaio --rw=randwrite --bs=4k --numjobs=8 --size=4g --iodepth=8 --runtime=60 --time_based --end_fsync=1\n\n\n# sync randwrite = (writes 1G)\nfio --filename=$FILENAME --name=sync_randwrite --rw=randwrite --bs=4k --direct=1 --sync=1 --numjobs=1 --ioengine=psync --iodepth=1 --refill_buffers --size=1G --loops=1 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n# sync randread (writes 10x1G)\nfio --filename=$FILENAME --name=sync_randread --rw=randread --bs=4k --direct=1 --sync=1 --numjobs=1 --ioengine=psync --iodepth=1 --refill_buffers --size=1G --loops=10 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n# seq sync seqwrite (writes 5G)\nfio --filename=$FILENAME --name=sync_seqwrite --rw=write --bs=4M --direct=1 --sync=1 --numjobs=1 --ioengine=psync --iodepth=1 --refill_buffers --size=5G --loops=1 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n# seq sync seqread (writes 10x1G)\nfio --filename=$FILENAME --name=sync_seqread --rw=read --bs=4M --direct=1 --sync=1 --numjobs=1 --ioengine=psync --iodepth=1 --refill_buffers --size=1G --loops=10 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async uncached randwrite (writes 4G)\nfio --filename=$FILENAME --name=async_uncached_randwrite --rw=randwrite --bs=4k --direct=1 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=1G --loops=1 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async cached randwrite (writes 4G)\nfio --filename=$FILENAME --name=async_cached_randwrite --rw=randwrite --bs=4k --direct=0 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=1G --loops=1 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async uncached randread (writes 4G)\nfio --filename=$FILENAME --name=async_uncached_randread --rw=randread --bs=4k --direct=1 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=1G --loops=10 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async cached randread (writes 4G)\nfio --filename=$FILENAME --name=async_cached_randread --rw=randread --bs=4k --direct=0 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=1G --loops=10 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async uncached seqwrite (writes 8G)\nfio --filename=$FILENAME --name=async_uncached_seqwrite --rw=write --bs=4M --direct=1 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=2G --loops=1 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async cached seqwrite (writes 8G)\nfio --filename=$FILENAME --name=async_cached_seqwrite --rw=write --bs=4M --direct=0 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=2G --loops=1 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async uncached seqread (writes 4G)\nfio --filename=$FILENAME --name=async_uncached_seqread --rw=read --bs=4M --direct=1 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=1G --loops=50 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\n#async cached seqread (writes 4G)\nfio --filename=$FILENAME --name=async_cached_seqread --rw=read --bs=4M --direct=0 --sync=0 --numjobs=4 --ioengine=libaio --iodepth=32 --refill_buffers --size=1G --loops=50 --group_reporting | tee -a \"${LOGFILE}\"\nrm $FILENAME\n\nfstrim -a\n\nsleep 60\n\niostat | tee -a \"${LOGFILE}\"\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul> <ol> <li> <p>https://forum.proxmox.com/threads/how-to-best-benchmark-ssds.93543/\u00a0\u21a9</p> </li> </ol>"},{"location":"platform/external-secrets/","title":"External Secrets","text":"<p>After some hickups with the vault-secrets-webhook I want to give external secrets a try. With the webhook I had issues with the env variables as not all Helm Charts made it so easy to use like the kube-monitoring-stack: Grafana was using some complicated mapping setup which made it necessary to use Secrets and <code>secret-refs</code>. And thats where my issues started because the vault-secrets-webhook did not reliably replace the placeholders ....</p>"},{"location":"platform/external-secrets/#install-operator","title":"Install Operator","text":"<p>I created a new Helm Chart App for external secrets</p> Chart.yaml<pre><code>apiVersion: v2\nname: external-secrets\nversion: 0.0.0\ndependencies:\n  - name: external-secrets\n    version: 0.5.9\n    repository: https://charts.external-secrets.io\n</code></pre> values.yaml<pre><code>external-secrets:\n  serviceMonitor:\n    enabled: true\n</code></pre> <p>That will create the very basic setup for External Secrets.</p>"},{"location":"platform/external-secrets/#setup-store","title":"Setup Store","text":"<p>The next and most important part is the Secret Store. This is the actual connection to the vault and has the vault coordinates as well as access keys. To make it an easy setup, this is part of the external secrets application.</p> templates/secret-store.yaml<pre><code>apiVersion: external-secrets.io/v1\nkind: ClusterSecretStore\nmetadata:\n  name: vault-backend\nspec:\n  provider:\n    vault:\n      server: \"http://vault.vault.svc:8200\"\n      path: \"kubernetes\"\n      version: \"v2\"\n      auth:\n        # Authenticate against Vault using a Kubernetes ServiceAccount\n        # token stored in a Secret.\n        # https://www.vaultproject.io/docs/auth/kubernetes\n        kubernetes:\n          # Path where the Kubernetes authentication backend is mounted in Vault\n          mountPath: \"kubernetes\"\n          # A required field containing the Vault Role to assume.\n          role: \"externalSecret\" # (1)\n          # Optional service account field containing the name\n          # of a kubernetes ServiceAccount\n          serviceAccountRef:\n            name: \"external-secrets\"\n</code></pre> <ol> <li>This role name has to be created in the Vault or configured for the operator     to create it.</li> </ol>"},{"location":"platform/external-secrets/#usecreate-external-secret","title":"Use/Create external secret","text":"<p>Last step: create a secret over the external secrets operator. Because the store is clusterwide available, the only missing part is the <code>ExternalSecret</code> definition.</p> <p>As an example, the following is the OIDC client id and secret for grafana. I stored all the authentik OIDC clients in the vault. The external secret manifest resides in the templates folder of the monitoring stack.</p> <p>templates/grafana-oidc-secret.yaml<pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: grafana-oidc-secret\n  namespace: monitoring-stack\nspec:\n  refreshInterval: 1m\n  secretStoreRef:\n    name: vault-backend # (1)\n    kind: ClusterSecretStore\n    namespace: external-secrets\n  target:\n    name: grafana-oidc-secret\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/part-of: monitoring-stack\n      data:\n        - secretKey: oidc-id\n          remoteRef:\n            key: secret/data/framsburg/grafana/oidc # (2)\n            property: client-id # (3)\n        - secretKey: oidc-secret\n          remoteRef:\n            key: secret/data/framsburg/grafana/oidc\n            property: client-secret\n</code></pre> 1.  Name of the configured Secret Store (Make sure the kind is in sync with your     Secret Store) 2.  Path of the secret in Vault. <code>secret</code> is the name of the secret engine,     <code>data</code> is part of the path although not visible in Vault ... some Hashicorp     Vault shenanigan 3.  Key of the secret</p> <p>The generated secret will look like this:</p> <pre><code>\n</code></pre>"},{"location":"platform/external-secrets/#reference","title":"Reference","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"platform/mail-service/","title":"Mail Service","text":"<p>This document covers SMTP relay configuration for sending emails from applications running in the cluster.</p>"},{"location":"platform/mail-service/#recommendation","title":"Recommendation","text":"<p>Mailtrap is recommended for the following reasons:</p> <ul> <li>Permanent free tier (1,000 emails/month)</li> <li>Email testing/sandbox environment for development</li> <li>Production sending available on the same platform</li> <li>Simple SMTP credentials - easy to configure as a relay</li> </ul> <p>Alternative: Use Resend if you prefer a modern, developer-friendly API with a generous free tier (3,000 emails/month).</p>"},{"location":"platform/mail-service/#service-comparison","title":"Service Comparison","text":"Service Free Tier Duration Recommendation Mailtrap 1,000 emails/month Permanent \u2705 Recommended Resend 3,000 emails/month Permanent \u2705 Great alternative Brevo 300 emails/day Permanent \u2705 Good option Mailgun None Pay-as-you-go \u274c No free tier SendGrid 100 emails/day 60 days only \u274c Expires Postmark 100 emails/month 30 days only \u274c Expires"},{"location":"platform/mail-service/#service-candidates","title":"Service Candidates","text":""},{"location":"platform/mail-service/#mailtrap","title":"Mailtrap","text":"<p>Pricing</p> <ul> <li>Free Tier (permanent)</li> <li>Email testing/sandbox environment</li> <li>Production sending available</li> <li>SMTP and API access</li> </ul>"},{"location":"platform/mail-service/#resend","title":"Resend","text":"<p>Pricing</p> <ul> <li>Free Tier (3,000 emails/month, permanent)</li> <li>Modern developer-friendly API</li> <li>SMTP and API access</li> <li>Good documentation</li> </ul>"},{"location":"platform/mail-service/#brevo-formerly-sendinblue","title":"Brevo (formerly Sendinblue)","text":"<p>Pricing</p> <ul> <li>Free Tier (300 emails/day, permanent)</li> <li>Established service</li> <li>SMTP and API access</li> </ul>"},{"location":"platform/mail-service/#mailgun","title":"Mailgun","text":"<p>Pricing</p> <ul> <li>No free tier (pay-as-you-go only)</li> <li>Not recommended for homelab use</li> </ul>"},{"location":"platform/mail-service/#sendgrid","title":"SendGrid","text":"<p>Pricing</p> <ul> <li>Free Tier only valid for 60 days</li> <li>Not recommended for long-term use</li> </ul>"},{"location":"platform/mail-service/#postmark","title":"Postmark","text":"<p>Pricing</p> <ul> <li>Free Tier only valid for 30 days</li> <li>Not recommended for long-term use</li> </ul>"},{"location":"platform/mail-service/#architecture","title":"Architecture","text":"<p>A central SMTP relay service runs inside the cluster and forwards all outgoing mail to Mailtrap. Applications connect to the internal relay without needing external credentials.</p> <pre><code>flowchart TB\n    subgraph cluster[Kubernetes Cluster]\n        authentik[Authentik]\n        grafana[Grafana]\n        apps[Other Apps]\n\n        subgraph smtp-ns[smtp namespace]\n            relay[smtp-relay&lt;br/&gt;ClusterIP :25]\n        end\n\n        authentik --&gt; relay\n        grafana --&gt; relay\n        apps --&gt; relay\n    end\n\n    relay --&gt; mailtrap[Mailtrap]</code></pre> <p>Benefits:</p> <ul> <li>Single point of configuration for external SMTP credentials</li> <li>Apps use simple internal DNS (<code>smtp-relay.smtp.svc.cluster.local:25</code>)</li> <li>No authentication required for internal apps</li> <li>Easy to switch upstream providers without touching app configs</li> </ul>"},{"location":"platform/mail-service/#smtp-relay-options","title":"SMTP Relay Options","text":"Solution Resources Notes docker-mailserver ~256MB RAM Full-featured, mature, recommended Maddy ~50MB RAM Lightweight alternative, single binary"},{"location":"platform/mail-service/#cluster-configuration","title":"Cluster Configuration","text":""},{"location":"platform/mail-service/#namespace","title":"Namespace","text":"<pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: smtp\n</code></pre>"},{"location":"platform/mail-service/#external-secret-for-mailtrap","title":"External Secret for Mailtrap","text":"<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: mailtrap-credentials\n  namespace: smtp\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: onepassword-connect\n  target:\n    name: mailtrap-credentials\n  data:\n    - secretKey: SMTP_USER\n      remoteRef:\n        key: mailtrap\n        property: username\n    - secretKey: SMTP_PASS\n      remoteRef:\n        key: mailtrap\n        property: password\n</code></pre>"},{"location":"platform/mail-service/#docker-mailserver-deployment","title":"docker-mailserver Deployment","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: mailserver-config\n  namespace: smtp\ndata:\n  postfix-relaymap.cf: |\n    * [live.smtp.mailtrap.io]:587\n  postfix-sasl-password.cf: |\n    [live.smtp.mailtrap.io]:587 $(RELAY_USER):$(RELAY_PASSWORD)\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smtp-relay\n  namespace: smtp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: smtp-relay\n  template:\n    metadata:\n      labels:\n        app: smtp-relay\n    spec:\n      containers:\n        - name: mailserver\n          image: ghcr.io/docker-mailserver/docker-mailserver:latest\n          ports:\n            - containerPort: 25\n          env:\n            - name: OVERRIDE_HOSTNAME\n              value: \"mail.cluster.local\"\n            - name: ENABLE_OPENDKIM\n              value: \"0\"\n            - name: ENABLE_OPENDMARC\n              value: \"0\"\n            - name: ENABLE_POLICYD_SPF\n              value: \"0\"\n            - name: ENABLE_AMAVIS\n              value: \"0\"\n            - name: ENABLE_SPAMASSASSIN\n              value: \"0\"\n            - name: ENABLE_CLAMAV\n              value: \"0\"\n            - name: ENABLE_FAIL2BAN\n              value: \"0\"\n            - name: ENABLE_POSTGREY\n              value: \"0\"\n            - name: SMTP_ONLY\n              value: \"1\"\n            - name: DEFAULT_RELAY_HOST\n              value: \"[live.smtp.mailtrap.io]:587\"\n            - name: RELAY_USER\n              valueFrom:\n                secretKeyRef:\n                  name: mailtrap-credentials\n                  key: SMTP_USER\n            - name: RELAY_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: mailtrap-credentials\n                  key: SMTP_PASS\n          volumeMounts:\n            - name: config\n              mountPath: /tmp/docker-mailserver\n              readOnly: true\n            - name: mail-data\n              mountPath: /var/mail\n            - name: mail-state\n              mountPath: /var/mail-state\n          resources:\n            requests:\n              memory: \"128Mi\"\n              cpu: \"50m\"\n            limits:\n              memory: \"256Mi\"\n              cpu: \"200m\"\n      volumes:\n        - name: config\n          configMap:\n            name: mailserver-config\n        - name: mail-data\n          emptyDir: {}\n        - name: mail-state\n          emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: smtp-relay\n  namespace: smtp\nspec:\n  selector:\n    app: smtp-relay\n  ports:\n    - port: 25\n      targetPort: 25\n</code></pre>"},{"location":"platform/mail-service/#application-configuration","title":"Application Configuration","text":"<p>Applications connect to the internal relay. No credentials required.</p>"},{"location":"platform/mail-service/#authentik","title":"Authentik","text":"<pre><code>email:\n  host: smtp-relay.smtp.svc.cluster.local\n  port: 25\n  use_tls: false\n  use_ssl: false\n  from: \"Authentik &lt;authentik@&lt;your-domain&gt;&gt;\"\n</code></pre>"},{"location":"platform/mail-service/#grafana","title":"Grafana","text":"<pre><code>smtp:\n  enabled: true\n  host: smtp-relay.smtp.svc.cluster.local:25\n  from_address: grafana@&lt;your-domain&gt;\n</code></pre>"},{"location":"platform/mail-service/#generic-application","title":"Generic Application","text":"<pre><code>env:\n  - name: SMTP_HOST\n    value: \"smtp-relay.smtp.svc.cluster.local\"\n  - name: SMTP_PORT\n    value: \"25\"\n</code></pre>"},{"location":"platform/mail-service/#setup-steps","title":"Setup Steps","text":"<ol> <li>Create Mailtrap Account - Sign up at mailtrap.io</li> <li>Add and Verify Domain - Configure DNS records (SPF, DKIM, DMARC)</li> <li>Store Credentials - Add Mailtrap credentials to your secret store (1Password, Vault, etc.)</li> <li>Deploy SMTP Relay - Apply the namespace, external secret, and docker-mailserver deployment</li> <li>Configure Applications - Point apps to <code>smtp-relay.smtp.svc.cluster.local:25</code></li> </ol>"},{"location":"platform/mail-service/#troubleshooting","title":"Troubleshooting","text":""},{"location":"platform/mail-service/#test-smtp-relay","title":"Test SMTP Relay","text":"<pre><code># Port-forward to the relay\nkubectl port-forward -n smtp svc/smtp-relay 2525:25\n\n# Send test email\nswaks --to test@example.com --from test@&lt;your-domain&gt; --server localhost:2525\n</code></pre>"},{"location":"platform/mail-service/#check-logs","title":"Check Logs","text":"<pre><code>kubectl logs -n smtp -l app=smtp-relay -f\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"platform/postgres-cnpg/","title":"Postgres Cluster on CloudNative PG","text":"<p>CouldNative PG is a opensourced operator from Enterprise DB one of the main contributers of PostgreSQL.</p>"},{"location":"platform/postgres-cnpg/#base-setup","title":"Base Setup","text":"<p>Helm Chart</p>"},{"location":"platform/postgres-cnpg/#backup","title":"Backup","text":"<p>Similar as Zalando</p>"},{"location":"platform/postgres-cnpg/#reload","title":"Reload","text":"<p>To force restart/reload of secrets and configmaps in already created cluster, the two either require the label <code>cnpg.io/reload</code> or must be reloaded manually.</p> example with external secret<pre><code>apiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: cnpg-minio-access\nspec:\n  refreshInterval: 1m\n  secretStoreRef:\n    name: vault-backend\n    kind: ClusterSecretStore\n  target:\n    name: cnpg-minio-access\n    template:\n      metadata:\n        labels:\n          \"cnpg.io/reload\": \"true\"\n  data:\n</code></pre> <pre><code>kubectl cnpg reload\n</code></pre>"},{"location":"platform/postgres-cnpg/#monitoring","title":"Monitoring","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"platform/postgres-zalando/","title":"Database Cluster on Zalando Postgres Operator","text":"<p>The database setup is based on the zalando-operator. With one cluster for all productive applications and one or multipe for test purposes.</p> <p>The productive cluster is created as HA cluster with pg_bouncer but without a standby database.</p>"},{"location":"platform/postgres-zalando/#operator-setup","title":"Operator setup","text":""},{"location":"platform/postgres-zalando/#backup","title":"Backup","text":"<p>Use own Minio for Backup.</p> <p>Create new Policy with the name <code>SpiloS3Access</code> and the poliy configuration: <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::postgres-backups\"\n                \"arn:aws:s3:::postgres-backups/*\"\n            ]\n        }\n    ]\n}\n</code></pre></p> <p>Create new identity with <code>postgres-pod-role</code> with password and assign the previously created policy. Store both in the Vault under <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code></p>"},{"location":"platform/postgres-zalando/#setup-cluster","title":"Setup Cluster","text":"<p>Setup Cluster with own S3 Buckets, like Minio, requires some additional configurations. Some leads can be found here</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"platform/secrets-csi/","title":"Secrets with CSI","text":""},{"location":"platform/secrets-csi/#vault-secret-injection-with-csi","title":"Vault Secret Injection with CSI","text":"<p>One way to use credentials from the vault inside pods is with CSI.</p> <ul> <li>Vault post-start command to enable kubernetes auth-method</li> <li>Use Vault with CSI</li> <li>Install CSI Driver CRD with Chart</li> <li>Define a generic SecretProviderClass template as it is needed for each secret (quite a lot of boilerplate)</li> </ul> <p>In case you need the vault command you can easily log into the shell with:</p> <pre><code>$ kubectl exec -it vault-0 -- /bin/sh\n</code></pre> <p>Create the secrets with:</p> <pre><code>$ vault kv put kv-v2/k8s/framsburg/dex client-id=\"someID\" client-secret=\"someSecret\"\n</code></pre> <p>Enable and activate kubernetes auth method <pre><code>$ vault auth enable kubernetes\n$ vault write auth/kubernetes/config \\\n    issuer=\"https://kubernetes.default.svc.cluster.local\" \\\n    token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n    kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\\n    kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n</code></pre></p> <p>Create a policy: <pre><code>$ vault policy write dex-app - &lt;&lt;EOF\npolicy dex-app:\npath \"kv-v2/data/k8s/framsburg/dex\" {\n  capabilities = [\"read\"]\n}\nEOF\n</code></pre></p> <p>Write a role to map a service account with a policy</p> <pre><code>$ vault write auth/kubernetes/role/dex-app \\\n    bound_service_account_names=dex \\\n    bound_service_account_namespaces=dex \\\n    policies=dex-app \\\n    ttl=20m\nSuccess! Data written to: auth/kubernetes/role/dex-app\n</code></pre>"},{"location":"platform/secrets-csi/#secret-class","title":"Secret Class","text":"<pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: vault-dex\nspec:\n  provider: vault\n  parameters:\n    vaultAddress: \"http://vault.vault:8200\"\n    roleName: \"dex-app\"\n    objects: |\n      - objectName: \"oidc-id\"\n        secretPath: \"kv-v2/data/k8s/framsburg/dex\"\n        secretKey: \"client-id\"\n      - objectName: \"oidc-secret\"\n        secretPath: \"kv-v2/data/k8s/framsburg/dex\"\n        secretKey: \"client-secret\"\n  secretObjects:\n    - data:\n        - key: id\n          objectName: oidc-id\n        - key: secret\n          objectName: oidc-secret\n      secretName: oidc\n      type: Opaque\n</code></pre>"},{"location":"platform/secrets-csi/#volumes-in-a-chart","title":"Volumes in a Chart","text":"<pre><code>...\n  env:\n    - name: GITHUB_CLIENT_ID\n      valueFrom:\n        secretKeyRef:\n          name: oidc\n          key: id\n    - name: GITHUB_CLIENT_SECRET\n      valueFrom:\n        secretKeyRef:\n          name: oidc\n          key: secret\n  envFrom:\n    - secretRef:\n        name: oidc\n...\n  volumeMounts:\n    - name: 'secrets-store-inline'\n      mountPath: '/mnt/secrets-store'\n      readOnly: true\n  volumes:\n    - name: secrets-store-inline\n      csi:\n        driver: secrets-store.csi.k8s.io\n        readOnly: true\n        volumeAttributes:\n          secretProviderClass: vault-dex\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"setup/ansible/","title":"Pi Farm @ the river","text":""},{"location":"setup/ansible/#first-time","title":"First Time","text":"<p>Create credentials file in devops home, named <code>.vault-credentials</code> with permission 0600. This file contains the passphrase for the ansible vault.</p>"},{"location":"setup/ansible/#download-roles-from-the-ansible-galaxy","title":"Download roles from the ansible galaxy","text":"<p><code>ansible-galaxy install -r roles/requirements.yml</code></p>"},{"location":"setup/ansible/#new-servers","title":"New servers","text":"<p>To provision new (or existing systems) with the correct users and keys do the following:</p> <ul> <li>Add system to the hosts file</li> <li>Ensure the default user exists in <code>group_vars/&lt;distribution-name&gt;.yaml</code>   Otherwise add a file with the variable <code>ansible_user_first_run</code></li> <li>Run ansible with <code>ansible-playbook add-user-ssh.yaml --limit k3sworker05</code></li> </ul>"},{"location":"setup/ansible/#deploy-the-site","title":"Deploy the site","text":"<pre><code>ansible-playbook site.yml\n</code></pre>"},{"location":"setup/ansible/#upgrade-all-servers-to-latest-versions","title":"Upgrade all servers to latest versions","text":"<pre><code>ansible-playbook upgrade.yml\n</code></pre>"},{"location":"setup/ansible/#references","title":"References","text":"<ul> <li>https://github.com/prometheus/demo-site</li> </ul>"},{"location":"setup/ansible/#raspberry-firmware-update","title":"Raspberry Firmware Update","text":"<pre><code>$ sudo rpi-eeprom-update\n$ sudo apt-get install rpi-eeprom\n$ sudo rpi-eeprom-update -a\n\n$ sudo raspi-config\n</code></pre> <p>TODO:</p>"},{"location":"setup/ansible/#setup-requirements","title":"setup requirements","text":"<ul> <li> <p>update hostname: <code>sudo hostnamectl set-hostname k3s...</code></p> </li> <li> <p>cgroupt in /boot/firmware/cmdline.txt: (on ubuntu /boot/fir) append <code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1</code></p> </li> </ul>"},{"location":"setup/ansible/#install-k3s-server","title":"install k3s server","text":"<pre><code>$ export K3S_DATASTORE_ENDPOINT='postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable'\n\n$ curl -sfL https://get.k3s.io | sh -s - server --node-taint CriticalAddonsOnly=true:NoExecute --tls-san 192.168.42.60\n\n--datastore-endpoint postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable\n--tls-san k3s.framsburg.ch\n</code></pre> <pre><code>$ sudo journalctl -u k3s.service\n</code></pre> <p>get token <code>sudo cat /var/lib/rancher/k3s/server/node-token</code></p>"},{"location":"setup/ansible/#add-second-server","title":"Add second server","text":"<ul> <li>export database aka the datastore endpoint</li> <li>export Token you got from the first server</li> <li>Then start the second server with the same command as above, but with the <code>server</code> argument.</li> </ul> <pre><code>$ export K3S_DATASTORE_ENDPOINT='postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable'\n$ export K3S_TOKEN=K10c72f4e5f9fd862f0a1e91f9d1f91b16b2580621273705ee76528a66ed45f9819::server:5401d36937aa612639acb4d1083c2800\n\ncurl -sfL https://get.k3s.io | sh -s - server \\\n--datastore-endpoint postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable \\\n--tls-san k3s.framsburg.ch\n</code></pre>"},{"location":"setup/ansible/#install-agent-aka-worker-node","title":"install agent aka worker node","text":"<p>First log into agent node</p> <pre><code>export K3S_URL=https://k3s.framsburg.ch:6443\nexport K3S_TOKEN=K10c72f4e5f9fd862f0a1e91f9d1f91b16b2580621273705ee76528a66ed45f9819::server:5401d36937aa612639acb4d1083c2800\ncurl -sfL https://get.k3s.io | sh -\n</code></pre>"},{"location":"setup/ansible/#metallb","title":"MetalLB","text":"<p><code>kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\"$(openssl rand -base64 128)\" --dry-run -o yaml | kubectl apply -f -</code></p> <p>use ansible vault for secretkey.</p>"},{"location":"setup/ansible/#dual-stack-ingress","title":"Dual Stack Ingress","text":"<p>Ideas from Digitalis</p>"},{"location":"setup/ansible/#install-longhorn","title":"Install Longhorn","text":"<p>https://www.ekervhen.xyz/posts/2021-02/troubleshooting-longhorn-and-dns-networking/</p>"},{"location":"setup/ansible/#new-disk","title":"New disk","text":"<p>find out device <code>lsblk -f</code> on new devices <code>wipefs -a /dev/{{ var_disk }}</code></p> <pre><code>$ sudo fdisk -l\n$ sudo fdisk /dev/sdb\n\nCommand: n\nPartition number: 1 (default)\nFirst sector: (default)\nLast sector: (default)\nCommand: w\n\n$ sudo fdisk -l\n</code></pre> <pre><code>$ sudo mkfs -t ext4 /dev/sdb1\n</code></pre>"},{"location":"setup/ansible/#install-cert-manager","title":"install cert-manager","text":"<p>log into localhost or commander</p> <pre><code>$ export KUBECONFIG='~/.kube/config-k3s'\n\n\n# If you have installed the CRDs manually instead of with the `--set installCRDs=true` option added to your Helm install command, you should upgrade your CRD resources before upgrading the Helm chart:\n\n$ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml\n\n# Add the Jetstack Helm repository\nhelm repo add jetstack https://charts.jetstack.io\n\n# Update your local Helm chart repository cache\n$ helm repo update\n\n# Install the cert-manager Helm chart\n$ helm install cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.5.4 \\\n  --set installCRDs=true \\\n  --debug\n</code></pre>"},{"location":"setup/ansible/#install-rancher","title":"install rancher","text":"<p>log into localhost or commander</p> <pre><code>$ export KUBECONFIG='~/.kube/config-k3s'\n$ helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n\n$ kubectl create namespace cattle-system\n\n# Update your local Helm chart repository cache\n$ helm repo update\n\n$ helm install rancher rancher-latest/rancher \\\n  --namespace cattle-system \\\n  --create-namespace \\\n  --set hostname=rancher.framsburg.ch \\\n  --set bootstrapPassword=admin \\\n  --set ingress.tls.source=rancher  --wait --debug --timeout 10m0s\n\n\n$ kubectl -n cattle-system rollout status deploy/rancher\n</code></pre>"},{"location":"setup/ansible/#uninstall-cert-manager","title":"uninstall cert-manager","text":"<pre><code>$ helm uninstall cert-manager -n cert-manager\n$ kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.crds.yaml\n$ kubectl delete  job.batch/cert-manager-startupapicheck -n cert-manager\n$ kubectl delete rolebinding.rbac.authorization.k8s.io/cert-manager-startupapicheck:create-cert -n cert-manager\n$ kubectl delete role.rbac.authorization.k8s.io/cert-manager-startupapicheck:create-cert -n cert-manager\n</code></pre>"},{"location":"setup/ansible/#argo-cd","title":"argo cd","text":""},{"location":"setup/ansible/#prerequisites","title":"prerequisites","text":"<p>Install Go * [https://golang.org/doc/install]</p> <p>Install Docker * [https://docs.docker.com/engine/install/ubuntu/]</p> <p>Clone and build argo-cd according to <sup>1</sup> * <code>git clone https://github.com/argoproj/argo-cd.git</code> * <code>cd argo-cd</code> * <code>make armimage</code></p>"},{"location":"setup/ansible/#plex","title":"Plex","text":"<p>Plex image after k8s-at-home/plex has an initial issue with recognising itself as plex media server. The reason is the claim token (https://raw.githubusercontent.com/uglymagoo/plex-claim-server/master/plex-claim-server.sh)</p> <p>if [ ! -z \"${PLEX_CLAIM}\" ] &amp;&amp; [ -z \"${token}\" ]; then   echo \"Attempting to obtain server token from claim token\"   loginInfo=\"$(curl -X POST \\         -H 'X-Plex-Client-Identifier: '${clientId} \\         -H 'X-Plex-Product: Plex Media Server'\\         -H 'X-Plex-Version: 1.1' \\         -H 'X-Plex-Provides: server' \\         -H 'X-Plex-Platform: Linux' \\         -H 'X-Plex-Platform-Version: 1.0' \\         -H 'X-Plex-Device-Name: PlexMediaServer' \\         -H 'X-Plex-Device: Linux' \\         \"https://plex.tv/api/claim/exchange?token=${PLEX_CLAIM}\")\"   token=\"$(echo \"$loginInfo\" | sed -n 's/.(.)&lt;/authentication-token&gt;.*/\\1/p')\" <p>if [ \"$token\" ]; then     setPref \"PlexOnlineToken\" \"${token}\"     echo \"Plex Media Server successfully claimed\"   fi fi</p>"},{"location":"setup/ansible/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"setup/ansible/#get-rid-of-node","title":"Get rid of node:","text":"<p>kubectl drain  kubectl delete"},{"location":"setup/ansible/#list-all-helm-installations","title":"list all helm installations:","text":"<p>helm list -a  -A</p>"},{"location":"setup/ansible/#uninstall-helm-installation","title":"uninstall helm installation:","text":"<p>helm uninstall  -n"},{"location":"setup/ansible/#remove-dangling-namespaces","title":"Remove dangling namespaces:","text":"<p>( NAMESPACE=your-rogue-namespace kubectl proxy &amp; kubectl get namespace $NAMESPACE -o json |jq '.spec = {\"finalizers\":[]}' &gt;temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize )</p>"},{"location":"setup/ansible/#remove-old-logs","title":"Remove old logs:","text":"<p>sudo journalctl --rotate --vacuum-time=5s sudo journalctl --rotate --vacuum-size=500M</p>"},{"location":"setup/ansible/#remove-ssh-keys","title":"Remove SSH keys","text":"<p><code>sudo ssh-keygen -f \"/root/.ssh/known_hosts\" -R \"k3sworker02\"</code> <code>ssh-keygen -f \"/home/devops/.ssh/known_hosts\" -R \"k3sworker02\"</code></p>"},{"location":"setup/ansible/#replace-master-node","title":"Replace master node","text":"<ul> <li>Drain master node</li> <li>Move master node to end of ansible master group list (if it is at the top, it will be initialized as new cluster)</li> <li></li> </ul>"},{"location":"setup/ansible/#install-etcdctl","title":"Install etcdctl","text":"<p><pre><code>$ VERSION=\"v3.5.4\"\n$ curl -L https://github.com/etcd-io/etcd/releases/download/${VERSION}/etcd-${VERSION}-linux-arm64.tar.gz --output etcdctl-linux-arm64.tar.gz\n$ sudo tar -zxvf etcdctl-linux-arm64.tar.gz --strip-components=1 -C /usr/local/bin etcd-${VERSION}-linux-arm64/etcdctl\n</code></pre> <pre><code>$ sudo etcdctl --cacert=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt --cert=/var/lib/rancher/k3s/server/tls/etcd/client.crt --key=/var/lib/rancher/k3s/server/tls/etcd/client.key version\n</code></pre></p>"},{"location":"setup/ansible/#unix-utils","title":"Unix Utils","text":"<p>Find listening ports: <pre><code>$ sudo lsof -i -P -n | grep LISTEN\n</code></pre></p>"},{"location":"setup/ansible/#next-steps","title":"Next Steps:","text":"<ul> <li>https://greg.jeanmart.me/2020/04/13/deploy-prometheus-and-grafana-to-monitor-a-k/</li> <li>https://www.civo.com/learn/monitoring-k3s-with-the-prometheus-operator-and-custom-email-alerts</li> <li>https://github.com/atoy3731/k8s-tools-app</li> </ul>"},{"location":"setup/ansible/#references_1","title":"References:","text":"<ul> <li>https://rpi4cluster.com/k3s/k3s-hardware/</li> <li></li> <li> <p>argocd selfdeploy: [https://www.arthurkoziel.com/setting-up-argocd-with-helm/]</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul> <ol> <li> <p>https://github.com/argoproj/argo-cd/pull/3554\u00a0\u21a9</p> </li> </ol>"},{"location":"setup/bare-metal/","title":"Bare Metal Server Setup","text":"<p>This guide covers the setup and configuration of bare metal servers for the k3s cluster, including MinIO storage, backup configuration, and server-specific settings.</p> <p>Raspberry Pi Hardware Setup</p> <p>For detailed Raspberry Pi hardware setup and configuration, see the Raspberry Pi Hardware Guide.</p>"},{"location":"setup/bare-metal/#quick-start-bootstrap-new-node","title":"Quick Start: Bootstrap New Node","text":"<p>The initial setup is done with Ansible. For detailed hardware-specific instructions, refer to:</p> <ul> <li>Raspberry Pi Setup \u2014 Complete Pi hardware guide</li> <li>Proxmox VMs \u2014 VM configuration (if available)</li> </ul>"},{"location":"setup/bare-metal/#prepare-boot-device-raspberry-pi","title":"Prepare Boot Device (Raspberry PI)","text":"<ul> <li>Download newest Raspberry Pi Imager</li> <li>Image Ubuntu Server<ul> <li>OS -&gt; Other general purpose OS -&gt; Ubuntu -&gt; Ubuntu Server 24.04 LTS (64-bit)</li> <li>Customisation:<ul> <li>Set username/password to <code>ubuntu</code>/<code>ubuntu</code></li> <li>Enable SSH with password auth</li> </ul> </li> </ul> </li> </ul>"},{"location":"setup/bare-metal/#first-time-setup-notes","title":"First-Time Setup Notes","text":"<ul> <li>Initial login: <code>ubuntu</code>/<code>ubuntu</code> (change password on first login)</li> <li>Update OS: <code>sudo apt update &amp;&amp; sudo apt full-upgrade -y &amp;&amp; sudo reboot</code></li> <li>Update firmware (Raspberry Pi): <code>sudo rpi-eeprom-update -a &amp;&amp; sudo reboot</code></li> </ul>"},{"location":"setup/bare-metal/#bootstrap-process","title":"Bootstrap Process","text":"<ul> <li>Prerequisite: MAC address configured in DHCP server</li> <li>Remove old host keys if re-imaging: <code>ssh-keygen -R &lt;oldhost&gt; &amp;&amp; ssh-keygen -R &lt;ip&gt;</code></li> <li>Configure new host in ansible <code>hosts.yaml</code></li> <li>Run ansible with <code>ansible-playbook add-user-ssh.yaml --limit &lt;newhost&gt;</code></li> <li>To join cluster run <code>ansible-playbook playbooks/06_k3s_secure.yaml</code></li> </ul>"},{"location":"setup/bare-metal/#hardware-notes","title":"Hardware Notes","text":""},{"location":"setup/bare-metal/#poe-hat","title":"PoE HAT","text":"<p>See Raspberry Pi guide for fan control configuration.</p>"},{"location":"setup/bare-metal/#power-supply-ups","title":"Power Supply / UPS","text":"<p>For UPS integration: https://github.com/dzomaya/NUTandRpi</p>"},{"location":"setup/bare-metal/#status-indicators","title":"Status Indicators","text":"<p>BlinkStick Nano for visual status: https://www.blinkstick.com/products/blinkstick-nano</p>"},{"location":"setup/bare-metal/#longhorn-storage-disks","title":"Longhorn Storage Disks","text":"<p>For detailed Longhorn disk setup on Raspberry Pi workers, see the Raspberry Pi guide.</p> <p>Troubleshooting: https://www.ekervhen.xyz/posts/2021-02/troubleshooting-longhorn-and-dns-networking/</p>"},{"location":"setup/bare-metal/#minio-setup","title":"MinIO Setup","text":"<p>MinIO is used as an S3-compatible storage backend for backups (etcd snapshots, Longhorn volumes).</p> <p>TLS/Certificate Requirements</p> <p>MinIO should be set up with a proper domain and valid certificate. Many systems do not support insecure connections or custom certificates.</p>"},{"location":"setup/bare-metal/#installation","title":"Installation","text":"<p>Install MinIO using the official installer. See MinIO Documentation.</p>"},{"location":"setup/bare-metal/#tls-certificates","title":"TLS Certificates","text":""},{"location":"setup/bare-metal/#option-1-certbot-lets-encrypt","title":"Option 1: Certbot (Let's Encrypt)","text":"<p>You can use an HTTP or DNS challenge. DNS challenge is described on DigitalOcean.</p> <pre><code>$ certbot certonly \\\n  --config-dir config \\\n  --work-dir workdir \\\n  --logs-dir logs \\\n  --manual \\\n  --preferred-challenges dns \\\n  --debug-challenges \\\n  -d minio.example.com\n</code></pre> <p>During execution, certbot requires you to add a specific DNS TXT record to your domain:</p> <p></p>"},{"location":"setup/bare-metal/#option-2-opnsense-acme-client","title":"Option 2: OPNsense ACME Client","text":"<p>OPNsense has an ACME service plugin for automatic certificate generation.</p> <p>Key Detail: MinIO uses the full chain (<code>cert + ca</code>) for the public certificate, not just the cert file. See MinIO TLS Documentation.</p> <p>OPNsense ACME Automation Config:</p> Field Value Name Upload Minio Certificate Run Command Upload certificate via SFTP SFTP Host minio.server SFTP Port 22 Remote Path <code>/home/minio-user/.minio/certs</code> Naming \"key.pem\" private.key Naming \"fullchain.pem\" public.crt"},{"location":"setup/bare-metal/#prometheus-monitoring","title":"Prometheus Monitoring","text":"<p>MinIO requires environment variables for Prometheus integration:</p> <pre><code>$ export MINIO_PROMETHEUS_URL=https://prometheus.example.com\n$ export MINIO_PROMETHEUS_JOB_ID=minio-job\n$ export MINIO_PROMETHEUS_AUTH_TYPE=public  # If not using JWT auth\n</code></pre> <p>Add these to the MinIO service configuration or environment file.</p>"},{"location":"setup/bare-metal/#minio-bucket-for-k3s-backups","title":"MinIO Bucket for K3S Backups","text":"<p>Create a bucket and user for etcd snapshot backups:</p> <p>Create bucket structure</p> <pre><code>$ mc mb myminio/k3s\n$ mc mb myminio/k3s/etcd-snapshot\n</code></pre> <p>Create user</p> <pre><code>$ mc admin user add myminio k3s k3sk3sk3s\n</code></pre> <p>Create policy</p> <pre><code>cat &gt; /tmp/etcd-backups-policy.json &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n      \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:PutBucketPolicy\",\n        \"s3:GetBucketPolicy\",\n        \"s3:DeleteBucketPolicy\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3s\"\n      ],\n      \"Sid\": \"\"\n    },\n    {\n      \"Action\": [\n        \"s3:AbortMultipartUpload\",\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:PutObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3s/*\"\n      ],\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n</code></pre> <p>Apply policy</p> <pre><code>$ mc admin policy create myminio etcd-backups-policy /tmp/etcd-backups-policy.json\n$ mc admin policy attach myminio etcd-backups-policy --user k3s\n</code></pre>"},{"location":"setup/bare-metal/#k3s-etcd-backup-configuration","title":"K3S Etcd Backup Configuration","text":"<p>Configure k3s to automatically back up etcd snapshots to MinIO.</p>"},{"location":"setup/bare-metal/#ansible-configuration","title":"Ansible Configuration","text":"<p>Add to <code>k3s-server.service</code> template:</p> <pre><code>[Service]\nExecStart={{ k3s_binary_path }}/k3s server \\\n    # ... other flags ...\n{% if backup_s3_enabled %}\n    --etcd-s3 \\\n    --etcd-snapshot-schedule-cron='{{ backup_schedule_cron }}' \\\n    --etcd-s3-endpoint='{{ backup_s3_endpoint }}' \\\n    --etcd-s3-endpoint-ca='{{ systemd_dir }}/k3s-server.service.crt' \\\n    --etcd-s3-bucket='{{ backup_s3_bucket }}' \\\n    --etcd-s3-folder='{{ backup_s3_folder }}' \\\n    --etcd-s3-access-key='{{ backup_s3_access_key }}' \\\n    --etcd-s3-secret-key='{{ backup_s3_secret_key }}' \\\n{% endif %}\n</code></pre>"},{"location":"setup/bare-metal/#ansible-variables","title":"Ansible Variables","text":"<p>Edit vault file with <code>ansible-vault edit group_vars/all.yaml</code> and add the access and secret key for the new bucket:</p> group_vars/all.yaml<pre><code>backup_s3_access_key: k3s\nbackup_s3_secret_key: k3sk3sk3s\n</code></pre> <p>In <code>hosts.yaml</code> or group vars:</p> hosts.yaml<pre><code>backup_schedule_cron: '0 */6 * * *'  # Every 6 hours\nbackup_s3_bucket: k3s\nbackup_s3_endpoint: minio.framsburg.ch:9000\n</code></pre> <p>In case the MinIO server uses a self-signed or custom CA certificate, add the CA cert to the k3s server nodes by adding <code>backup_s3_endpoint_ca</code>: to the variables above:</p> hosts.yaml<pre><code>backup_s3_endpoint_ca: |\n  -----BEGIN CERTIFICATE-----\n  MIIDgTCCAmmgAwIBAgIJAJ85e+K5ngFRMA0GCSqGSIb3DQEBCwUAMGsxCzAJBgNV...\n</code></pre>"},{"location":"setup/bare-metal/#rolling-updates","title":"Rolling Updates","text":"<p>The initial Ansible playbook is optimized for cluster initialization (setup/restore), which requires this order:</p> <ol> <li>First control plane node (initializes or restores etcd)</li> <li>Other control plane nodes (sync to first)</li> <li>All worker nodes</li> </ol> <p>This is efficient for setup but would cause outages on a live cluster. For production updates, you need a sequential rolling update that properly drains nodes.</p>"},{"location":"setup/bare-metal/#update-k3s-version","title":"Update K3S Version","text":"<p>Update the version in <code>hosts.yaml</code> or manifest file.</p> <p>Use playbook <code>07_k3s_update.yml</code> and update control plane nodes one at a time:</p> <pre><code># Update each server individually\nansible-playbook playbooks/07_k3s_update.yml --limit k3s-server01\n# Wait for cluster to stabilize\nansible-playbook playbooks/07_k3s_update.yml --limit k3s-server02\n# Wait for cluster to stabilize\nansible-playbook playbooks/07_k3s_update.yml --limit k3s-server03\n</code></pre> <p>Important</p> <p>Wait for Longhorn and other storage systems to recover between reboots. They need time to stabilize or may block shutdowns and risk data loss.</p>"},{"location":"setup/bare-metal/#replace-existing-host","title":"Replace Existing Host","text":"<p>If replacing a node:</p> <ol> <li>Remove node from cluster: <code>kubectl delete node &lt;nodename&gt;</code></li> <li>Verify node is removed from etcd member list <pre><code>etcdctl member list\n</code></pre> No new node can join if the cluster is unhealthy due to missing members!</li> <li>Add new host to Ansible inventory</li> <li>Run bootstrap playbook for new node:    <pre><code>ansible-playbook add-user-ssh.yaml --limit &lt;newhost&gt;\nansible-playbook playbooks/06_k3s_secure.yaml # (1)!\n</code></pre></li> </ol> <ol> <li>Careful: If you use limit here without any other master node, you will initialize a new cluster instead of joining!</li> </ol>"},{"location":"setup/bare-metal/#hardware-specific-configuration","title":"Hardware-Specific Configuration","text":""},{"location":"setup/bare-metal/#lenovo-tiny-servers-e1000-nic-issues","title":"Lenovo Tiny Servers \u2014 E1000 NIC Issues","text":"<p>Some tiny servers (like Lenovo ThinkCentre) may have network card issues with hardware offloading. If the network adapter hangs:</p> <pre><code># Disable offloading features\nethtool -K &lt;ADAPTER&gt; gso off gro off tso off\n</code></pre> <p>Add this as a startup command in <code>/etc/network/if-up.d/</code> to persist across reboots.</p>"},{"location":"setup/bare-metal/#proxmox-adding-new-network-card","title":"Proxmox \u2014 Adding New Network Card","text":"<p>When adding a new network card to replace an old one:</p> <ol> <li> <p>Check for missing kernel modules: <pre><code>dmesg | grep -i ethernet\n# Should return no error messages\n</code></pre></p> </li> <li> <p>List all interfaces: <pre><code>ip link show\n# Note the name of the new interface (e.g., enp3s0)\n</code></pre></p> </li> <li> <p>Update <code>/etc/network/interfaces</code>: Before: <pre><code>auto lo\niface lo inet loopback\n\niface eno1 inet manual\n        gso-offload off\n        tso-offload off\n\nauto vmbr0\niface vmbr0 inet static\n        address 192.168.42.110/24\n        gateway 192.168.42.1\n        bridge-ports eno1\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n</code></pre> After: <pre><code>auto lo\niface lo inet loopback\n\niface enp3s0 inet manual\n\nauto vmbr0\niface vmbr0 inet static\n        address 192.168.42.111/24\n        gateway 192.168.42.1\n        bridge-ports enp3s0\n        bridge-stp off\n        bridge-fd 0\n        bridge-vlan-aware yes\n        bridge-vids 2-4094\n</code></pre></p> </li> <li> <p>Restart networking: <pre><code>systemctl restart networking.service\n</code></pre></p> </li> </ol>"},{"location":"setup/bare-metal/#hardware-vendors-resources","title":"Hardware Vendors &amp; Resources","text":"<p>Useful vendors for homelab hardware:</p>"},{"location":"setup/bare-metal/#rack-hardware","title":"Rack Hardware","text":"<ul> <li>MK1 Manufacturing \u2014 Custom rack solutions</li> <li>RackNex - Lenovo servers</li> <li>Uctronics - Raspberry Pi accessories</li> </ul>"},{"location":"setup/bare-metal/#servers-components","title":"Servers &amp; Components","text":"<ul> <li>45Homelab - HL15 storage server</li> <li>ServeTheHome \u2014 Project TinyMiniMicro</li> <li>PCHc.ch - Lenovo Tiny LAN cards</li> </ul>"},{"location":"setup/bare-metal/#used-hardware-ebay-examples","title":"Used Hardware (eBay Examples)","text":"<ul> <li>ServerSchmiede \u2014 TrueNAS and Supermicro configurators</li> <li>Serverando \u2014 Various servers</li> </ul> <p>Or search for \"Lenovo ThinkCentre Tiny\", \"HP EliteDesk Mini\", or \"Dell OptiPlex Micro\" for compact, power-efficient servers like:</p> <ul> <li>https://www.ebay.de/itm/404433125454?itmmeta=01J2635VSAH35D01S3P83AEHSX&amp;hash=item5e2a17c04e:g:eN8AAOSwNFxktdsz&amp;var=674539469591</li> <li>https://www.ebay.de/itm/305247609020?itmmeta=01J262NDCBCZ5V7WGW29NDQGZC&amp;hash=item47122ce0bc:g:fFcAAOSw95plqVp0&amp;itmprp=enc%3AAQAJAAAA4P%2BHsi8ZJxxkeeuXEbknuuEvnrzmVSyLnf8JsUZ6M3ubQM06d5Ztt8bkEqeBHOEQIseuaFiwu%2BYeZWs2ohLL9pgM8QdmI7XsxgytIKQ5lFwdND6qDTMi5ODfbropBH5gVAVKfJ6hmSx1MuvFA2O1cmz8DAxXaETqWV33zz1phg3vNu9c7P2qmopNvlD28mJB0UlnxGhm8I3NHdEp8FUPw4FB0Kit63lUxxvDAiRGQ5IR3LQCBl7xmJHFnhIkaDfDnBCC2rsIXj903lnHDSGRuvXc4R4DF%2FXQQa5PekGctP7R%7Ctkp%3ABFBMotbVwpFk</li> <li>https://www.ebay.de/itm/404433125454?chn=ps&amp;_ul=DE&amp;var=674539291378&amp;norover=1&amp;mkevt=1&amp;mkrid=707-166974-037691-2&amp;mkcid=2&amp;mkscid=101&amp;itemid=674539502517_404433125454&amp;targetid=2279743046334&amp;device=c&amp;mktype=pla&amp;googleloc=1002964&amp;poi=&amp;campaignid=20743725838&amp;mkgroupid=156953869822&amp;rlsatarget=pla-2279743046334&amp;abcId=9330607&amp;merchantid=5361935233&amp;geoid=1002964&amp;gad_source=1&amp;gbraid=0AAAAAD_G4xa4G7JEe2Z80T4KhK0Ii0ERO&amp;gclid=Cj0KCQjw-ai0BhDPARIsAB6hmP6Go03PZEXz5yMqYFizH6-J0wiekFwj-PhXREKvgglAMvHLL7MFWVsaAk52EALw_wcB</li> </ul> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"setup/custom-images/","title":"Custom Docker Images","text":"<p>https://github.com/norwoodj/helm-docs/blob/master/README.md https://pre-commit.com/#install https://github.com/norwoodj/helm-docs https://keepachangelog.com/en/1.0.0/</p> <p>k8s best practices for images: https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-how-and-why-to-build-small-container-images</p> <p>For the task of creating my own images I got inspired by the setup from k8s-at-home and of it's contributers like onedr0p</p>"},{"location":"setup/custom-images/#testing","title":"Testing","text":"<p>Use GOSS.</p> <p>Use <code>dgoss</code> locally which is just a docker wrapper for goss.</p>"},{"location":"setup/custom-images/#references","title":"References","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"troubleshooting/applications/","title":"Application Debugging","text":"<p>Guide for debugging application-specific issues in your k3s cluster.</p>"},{"location":"troubleshooting/applications/#quick-diagnostics","title":"Quick Diagnostics","text":"<pre><code># Check pod status\nkubectl get pods -n &lt;namespace&gt;\n\n# View pod logs\nkubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod-name&gt; -n &lt;namespace&gt; --previous  # Previous crash\n\n# Describe pod for events\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n\n# Execute shell in pod\nkubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- /bin/sh\n\n# Check resource usage\nkubectl top pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"troubleshooting/applications/#pod-startup-issues","title":"Pod Startup Issues","text":""},{"location":"troubleshooting/applications/#image-pull-errors","title":"Image Pull Errors","text":"<p>Symptoms: <code>ImagePullBackOff</code> or <code>ErrImagePull</code></p> <p>Diagnosis: <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for: Failed to pull image\n</code></pre></p> <p>Common Causes:</p> <ol> <li>Wrong image name or tag: Verify image exists</li> <li>Private registry auth missing:    <pre><code>kubectl create secret docker-registry regcred \\\n  --docker-server=&lt;registry&gt; \\\n  --docker-username=&lt;username&gt; \\\n  --docker-password=&lt;password&gt; \\\n  -n &lt;namespace&gt;\n</code></pre></li> <li>Rate limit (Docker Hub): Authenticate to increase rate limit</li> </ol>"},{"location":"troubleshooting/applications/#application-crashes-on-startup","title":"Application Crashes on Startup","text":"<p>See Common Issues - CrashLoopBackOff</p> <p>Check exit code: <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for: Exit Code\n\n# Common exit codes:\n# 0   - Success (shouldn't crash)\n# 1   - General error\n# 137 - SIGKILL (OOMKilled)\n# 143 - SIGTERM (terminated)\n</code></pre></p>"},{"location":"troubleshooting/applications/#configuration-issues","title":"Configuration Issues","text":"<p>Check ConfigMaps and Secrets:</p> <pre><code># Verify ConfigMap exists and has correct data\nkubectl get configmap &lt;name&gt; -n &lt;namespace&gt; -o yaml\n\n# Verify Secret exists\nkubectl get secret &lt;name&gt; -n &lt;namespace&gt; -o yaml\n\n# Decode secret\nkubectl get secret &lt;name&gt; -n &lt;namespace&gt; -o jsonpath='{.data.&lt;key&gt;}' | base64 -d\n\n# Check environment variables in pod\nkubectl exec &lt;pod-name&gt; -n &lt;namespace&gt; -- env\n</code></pre>"},{"location":"troubleshooting/applications/#runtime-issues","title":"Runtime Issues","text":""},{"location":"troubleshooting/applications/#out-of-memory-oomkilled","title":"Out of Memory (OOMKilled)","text":"<p>Symptoms: Pod restarts frequently, exit code 137</p> <p>Diagnosis:</p> <pre><code># Check events\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for: OOMKilled\n\n# Monitor memory usage\nkubectl top pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Fix:</p> <pre><code># Increase memory limit\nresources:\n  limits:\n    memory: \"512Mi\"  # Increase this\n  requests:\n    memory: \"256Mi\"\n</code></pre>"},{"location":"troubleshooting/applications/#cpu-throttling","title":"CPU Throttling","text":"<p>Symptoms: Application slow, high CPU usage</p> <p>Diagnosis:</p> <pre><code># Check CPU usage\nkubectl top pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Fix:</p> <pre><code>resources:\n  limits:\n    cpu: \"1000m\"  # Increase (1 core = 1000m)\n  requests:\n    cpu: \"500m\"\n</code></pre>"},{"location":"troubleshooting/applications/#database-connection-issues","title":"Database Connection Issues","text":"<p>Symptoms: Application can't connect to database</p> <p>Check:</p> <ol> <li> <p>Database pod is running:    <pre><code>kubectl get pods -n &lt;namespace&gt; -l app=postgres\n</code></pre></p> </li> <li> <p>Service exists:    <pre><code>kubectl get svc &lt;db-service&gt; -n &lt;namespace&gt;\n</code></pre></p> </li> <li> <p>Test connectivity from app pod:    <pre><code>kubectl exec -it &lt;app-pod&gt; -n &lt;namespace&gt; -- nc -zv &lt;db-service&gt; 5432\n</code></pre></p> </li> <li> <p>Check credentials: Verify secret exists and app uses it</p> </li> </ol>"},{"location":"troubleshooting/applications/#livenessreadiness-probe-failures","title":"Liveness/Readiness Probe Failures","text":"<p>Symptoms: Pod keeps restarting, shows not ready</p> <p>Diagnosis:</p> <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for: Liveness probe failed / Readiness probe failed\n</code></pre> <p>Fix: Adjust probe timing:</p> <pre><code>livenessProbe:\n  httpGet:\n    path: /health\n    port: 8080\n  initialDelaySeconds: 60  # Give app time to start\n  periodSeconds: 10\n  timeoutSeconds: 5\n  failureThreshold: 3\n</code></pre>"},{"location":"troubleshooting/applications/#debugging-techniques","title":"Debugging Techniques","text":""},{"location":"troubleshooting/applications/#interactive-debugging","title":"Interactive Debugging","text":"<p>Enter shell in running pod:</p> <pre><code>kubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- /bin/sh\n# or /bin/bash if available\n</code></pre> <p>Run debug container in same pod:</p> <pre><code>kubectl debug &lt;pod-name&gt; -n &lt;namespace&gt; -it --image=nicolaka/netshoot\n</code></pre>"},{"location":"troubleshooting/applications/#log-analysis","title":"Log Analysis","text":"<p>Follow logs:</p> <pre><code>kubectl logs -f &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Multiple containers in pod:</p> <pre><code># List containers\nkubectl get pod &lt;pod-name&gt; -n &lt;namespace&gt; -o jsonpath='{.spec.containers[*].name}'\n\n# View specific container\nkubectl logs &lt;pod-name&gt; -c &lt;container-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Logs from all pods in deployment:</p> <pre><code>kubectl logs -n &lt;namespace&gt; -l app=&lt;app-label&gt; --tail=100\n</code></pre>"},{"location":"troubleshooting/applications/#events","title":"Events","text":"<p>Check recent events:</p> <pre><code># Namespace-specific\nkubectl get events -n &lt;namespace&gt; --sort-by='.lastTimestamp' | tail -20\n\n# Watch events\nkubectl get events -n &lt;namespace&gt; --watch\n</code></pre>"},{"location":"troubleshooting/applications/#port-forwarding-for-testing","title":"Port Forwarding for Testing","text":"<p>Forward pod port to localhost:</p> <pre><code>kubectl port-forward &lt;pod-name&gt; -n &lt;namespace&gt; 8080:80\n# Access at http://localhost:8080\n</code></pre>"},{"location":"troubleshooting/applications/#application-specific-debugging","title":"Application-Specific Debugging","text":""},{"location":"troubleshooting/applications/#web-applications","title":"Web Applications","text":"<p>Test endpoints:</p> <pre><code># HTTP test\nkubectl exec -it &lt;pod-name&gt; -n &lt;namespace&gt; -- curl localhost:8080/health\n\n# With port-forward\nkubectl port-forward &lt;pod-name&gt; -n &lt;namespace&gt; 8080:8080\ncurl http://localhost:8080\n</code></pre>"},{"location":"troubleshooting/applications/#database-applications","title":"Database Applications","text":"<p>PostgreSQL:</p> <pre><code># Enter psql\nkubectl exec -it &lt;postgres-pod&gt; -n &lt;namespace&gt; -- psql -U &lt;username&gt; -d &lt;database&gt;\n\n# Check connections\nkubectl exec -it &lt;postgres-pod&gt; -n &lt;namespace&gt; -- psql -U postgres -c \"SELECT * FROM pg_stat_activity;\"\n</code></pre>"},{"location":"troubleshooting/applications/#background-jobsworkers","title":"Background Jobs/Workers","text":"<p>Check job status:</p> <pre><code>kubectl get jobs -n &lt;namespace&gt;\nkubectl describe job &lt;job-name&gt; -n &lt;namespace&gt;\n\n# Check job pod logs\nkubectl logs job/&lt;job-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>CronJob debugging:</p> <pre><code>kubectl get cronjobs -n &lt;namespace&gt;\nkubectl get jobs -n &lt;namespace&gt; --sort-by=.status.startTime\n\n# Manually trigger CronJob\nkubectl create job --from=cronjob/&lt;cronjob-name&gt; test-run -n &lt;namespace&gt;\n</code></pre>"},{"location":"troubleshooting/applications/#performance-debugging","title":"Performance Debugging","text":""},{"location":"troubleshooting/applications/#slow-application-response","title":"Slow Application Response","text":"<p>Check resource usage:</p> <pre><code>kubectl top pod &lt;pod-name&gt; -n &lt;namespace&gt;\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre> <p>Check node resources:</p> <pre><code>kubectl top nodes\nkubectl describe node &lt;node-name&gt;\n</code></pre>"},{"location":"troubleshooting/applications/#common-patterns","title":"Common Patterns","text":""},{"location":"troubleshooting/applications/#init-container-failures","title":"Init Container Failures","text":"<p>Symptoms: Pod stuck in <code>Init:0/1</code></p> <p>Check init container logs:</p> <pre><code>kubectl logs &lt;pod-name&gt; -c &lt;init-container&gt; -n &lt;namespace&gt;\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre>"},{"location":"troubleshooting/applications/#quick-reference","title":"Quick Reference","text":"<pre><code># Pod status\nkubectl get pods -n &lt;namespace&gt;\n\n# Pod logs\nkubectl logs &lt;pod&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod&gt; -n &lt;namespace&gt; --previous\nkubectl logs -f &lt;pod&gt; -n &lt;namespace&gt;\n\n# Pod description\nkubectl describe pod &lt;pod&gt; -n &lt;namespace&gt;\n\n# Execute commands\nkubectl exec &lt;pod&gt; -n &lt;namespace&gt; -- &lt;command&gt;\nkubectl exec -it &lt;pod&gt; -n &lt;namespace&gt; -- /bin/sh\n\n# Port forwarding\nkubectl port-forward &lt;pod&gt; -n &lt;namespace&gt; 8080:80\n\n# Resource usage\nkubectl top pod &lt;pod&gt; -n &lt;namespace&gt;\n\n# Events\nkubectl get events -n &lt;namespace&gt; --sort-by='.lastTimestamp'\n\n# Debug container\nkubectl debug &lt;pod&gt; -n &lt;namespace&gt; -it --image=nicolaka/netshoot\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"troubleshooting/certificates/","title":"Certificate Issues","text":"<p>Common certificate-related problems and their solutions.</p>"},{"location":"troubleshooting/certificates/#quick-diagnostics","title":"Quick Diagnostics","text":"<pre><code># Check all certificates\nkubectl get certificate -A\n\n# Check specific certificate\nkubectl describe certificate &lt;cert-name&gt; -n &lt;namespace&gt;\n\n# Check certificate requests\nkubectl get certificaterequest -A\n\n# Check cert-manager logs\nkubectl logs -n cert-manager -l app=cert-manager --tail=100\n</code></pre>"},{"location":"troubleshooting/certificates/#certificate-not-being-issued","title":"Certificate Not Being Issued","text":""},{"location":"troubleshooting/certificates/#symptoms","title":"Symptoms","text":"<ul> <li>Certificate shows <code>Ready: False</code></li> <li>HTTPS access fails with invalid certificate</li> <li>IngressRoute shows certificate pending</li> </ul>"},{"location":"troubleshooting/certificates/#common-causes-fixes","title":"Common Causes &amp; Fixes","text":""},{"location":"troubleshooting/certificates/#1-dns-validation-failing-acme-dns-01-challenge","title":"1. DNS Validation Failing (ACME DNS-01 Challenge)","text":"<p>Problem: Let's Encrypt can't validate domain ownership via DNS</p> <p>Diagnosis: <pre><code>kubectl describe challenge -A\n# Look for error messages about DNS\n</code></pre></p> <p>Fix: - Verify DNS provider credentials are correct (in Secret) - Check CloudFlare API token has correct permissions - Verify domain is configured in DNS provider - Wait for DNS propagation (can take minutes)</p> <pre><code># Verify DNS record exists\ndig _acme-challenge.&lt;your-domain&gt; TXT\n\n# Delete and retry\nkubectl delete certificate &lt;cert-name&gt; -n &lt;namespace&gt;\n# ArgoCD will recreate it\n</code></pre>"},{"location":"troubleshooting/certificates/#2-lets-encrypt-rate-limiting","title":"2. Let's Encrypt Rate Limiting","text":"<p>Problem: Too many certificate requests in short time</p> <p>Error Message: <code>\"rate limit exceeded\"</code></p> <p>Fix: - Use Let's Encrypt staging issuer for testing - Wait 1 hour before retrying - Consolidate certificates (use wildcard certs)</p> <pre><code># Use staging issuer for testing\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: test-cert\nspec:\n  issuerRef:\n    name: letsencrypt-staging  # Change to letsencrypt-prod later\n    kind: ClusterIssuer\n</code></pre>"},{"location":"troubleshooting/certificates/#3-wrong-issuer-configuration","title":"3. Wrong Issuer Configuration","text":"<p>Problem: Certificate references non-existent ClusterIssuer</p> <p>Fix: <pre><code>kubectl get clusterissuer\nkubectl describe clusterissuer letsencrypt-prod\n</code></pre></p>"},{"location":"troubleshooting/certificates/#certificate-renewal-issues","title":"Certificate Renewal Issues","text":""},{"location":"troubleshooting/certificates/#automatic-renewal-not-working","title":"Automatic Renewal Not Working","text":"<p>cert-manager should auto-renew certificates 30 days before expiration.</p> <p>Check: <pre><code># View certificate expiration\nkubectl get certificate -A -o json | jq -r '.items[] | \"\\(.metadata.namespace)/\\(.metadata.name): \\(.status.notAfter)\"'\n\n# Check renewal status\nkubectl describe certificate &lt;cert-name&gt; -n &lt;namespace&gt;\n</code></pre></p> <p>Fix: <pre><code># Force renewal by deleting secret\nkubectl delete secret &lt;cert-secret&gt; -n &lt;namespace&gt;\n\n# Or delete certificate (ArgoCD will recreate)\nkubectl delete certificate &lt;cert-name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"troubleshooting/certificates/#certificate-expired","title":"Certificate Expired","text":""},{"location":"troubleshooting/certificates/#symptoms_1","title":"Symptoms","text":"<ul> <li>Browser shows \"Certificate Expired\" error</li> <li>Services reject HTTPS connections</li> </ul>"},{"location":"troubleshooting/certificates/#fix","title":"Fix","text":"<pre><code># Delete certificate and secret\nkubectl delete certificate &lt;cert-name&gt; -n &lt;namespace&gt;\nkubectl delete secret &lt;cert-secret&gt; -n &lt;namespace&gt;\n\n# cert-manager will reissue\n# Watch progress\nkubectl get certificate -n &lt;namespace&gt; --watch\n</code></pre>"},{"location":"troubleshooting/certificates/#wrong-certificate-being-used","title":"Wrong Certificate Being Used","text":""},{"location":"troubleshooting/certificates/#symptoms_2","title":"Symptoms","text":"<ul> <li>HTTPS works but shows wrong domain</li> <li>Certificate doesn't match expected domain</li> </ul>"},{"location":"troubleshooting/certificates/#causes","title":"Causes","text":"<ul> <li>Traefik using wrong certificate</li> <li>Multiple IngressRoutes for same host</li> <li>Missing TLS configuration</li> </ul>"},{"location":"troubleshooting/certificates/#fix_1","title":"Fix","text":"<p>Check IngressRoute TLS config: <pre><code>kubectl get ingressroute &lt;name&gt; -n &lt;namespace&gt; -o yaml\n</code></pre></p> <p>Should have: <pre><code>spec:\n  tls:\n    secretName: &lt;correct-cert-secret&gt;\n</code></pre></p> <p>Example correct IngressRoute: <pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: myapp\nspec:\n  entryPoints:\n    - websecure\n  routes:\n    - match: Host(`myapp.example.com`)\n      kind: Rule\n      services:\n        - name: myapp\n          port: 80\n  tls:\n    secretName: myapp-tls  # Must match Certificate secret name\n</code></pre></p>"},{"location":"troubleshooting/certificates/#self-signed-certificate-appearing","title":"Self-Signed Certificate Appearing","text":""},{"location":"troubleshooting/certificates/#symptoms_3","title":"Symptoms","text":"<ul> <li>Browser shows self-signed certificate warning</li> <li>Certificate issuer is \"Traefik Default Cert\"</li> </ul>"},{"location":"troubleshooting/certificates/#causes_1","title":"Causes","text":"<ul> <li>cert-manager hasn't issued certificate yet</li> <li>Certificate not referenced in IngressRoute</li> <li>Traefik using default certificate</li> </ul>"},{"location":"troubleshooting/certificates/#fix_2","title":"Fix","text":"<ol> <li> <p>Verify certificate exists and is ready:    <pre><code>kubectl get certificate -n &lt;namespace&gt;\n# Should show Ready: True\n</code></pre></p> </li> <li> <p>Verify IngressRoute references certificate:    <pre><code>tls:\n  secretName: &lt;cert-name&gt;-tls\n</code></pre></p> </li> <li> <p>Wait for cert issuance (can take 1-5 minutes)</p> </li> </ol>"},{"location":"troubleshooting/certificates/#wildcard-certificates","title":"Wildcard Certificates","text":"<p>For multiple subdomains:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: wildcard-cert\nspec:\n  secretName: wildcard-tls\n  issuerRef:\n    name: letsencrypt-prod\n    kind: ClusterIssuer\n  dnsNames:\n    - \"*.example.com\"\n    - example.com  # Also include root domain\n</code></pre> <p>Note: Wildcard certs REQUIRE DNS-01 challenge (can't use HTTP-01)</p>"},{"location":"troubleshooting/certificates/#debugging-cert-manager","title":"Debugging cert-manager","text":""},{"location":"troubleshooting/certificates/#check-cert-manager-health","title":"Check cert-manager Health","text":"<pre><code># Check pods\nkubectl get pods -n cert-manager\n\n# Check logs\nkubectl logs -n cert-manager -l app=cert-manager --tail=200\n\n# Check webhook\nkubectl get validatingwebhookconfigurations cert-manager-webhook\n</code></pre>"},{"location":"troubleshooting/certificates/#common-cert-manager-issues","title":"Common cert-manager Issues","text":"<p>Webhook Not Working: <pre><code># Check webhook pod\nkubectl get pods -n cert-manager -l app=webhook\n\n# Restart webhook\nkubectl rollout restart deployment cert-manager-webhook -n cert-manager\n</code></pre></p>"},{"location":"troubleshooting/certificates/#quick-reference","title":"Quick Reference","text":"<pre><code># List all certificates\nkubectl get certificate -A\n\n# Check certificate details\nkubectl describe certificate &lt;name&gt; -n &lt;namespace&gt;\n\n# Check certificate requests\nkubectl get certificaterequest -A\n\n# Check challenges (for ACME)\nkubectl get challenge -A\n\n# Force certificate renewal\nkubectl delete secret &lt;cert-secret&gt; -n &lt;namespace&gt;\n\n# View certificate expiration\nkubectl get certificate -A -o json | jq -r '.items[] | \"\\(.metadata.namespace)/\\(.metadata.name): \\(.status.notAfter)\"'\n\n# Check cert-manager logs\nkubectl logs -n cert-manager -l app=cert-manager --follow\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"troubleshooting/common-issues/","title":"Common Issues","text":"<p>This page covers the most common issues you'll encounter in day-to-day operations and their quick fixes.</p> <p>Quick Diagnostic Commands</p> <pre><code># Check cluster health\nkubectl get nodes\nkubectl get pods -A | grep -v Running\n\n# Check ArgoCD sync status\nkubectl get applications -n argocd-system\n\n# Check certificates\nkubectl get certificate -A\n\n# Check storage\nkubectl get pv,pvc -A\n</code></pre>"},{"location":"troubleshooting/common-issues/#pod-issues","title":"Pod Issues","text":""},{"location":"troubleshooting/common-issues/#pod-stuck-in-pending","title":"Pod Stuck in Pending","text":"<p>Symptoms: Pod shows <code>Pending</code> status for extended period</p> <p>Common Causes:</p> <ol> <li>Insufficient Resources <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for: \"Insufficient cpu\" or \"Insufficient memory\"\n</code></pre></li> </ol> <p>Fix:     - Reduce resource requests in deployment    - Add more nodes    - Remove resource limits if too restrictive</p> <ol> <li>No Available PV for PVC <pre><code>kubectl describe pvc &lt;pvc-name&gt; -n &lt;namespace&gt;\n# Look for: \"waiting for first consumer\"\n</code></pre></li> </ol> <p>Fix:     - Check if Longhorn is healthy: <code>kubectl get pods -n longhorn-system</code>    - Ensure nodes have available storage    - Check StorageClass exists: <code>kubectl get sc</code></p> <ol> <li>Node Selector Not Matching <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for: \"didn't match node selector\"\n</code></pre></li> </ol> <p>Fix: Update deployment to remove or fix node selector</p>"},{"location":"troubleshooting/common-issues/#pod-crashloopbackoff","title":"Pod CrashLoopBackOff","text":"<p>Symptoms: Pod continuously restarting</p> <p>Diagnosis: <pre><code># Check pod logs\nkubectl logs &lt;pod-name&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod-name&gt; -n &lt;namespace&gt; --previous  # logs from crashed container\n\n# Check events\nkubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p> <p>Common Causes:</p> <ol> <li>Application Error: Check logs for error messages</li> <li>Missing ConfigMap/Secret:     <pre><code>kubectl get configmap,secret -n &lt;namespace&gt;\n</code></pre></li> <li>Liveness Probe Failing: Adjust probe timing or fix health endpoint</li> <li>Init Container Failing: Check init container logs</li> </ol> <p>Fix: Depends on root cause, but often: - Fix application configuration - Ensure required secrets exist - Adjust probe configuration</p>"},{"location":"troubleshooting/common-issues/#pod-stuck-in-terminating","title":"Pod Stuck in Terminating","text":"<p>Symptoms: Pod won't delete, stuck in <code>Terminating</code> state</p> <p>Diagnosis: <pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for finalizers or stuck processes\n</code></pre></p> <p>Quick Fix (use cautiously): <pre><code># Force delete (last resort)\nkubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt; --grace-period=0 --force\n</code></pre></p>"},{"location":"troubleshooting/common-issues/#storage-issues","title":"Storage Issues","text":""},{"location":"troubleshooting/common-issues/#pvc-stuck-in-pending","title":"PVC Stuck in Pending","text":"<p>Symptoms: PersistentVolumeClaim won't bind</p> <p>Diagnosis: <pre><code>kubectl describe pvc &lt;pvc-name&gt; -n &lt;namespace&gt;\nkubectl get storageclass\n</code></pre></p> <p>Common Causes:</p> <ol> <li>No Available Storage</li> <li>Check Longhorn dashboard for capacity</li> <li> <p>Check node disk space on nodes</p> </li> <li> <p>StorageClass Not Found <pre><code>kubectl get sc\n</code></pre> Fix: Ensure StorageClass exists and matches PVC</p> </li> </ol>"},{"location":"troubleshooting/common-issues/#longhorn-volume-degraded","title":"Longhorn Volume Degraded","text":"<p>Symptoms: Longhorn dashboard shows degraded volumes</p> <p>Diagnosis: <pre><code>kubectl get volumes.longhorn.io -A\nkubectl describe volume &lt;volume-name&gt; -n longhorn-system\n</code></pre></p> <p>Common Causes:</p> <ol> <li>Node Down: Volume replica on failed node</li> <li>Disk Full: Node disk at capacity</li> <li>Network Issues: Replicas can't sync</li> </ol>"},{"location":"troubleshooting/common-issues/#network-issues","title":"Network Issues","text":""},{"location":"troubleshooting/common-issues/#service-not-accessible","title":"Service Not Accessible","text":"<p>Symptoms: Can't reach service via ClusterIP or external IP</p> <p>Diagnosis: <pre><code># Check service exists\nkubectl get svc &lt;service-name&gt; -n &lt;namespace&gt;\n\n# Check endpoints (pods backing the service)\nkubectl get endpoints &lt;service-name&gt; -n &lt;namespace&gt;\n</code></pre></p> <p>Common Causes:</p> <ol> <li>No Pods Ready: Service has no endpoints</li> <li>Label Mismatch: Pod labels don't match service selector</li> <li>Wrong Port: Service port doesn't match container port</li> </ol>"},{"location":"troubleshooting/common-issues/#ingress-not-working","title":"Ingress Not Working","text":"<p>Symptoms: Can't access service via domain name</p> <p>Diagnosis: <pre><code># Check Traefik is running\nkubectl get pods -n traefik\n\n# Check IngressRoute\nkubectl get ingressroute -n &lt;namespace&gt;\nkubectl describe ingressroute &lt;name&gt; -n &lt;namespace&gt;\n</code></pre></p> <p>Common Causes:</p> <ol> <li>DNS Not Resolving: Domain doesn't point to MetalLB IP</li> <li>IngressRoute Misconfigured: Wrong service name or port</li> <li>Traefik Not Healthy: Controller pods crashed</li> </ol>"},{"location":"troubleshooting/common-issues/#certificate-issues","title":"Certificate Issues","text":""},{"location":"troubleshooting/common-issues/#certificate-not-issued","title":"Certificate Not Issued","text":"<p>Symptoms: Certificate stuck in <code>False</code> or <code>Pending</code> status</p> <p>Diagnosis: <pre><code>kubectl get certificate -n &lt;namespace&gt;\nkubectl describe certificate &lt;cert-name&gt; -n &lt;namespace&gt;\n</code></pre></p> <p>Common Causes:</p> <ol> <li>DNS Validation Failing: ACME DNS-01 challenge can't complete</li> <li>Rate Limit: Let's Encrypt rate limits hit</li> <li>Wrong Issuer: Certificate references non-existent issuer</li> </ol>"},{"location":"troubleshooting/common-issues/#argocd-issues","title":"ArgoCD Issues","text":""},{"location":"troubleshooting/common-issues/#application-out-of-sync","title":"Application Out of Sync","text":"<p>Symptoms: ArgoCD shows application as \"OutOfSync\"</p> <p>Common Causes:</p> <ol> <li>Manual Change: Someone ran <code>kubectl apply</code> manually</li> <li>Ignored Differences: Resource has expected drift</li> <li>Git Repo Not Accessible: ArgoCD can't fetch from Git</li> </ol>"},{"location":"troubleshooting/common-issues/#quick-reference","title":"Quick Reference","text":"<pre><code># Pod status\nkubectl get pods -n &lt;namespace&gt;\nkubectl describe pod &lt;pod&gt; -n &lt;namespace&gt;\nkubectl logs &lt;pod&gt; -n &lt;namespace&gt;\n\n# Storage\nkubectl get pv,pvc -A\nkubectl get volumes.longhorn.io -A\n\n# Network\nkubectl get svc,endpoints -A\nkubectl get ingressroute -A\n\n# Certificates\nkubectl get certificate -A\n\n# ArgoCD\nkubectl get applications -n argocd-system\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"troubleshooting/disaster-recovery/","title":"Disaster Recovery","text":"<p>This guide covers how to recover from various disaster scenarios, from single component failures to complete cluster loss.</p> <p>Before Disaster Strikes - Critical Preparation</p> <p>Ensure you have:</p> <ul> <li> Break-glass kubeconfig saved externally (from <code>/etc/rancher/k3s/k3s.yaml</code>)</li> <li> Vault unseal keys stored securely outside the cluster</li> <li> Git repository backed up (GitHub provides this)</li> <li> Longhorn backups enabled to TrueNAS</li> <li> PostgreSQL backups configured</li> <li> Ansible inventory and vault passwords backed up</li> </ul>"},{"location":"troubleshooting/disaster-recovery/#quick-recovery-decision-tree","title":"Quick Recovery Decision Tree","text":"<pre><code>graph TD\n    A[Problem Detected] --&gt; B{What Failed?}\n    B --&gt;|Single Worker| C[15-60 min: Automatic rescheduling]\n    B --&gt;|Control Plane Node| D[30-60 min: Replace node]\n    B --&gt;|Storage/Longhorn| E[1-4 hrs: Restore from backup]\n    B --&gt;|Vault Sealed| F[15 min: Unseal with keys]\n    B --&gt;|Complete Cluster| G[4-8 hrs: Full rebuild]\n    B --&gt;|Network/Traefik| H[15-30 min: Restart components]</code></pre>"},{"location":"troubleshooting/disaster-recovery/#recovery-scenarios","title":"Recovery Scenarios","text":""},{"location":"troubleshooting/disaster-recovery/#scenario-1-single-worker-node-failure","title":"Scenario 1: Single Worker Node Failure","text":"<p>Impact: Minimal - workloads reschedule automatically</p> <p>Recovery Steps:</p> <ol> <li> <p>Verify node is down:    <pre><code>kubectl get nodes\n# Node shows NotReady or missing\n</code></pre></p> </li> <li> <p>Check pod redistribution:    <pre><code>kubectl get pods -A -o wide | grep &lt;failed-node&gt;\n# Should see pods rescheduling to other nodes\n</code></pre></p> </li> <li> <p>Fix or replace node:</p> </li> <li>If hardware failure: Replace hardware</li> <li>If software issue: SSH to node and investigate</li> <li> <p>If unrecoverable: Remove node and add new one</p> </li> <li> <p>Remove dead node (if needed):    <pre><code>kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-emptydir-data\nkubectl delete node &lt;node-name&gt;\n</code></pre></p> </li> <li> <p>Add replacement node:    <pre><code>cd ansible-directory\nansible-playbook add-user-ssh.yaml --limit &lt;new-node&gt;\nansible-playbook playbooks/06_k3s_secure.yaml --limit &lt;new-node&gt;\n</code></pre></p> </li> </ol> <p>Recovery Time: 15-60 minutes</p>"},{"location":"troubleshooting/disaster-recovery/#scenario-2-complete-cluster-loss","title":"Scenario 2: Complete Cluster Loss","text":"<p>Impact: Catastrophic - everything down</p> <p>This is why we use GitOps</p> <p>With GitOps, rebuilding is straightforward because all configuration is in Git.</p>"},{"location":"troubleshooting/disaster-recovery/#phase-1-rebuild-cluster-2-4-hours","title":"Phase 1: Rebuild Cluster (2-4 hours)","text":"<ol> <li> <p>Rebuild nodes with Ansible:    <pre><code>cd ansible-directory\nansible-playbook site.yml\nansible-playbook playbooks/06_k3s_secure.yaml\n</code></pre></p> </li> <li> <p>Verify cluster is up:    <pre><code>kubectl get nodes\n# All nodes should be Ready\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/disaster-recovery/#phase-2-bootstrap-argocd-30-minutes","title":"Phase 2: Bootstrap ArgoCD (30 minutes)","text":"<ol> <li> <p>Install ArgoCD manually (the only manual step):    <pre><code>kubectl create namespace argocd-system\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm install argocd argo/argo-cd -n argocd-system\n</code></pre></p> </li> <li> <p>Deploy root ApplicationSet:    <pre><code>kubectl apply -f apps-root-config/bootstrap/\n</code></pre></p> </li> <li> <p>Watch ArgoCD sync everything:    <pre><code>kubectl get applications -n argocd-system --watch\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/disaster-recovery/#phase-3-restore-data-1-4-hours","title":"Phase 3: Restore Data (1-4 hours)","text":"<ol> <li>Restore Vault:</li> <li>Restore Longhorn volume from backup</li> <li>Unseal Vault with saved keys</li> <li> <p>Or manually re-enter secrets</p> </li> <li> <p>Restore Longhorn volumes:</p> </li> <li>Longhorn UI \u2192 Backup tab</li> <li>Restore each critical volume</li> <li> <p>Update PVCs to use restored volumes</p> </li> <li> <p>Restore PostgreSQL databases:    <pre><code># CNPG will restore from backup automatically if configured\n</code></pre></p> </li> </ol> <p>Total Recovery Time: 4-8 hours</p>"},{"location":"troubleshooting/disaster-recovery/#scenario-3-vault-sealedlost","title":"Scenario 3: Vault Sealed/Lost","text":"<p>Impact: Severe - secrets unavailable, pods using secrets fail</p> <p>Recovery Steps:</p> <ol> <li> <p>Check Vault status:    <pre><code>kubectl exec -it vault-0 -n vault -- vault status\n# If sealed: true, needs unsealing\n</code></pre></p> </li> <li> <p>Unseal Vault (you need unseal keys):    <pre><code>kubectl exec -it vault-0 -n vault -- vault operator unseal &lt;key1&gt;\nkubectl exec -it vault-0 -n vault -- vault operator unseal &lt;key2&gt;\nkubectl exec -it vault-0 -n vault -- vault operator unseal &lt;key3&gt;\n# Need threshold number of keys (typically 3 of 5)\n</code></pre></p> </li> <li> <p>If Vault data lost completely:</p> </li> <li>With backup: Restore Vault data from backup</li> <li> <p>Without backup: Re-initialize Vault and re-enter all secrets (painful)</p> </li> <li> <p>Restart pods using secrets:    <pre><code># External Secrets will re-sync\nkubectl rollout restart deployment &lt;name&gt; -n &lt;namespace&gt;\n</code></pre></p> </li> </ol> <p>Recovery Time: 15 minutes (unseal), 2-8 hours (complete rebuild)</p>"},{"location":"troubleshooting/disaster-recovery/#scenario-4-storage-loss-longhorn","title":"Scenario 4: Storage Loss (Longhorn)","text":"<p>Impact: Severe - all persistent data lost if no backups</p> <p>Recovery Steps:</p> <ol> <li> <p>Check Longhorn status:    <pre><code>kubectl get pods -n longhorn-system\n</code></pre></p> </li> <li> <p>If volumes corrupted/lost:</p> </li> <li>With Backups: Restore from backup</li> <li> <p>Without Backups: Data is lost - rebuild from scratch</p> </li> <li> <p>Restore from Longhorn backup:    <pre><code># Via Longhorn UI:\n# 1. Go to Backup tab\n# 2. Find backup for lost volume\n# 3. Click \"Restore\"\n# 4. Create new volume from backup\n</code></pre></p> </li> </ol> <p>Recovery Time: 1-4 hours (depending on data size)</p>"},{"location":"troubleshooting/disaster-recovery/#scenario-5-certificate-manager-down","title":"Scenario 5: Certificate Manager Down","text":"<p>Impact: Moderate - existing certificates work, but renewals fail</p> <p>Recovery Steps:</p> <ol> <li> <p>Check cert-manager pods:    <pre><code>kubectl get pods -n cert-manager\n</code></pre></p> </li> <li> <p>Restart cert-manager:    <pre><code>kubectl rollout restart deployment cert-manager -n cert-manager\n</code></pre></p> </li> <li> <p>Force certificate renewal:    <pre><code>kubectl delete certificaterequest -A --all\n# cert-manager will recreate them\n</code></pre></p> </li> </ol> <p>Recovery Time: 15 minutes</p>"},{"location":"troubleshooting/disaster-recovery/#scenario-6-network-failure-traefikmetallb-down","title":"Scenario 6: Network Failure (Traefik/MetalLB Down)","text":"<p>Impact: Severe - can't access any services externally</p> <p>Recovery Steps:</p> <ol> <li> <p>Check MetalLB:    <pre><code>kubectl get pods -n metallb-system\nkubectl logs -n metallb-system -l app=metallb\n</code></pre></p> </li> <li> <p>Check Traefik:    <pre><code>kubectl get pods -n traefik\nkubectl logs -n traefik -l app.kubernetes.io/name=traefik\n</code></pre></p> </li> <li> <p>Restart components:    <pre><code>kubectl rollout restart deployment controller -n metallb-system\nkubectl rollout restart daemonset speaker -n metallb-system\nkubectl rollout restart deployment traefik -n traefik\n</code></pre></p> </li> </ol> <p>Recovery Time: 15-30 minutes</p>"},{"location":"troubleshooting/disaster-recovery/#backup-verification","title":"Backup Verification","text":""},{"location":"troubleshooting/disaster-recovery/#test-backups-regularly","title":"Test Backups Regularly","text":"<p>Monthly Task:</p> <ol> <li>Verify Longhorn backups:</li> <li>Longhorn UI \u2192 Backup tab</li> <li>Check last backup timestamp</li> <li> <p>Try restoring a test volume</p> </li> <li> <p>Verify PostgreSQL backups:    <pre><code>kubectl get backup -A\n</code></pre></p> </li> <li> <p>Test disaster recovery procedure (in test cluster):</p> </li> <li>Destroy test cluster</li> <li>Rebuild from scratch</li> <li>Restore data</li> <li>Document any issues</li> </ol>"},{"location":"troubleshooting/disaster-recovery/#emergency-contacts","title":"Emergency Contacts","text":"<p>External Dependencies:</p> <ul> <li>Domain/DNS: CloudFlare (has separate login)</li> <li>Git Repository: GitHub (has separate login)</li> <li>Backup Storage: TrueNAS (admin credentials)</li> </ul> <p>Critical Credentials (store in password manager):</p> <ul> <li>Break-glass kubeconfig</li> <li>Vault unseal keys</li> <li>Vault root token</li> <li>TrueNAS admin password</li> <li>Ansible vault password</li> <li>GitHub SSH keys</li> <li>CloudFlare API token</li> </ul>"},{"location":"troubleshooting/disaster-recovery/#post-recovery-tasks","title":"Post-Recovery Tasks","text":"<p>After recovering from a disaster:</p> <ol> <li>Update documentation: What worked? What didn't?</li> <li>Review root cause: Why did the failure occur?</li> <li>Improve automation: Can this be prevented or recovered faster?</li> <li>Test recovery process: Ensure it works next time</li> <li>Update backup strategy: Were backups sufficient?</li> </ol>"},{"location":"troubleshooting/disaster-recovery/#prevention","title":"Prevention","text":"<p>Best Practices:</p> <ul> <li>\u2705 Regular backups (automated)</li> <li>\u2705 Test restore procedures</li> <li>\u2705 Monitor backup success/failures</li> <li>\u2705 Keep credentials secure but accessible</li> <li>\u2705 Document everything (like this!)</li> <li>\u2705 Use GitOps (so configuration is always in Git)</li> <li>\u2705 High availability for critical components</li> <li> <p>\u2705 Regular maintenance and updates</p> </li> <li> <p>[CSI]: Container Storage Interface</p> </li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"troubleshooting/maintenance/","title":"Maintenance Tasks","text":"<p>Regular maintenance tasks to keep your k3s cluster healthy and running smoothly.</p>"},{"location":"troubleshooting/maintenance/#daily-tasks","title":"Daily Tasks","text":""},{"location":"troubleshooting/maintenance/#check-cluster-health","title":"Check Cluster Health","text":"<pre><code># Quick cluster health check\nkubectl get nodes\nkubectl get pods -A | grep -v Running\nkubectl get applications -n argocd-system | grep -v Synced\n\n# Check ArgoCD sync status\nkubectl get applications -n argocd-system -o custom-columns=NAME:.metadata.name,SYNC:.status.sync.status,HEALTH:.status.health.status\n</code></pre>"},{"location":"troubleshooting/maintenance/#monitor-resource-usage","title":"Monitor Resource Usage","text":"<pre><code># Node resources\nkubectl top nodes\n\n# Top memory consumers\nkubectl top pods -A --sort-by=memory | head -20\n\n# Top CPU consumers\nkubectl top pods -A --sort-by=cpu | head -20\n</code></pre>"},{"location":"troubleshooting/maintenance/#weekly-tasks","title":"Weekly Tasks","text":""},{"location":"troubleshooting/maintenance/#review-storage-usage","title":"Review Storage Usage","text":"<pre><code># Check PV usage\nkubectl get pv -o custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage,STATUS:.status.phase\n\n# Check Longhorn UI for disk usage\n# Clean up Released PVs\nkubectl get pv | grep Released\n</code></pre>"},{"location":"troubleshooting/maintenance/#check-certificate-expiration","title":"Check Certificate Expiration","text":"<pre><code># List certificates and expiration\nkubectl get certificate -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,READY:.status.conditions[0].status,EXPIRY:.status.notAfter\n</code></pre>"},{"location":"troubleshooting/maintenance/#review-failed-backups","title":"Review Failed Backups","text":"<pre><code># Check Longhorn backup status via UI\n# Check PostgreSQL backup status\nkubectl get backup -A\n</code></pre>"},{"location":"troubleshooting/maintenance/#monthly-tasks","title":"Monthly Tasks","text":""},{"location":"troubleshooting/maintenance/#system-updates","title":"System Updates","text":"<p>OS Updates (via Ansible):</p> <pre><code>cd ansible-directory\n\n# Check for updates\nansible all -m shell -a \"apt update &amp;&amp; apt list --upgradable\"\n\n# Apply updates (one node at a time for workers)\nansible-playbook upgrade.yml --limit &lt;node-name&gt;\n\n# Reboot if needed\nansible-playbook reboot.yml --limit &lt;node-name&gt;\n\n# Verify node comes back\nkubectl get nodes --watch\n</code></pre> <p>k3s Updates:</p> <pre><code># Check current version\nkubectl version --short\n\n# Update via Ansible\nansible-playbook playbooks/06_k3s_secure.yaml\n</code></pre>"},{"location":"troubleshooting/maintenance/#backup-verification","title":"Backup Verification","text":"<p>Test Longhorn Restore:</p> <ol> <li>Choose non-critical volume</li> <li>Create backup</li> <li>Delete volume</li> <li>Restore from backup</li> <li>Verify data integrity</li> </ol>"},{"location":"troubleshooting/maintenance/#cleanup-tasks","title":"Cleanup Tasks","text":"<p>Clean Up Old Docker Images:</p> <pre><code># On each node\nssh &lt;node&gt;\nsudo crictl rmi --prune\n</code></pre> <p>Clean Up Old Snapshots via Longhorn UI</p> <p>Clean Up Completed Jobs:</p> <pre><code># List old jobs\nkubectl get jobs -A --field-selector status.successful=1\n\n# Delete jobs older than 7 days\nkubectl delete job -n &lt;namespace&gt; &lt;job-name&gt;\n</code></pre>"},{"location":"troubleshooting/maintenance/#security-audit","title":"Security Audit","text":"<p>Check for Security Updates:</p> <pre><code># Check for CVEs in running images\n# Use tool like Trivy\n\n# Update base images via PRs\n</code></pre> <p>Review Secrets:</p> <pre><code># Check for secrets in namespaces\nkubectl get secrets -A\n\n# Ensure sensitive secrets are in Vault\n</code></pre>"},{"location":"troubleshooting/maintenance/#quarterly-tasks","title":"Quarterly Tasks","text":""},{"location":"troubleshooting/maintenance/#major-version-updates","title":"Major Version Updates","text":"<p>k3s Major Version Update:</p> <ol> <li>Review release notes</li> <li>Test in dev environment</li> <li>Backup cluster</li> <li>Update control plane nodes one at a time</li> <li>Update worker nodes</li> <li>Verify all applications</li> </ol>"},{"location":"troubleshooting/maintenance/#disaster-recovery-test","title":"Disaster Recovery Test","text":"<p>Full Cluster Rebuild Test:</p> <ol> <li>Document current state</li> <li>Destroy test cluster</li> <li>Rebuild from scratch</li> <li>Restore from backups</li> <li>Verify all services</li> <li>Document any issues</li> </ol>"},{"location":"troubleshooting/maintenance/#performance-review","title":"Performance Review","text":"<p>Analyze resource usage trends over 3 months: - CPU/memory trends - Storage growth - Network performance - Identify bottlenecks</p>"},{"location":"troubleshooting/maintenance/#maintenance-calendar","title":"Maintenance Calendar","text":""},{"location":"troubleshooting/maintenance/#daily","title":"Daily","text":"<ul> <li> Check cluster health</li> <li> Review critical alerts</li> </ul>"},{"location":"troubleshooting/maintenance/#weekly","title":"Weekly","text":"<ul> <li> Review storage usage</li> <li> Check certificate expiration</li> <li> Review failed backups</li> </ul>"},{"location":"troubleshooting/maintenance/#monthly","title":"Monthly","text":"<ul> <li> Apply OS updates</li> <li> Backup verification</li> <li> Cleanup tasks</li> <li> Security audit</li> </ul>"},{"location":"troubleshooting/maintenance/#quarterly","title":"Quarterly","text":"<ul> <li> Major version updates</li> <li> Disaster recovery test</li> <li> Performance review</li> </ul>"},{"location":"troubleshooting/maintenance/#maintenance-windows","title":"Maintenance Windows","text":""},{"location":"troubleshooting/maintenance/#planning-a-maintenance-window","title":"Planning a Maintenance Window","text":"<ol> <li>Schedule: Choose low-traffic time</li> <li>Notify: Inform users (if applicable)</li> <li>Backup: Ensure recent backups exist</li> <li>Test: Test changes in dev first</li> <li>Execute: Perform maintenance</li> <li>Verify: Check all services</li> </ol>"},{"location":"troubleshooting/maintenance/#drain-node-for-maintenance","title":"Drain Node for Maintenance","text":"<pre><code># Drain node (evict pods)\nkubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-emptydir-data\n\n# Perform maintenance\n# ...\n\n# Uncordon node (allow scheduling)\nkubectl uncordon &lt;node-name&gt;\n\n# Verify\nkubectl get nodes\n</code></pre>"},{"location":"troubleshooting/maintenance/#useful-scripts","title":"Useful Scripts","text":""},{"location":"troubleshooting/maintenance/#health-check-script","title":"Health Check Script","text":"<pre><code>#!/bin/bash\n# cluster-health-check.sh\n\necho \"=== Cluster Health Check ===\"\necho \"\"\n\necho \"Node Status:\"\nkubectl get nodes\necho \"\"\n\necho \"Failed Pods:\"\nkubectl get pods -A | grep -v Running | grep -v Completed\necho \"\"\n\necho \"ArgoCD Sync Status:\"\nkubectl get applications -n argocd-system | grep -v Synced\necho \"\"\n\necho \"Certificate Status:\"\nkubectl get certificate -A | grep -v \"True\"\necho \"\"\n\necho \"Resource Usage:\"\nkubectl top nodes\n</code></pre>"},{"location":"troubleshooting/maintenance/#cleanup-script","title":"Cleanup Script","text":"<pre><code>#!/bin/bash\n# cleanup.sh\n\necho \"Cleaning up old jobs...\"\nkubectl delete jobs --field-selector status.successful=1 -A\n\necho \"Cleaning up old pods...\"\nkubectl delete pods --field-selector status.phase=Succeeded -A\nkubectl delete pods --field-selector status.phase=Failed -A\n\necho \"Done!\"\n</code></pre>"},{"location":"troubleshooting/maintenance/#quick-reference","title":"Quick Reference","text":"<pre><code># Check cluster health\nkubectl get nodes\nkubectl get pods -A | grep -v Running\n\n# Check resource usage\nkubectl top nodes\nkubectl top pods -A --sort-by=memory\n\n# Check ArgoCD\nkubectl get applications -n argocd-system\n\n# Check certificates\nkubectl get certificate -A\n\n# Check storage\nkubectl get pv,pvc -A\n\n# Drain node\nkubectl drain &lt;node&gt; --ignore-daemonsets --delete-emptydir-data\n\n# Uncordon node\nkubectl uncordon &lt;node&gt;\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"troubleshooting/network/","title":"Network Debugging","text":"<p>Troubleshooting guide for network-related issues including connectivity, DNS, ingress, and load balancing.</p>"},{"location":"troubleshooting/network/#quick-diagnostics","title":"Quick Diagnostics","text":"<pre><code># Check network pods\nkubectl get pods -n kube-system | grep -E \"(coredns|flannel)\"\nkubectl get pods -n traefik\nkubectl get pods -n metallb-system\n\n# Check services and endpoints\nkubectl get svc,endpoints -A | grep &lt;service&gt;\n\n# Check ingress routes\nkubectl get ingressroute -A\n\n# Test DNS\nkubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- nslookup kubernetes.default\n</code></pre>"},{"location":"troubleshooting/network/#dns-resolution-issues","title":"DNS Resolution Issues","text":""},{"location":"troubleshooting/network/#internal-dns-not-working","title":"Internal DNS Not Working","text":"<p>Symptoms: - Pods can't resolve service names - <code>nslookup &lt;service&gt;.&lt;namespace&gt;.svc.cluster.local</code> fails</p> <p>Diagnosis:</p> <pre><code># Check CoreDNS is running\nkubectl get pods -n kube-system -l k8s-app=kube-dns\n\n# Check CoreDNS logs\nkubectl logs -n kube-system -l k8s-app=kube-dns --tail=100\n\n# Test DNS from pod\nkubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- nslookup kubernetes.default.svc.cluster.local\n</code></pre> <p>Common Fixes:</p> <ol> <li> <p>CoreDNS pods down: Restart them    <pre><code>kubectl rollout restart deployment coredns -n kube-system\n</code></pre></p> </li> <li> <p>Wrong DNS server in pod:    <pre><code>kubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- cat /etc/resolv.conf\n# Should show: nameserver 10.43.0.10 (or similar cluster IP)\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/network/#external-dns-not-resolving","title":"External DNS Not Resolving","text":"<p>Symptoms: Can't reach external websites from pods</p> <p>Fix: Check CoreDNS forward configuration: <pre><code>kubectl edit configmap coredns -n kube-system\n# Ensure: forward . /etc/resolv.conf\n</code></pre></p>"},{"location":"troubleshooting/network/#pod-to-pod-communication-issues","title":"Pod-to-Pod Communication Issues","text":""},{"location":"troubleshooting/network/#pods-cant-communicate-across-nodes","title":"Pods Can't Communicate Across Nodes","text":"<p>Diagnosis:</p> <pre><code># Test from pod\nkubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- ping &lt;other-pod-ip&gt;\n\n# Check CNI (Flannel)\nkubectl get pods -n kube-system -l app=flannel\nkubectl logs -n kube-system -l app=flannel --tail=100\n</code></pre> <p>Fix:</p> <pre><code># Restart Flannel\nkubectl rollout restart daemonset kube-flannel -n kube-system\n</code></pre>"},{"location":"troubleshooting/network/#service-access-issues","title":"Service Access Issues","text":"<p>See Common Issues - Service Not Accessible</p>"},{"location":"troubleshooting/network/#metallb-issues","title":"MetalLB Issues","text":""},{"location":"troubleshooting/network/#metallb-not-assigning-ips","title":"MetalLB Not Assigning IPs","text":"<p>See Common Issues - MetalLB Not Assigning IPs</p>"},{"location":"troubleshooting/network/#metallb-layer-2-arp-issues","title":"MetalLB Layer 2 ARP Issues","text":"<p>Symptoms: External IP assigned but not reachable</p> <p>Diagnosis:</p> <pre><code># Check MetalLB speaker logs\nkubectl logs -n metallb-system -l component=speaker --tail=100\n\n# Check ARP table (on a client machine)\narp -a | grep &lt;external-ip&gt;\n</code></pre> <p>Fix: Ensure network allows ARP, verify speaker pods run on all nodes</p>"},{"location":"troubleshooting/network/#traefik-ingress-issues","title":"Traefik Ingress Issues","text":""},{"location":"troubleshooting/network/#ingressroute-not-creating-route","title":"IngressRoute Not Creating Route","text":"<p>Diagnosis:</p> <pre><code># Check IngressRoute\nkubectl get ingressroute &lt;name&gt; -n &lt;namespace&gt;\nkubectl describe ingressroute &lt;name&gt; -n &lt;namespace&gt;\n\n# Check Traefik logs\nkubectl logs -n traefik -l app.kubernetes.io/name=traefik --tail=100 | grep &lt;your-domain&gt;\n\n# Check Traefik dashboard\nkubectl port-forward -n traefik svc/traefik 9000:9000\n# Open http://localhost:9000/dashboard/\n</code></pre> <p>Common Issues:</p> <ol> <li> <p>Wrong service name or port:    <pre><code>services:\n  - name: my-service  # Must match Service name\n    port: 80           # Must match Service port\n</code></pre></p> </li> <li> <p>Missing entryPoint:    <pre><code>entryPoints:\n  - websecure  # For HTTPS\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/network/#cant-access-traefik-dashboard","title":"Can't Access Traefik Dashboard","text":"<p>Fix:</p> <pre><code># Port-forward to access\nkubectl port-forward -n traefik svc/traefik 9000:9000\n# Open http://localhost:9000/dashboard/\n</code></pre>"},{"location":"troubleshooting/network/#connectivity-testing","title":"Connectivity Testing","text":""},{"location":"troubleshooting/network/#debug-container","title":"Debug Container","text":"<p>Run a debug container with network tools:</p> <pre><code>kubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- /bin/bash\n\n# Inside container:\n# ping &lt;ip&gt;\n# nslookup &lt;domain&gt;\n# curl &lt;url&gt;\n# traceroute &lt;ip&gt;\n</code></pre>"},{"location":"troubleshooting/network/#test-service-connectivity","title":"Test Service Connectivity","text":"<p>From within cluster:</p> <pre><code># Test service by name\nkubectl run -it --rm debug --image=curlimages/curl --restart=Never -- curl http://&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local\n\n# Test service by IP\nkubectl get svc &lt;service&gt; -n &lt;namespace&gt;\nkubectl run -it --rm debug --image=curlimages/curl --restart=Never -- curl http://&lt;cluster-ip&gt;:&lt;port&gt;\n</code></pre> <p>From external:</p> <pre><code># Test LoadBalancer service\ncurl http://&lt;external-ip&gt;:&lt;port&gt;\n\n# Test Ingress\ncurl -k https://&lt;domain&gt;\n</code></pre>"},{"location":"troubleshooting/network/#firewall-and-security","title":"Firewall and Security","text":""},{"location":"troubleshooting/network/#port-requirements","title":"Port Requirements","text":"<p>k3s requires: - 6443: Kubernetes API - 10250: kubelet - 2379-2380: etcd (control plane nodes) - 8472: Flannel VXLAN - 51820-51821: Flannel Wireguard</p> <p>Check firewall (on nodes): <pre><code>ssh &lt;node&gt;\nsudo iptables -L -n -v | grep -E \"(6443|10250|8472)\"\n</code></pre></p>"},{"location":"troubleshooting/network/#dns-record-management","title":"DNS Record Management","text":"<p>For external access:</p> <ol> <li> <p>Verify DNS points to MetalLB IP:    <pre><code>nslookup &lt;your-domain&gt;\n# Should resolve to your MetalLB external IP\n</code></pre></p> </li> <li> <p>Update DNS (example with CloudFlare):</p> </li> <li>Add A record: <code>*.example.com</code> \u2192 <code>&lt;metallb-ip&gt;</code></li> <li> <p>Or specific: <code>app.example.com</code> \u2192 <code>&lt;metallb-ip&gt;</code></p> </li> <li> <p>Wait for propagation:    <pre><code># Check from multiple locations\ndig &lt;your-domain&gt; @8.8.8.8\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/network/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/network/#high-network-latency","title":"High Network Latency","text":"<p>Diagnosis:</p> <pre><code># Test latency between nodes\nkubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- ping &lt;node-ip&gt;\n\n# Check network interface stats\nssh &lt;node&gt;\nifconfig\nnetstat -i\n</code></pre> <p>Causes: - Network congestion - Faulty network hardware - Wi-Fi (use wired for k8s nodes!)</p>"},{"location":"troubleshooting/network/#quick-reference","title":"Quick Reference","text":"<pre><code># Check all network components\nkubectl get pods -A | grep -E \"(coredns|flannel|traefik|metallb)\"\n\n# Test DNS\nkubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- nslookup kubernetes.default\n\n# Test connectivity to service\nkubectl run -it --rm debug --image=curlimages/curl --restart=Never -- curl http://&lt;service&gt;.&lt;namespace&gt;.svc.cluster.local\n\n# Check service endpoints\nkubectl get endpoints &lt;service&gt; -n &lt;namespace&gt;\n\n# Traefik dashboard\nkubectl port-forward -n traefik svc/traefik 9000:9000\n\n# Check Traefik logs\nkubectl logs -n traefik -l app.kubernetes.io/name=traefik --tail=200\n\n# MetalLB status\nkubectl get svc -A | grep LoadBalancer\nkubectl logs -n metallb-system -l component=speaker --tail=100\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"troubleshooting/storage/","title":"Storage Problems","text":"<p>Troubleshooting guide for storage-related issues in Longhorn and other storage systems.</p>"},{"location":"troubleshooting/storage/#quick-diagnostics","title":"Quick Diagnostics","text":"<pre><code># Check Longhorn health\nkubectl get pods -n longhorn-system\nkubectl get volumes.longhorn.io -A\n\n# Check PV/PVC status\nkubectl get pv,pvc -A\n\n# Check storage classes\nkubectl get storageclass\n\n# Check node disk space\nkubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.capacity.ephemeral-storage}{\"\\n\"}{end}'\n</code></pre>"},{"location":"troubleshooting/storage/#pvc-stuck-in-pending","title":"PVC Stuck in Pending","text":"<p>See Common Issues - PVC Stuck in Pending</p>"},{"location":"troubleshooting/storage/#longhorn-volume-degraded","title":"Longhorn Volume Degraded","text":""},{"location":"troubleshooting/storage/#symptoms","title":"Symptoms","text":"<ul> <li>Longhorn dashboard shows volume in \"Degraded\" state</li> <li>Volume has fewer replicas than configured</li> <li>Performance degradation</li> </ul>"},{"location":"troubleshooting/storage/#diagnosis","title":"Diagnosis","text":"<pre><code># Get volume status\nkubectl get volumes.longhorn.io -A\n\n# Describe specific volume\nkubectl describe volume &lt;volume-name&gt; -n longhorn-system\n\n# Check Longhorn UI\n# Navigate to: Volume \u2192 &lt;volume-name&gt; \u2192 Check replica status\n</code></pre>"},{"location":"troubleshooting/storage/#common-causes","title":"Common Causes","text":""},{"location":"troubleshooting/storage/#1-node-down-or-disconnected","title":"1. Node Down or Disconnected","text":"<p>Fix: - Bring node back online - Or force detach and rebuild replica</p> <pre><code># Check node status\nkubectl get nodes\n\n# If node permanently dead, remove it\nkubectl delete node &lt;node-name&gt;\n\n# Longhorn will rebuild replica on healthy node\n</code></pre>"},{"location":"troubleshooting/storage/#2-disk-full","title":"2. Disk Full","text":"<p>Check disk space on nodes: <pre><code># SSH to each node\nssh &lt;node&gt;\ndf -h\n\n# Check Longhorn disk usage\nls -lah /var/lib/longhorn/\ndu -sh /var/lib/longhorn/*\n</code></pre></p> <p>Fix: - Clean up old data - Expand disk - Add more nodes with storage</p>"},{"location":"troubleshooting/storage/#3-network-issues-between-replicas","title":"3. Network Issues Between Replicas","text":"<p>Check: - Network connectivity between nodes - Firewall rules - CNI (Flannel) health</p> <pre><code># Test connectivity between nodes\nkubectl run -it --rm debug --image=nicolaka/netshoot --restart=Never -- ping &lt;other-node-ip&gt;\n</code></pre>"},{"location":"troubleshooting/storage/#recovery-steps","title":"Recovery Steps","text":"<ol> <li>Identify problematic replica in Longhorn UI</li> <li>Remove failed replica (if node is dead)</li> <li>Add new replica - Longhorn automatically creates new replica</li> <li>Wait for rebuild - Can take hours for large volumes</li> </ol>"},{"location":"troubleshooting/storage/#longhorn-volume-wont-attach","title":"Longhorn Volume Won't Attach","text":""},{"location":"troubleshooting/storage/#symptoms_1","title":"Symptoms","text":"<ul> <li>Pod stuck in <code>ContainerCreating</code></li> <li>Error: \"Volume is not attached\"</li> </ul>"},{"location":"troubleshooting/storage/#diagnosis_1","title":"Diagnosis","text":"<pre><code>kubectl describe pod &lt;pod-name&gt; -n &lt;namespace&gt;\n# Look for volume attachment errors\n\nkubectl describe volume &lt;volume-name&gt; -n longhorn-system\n</code></pre>"},{"location":"troubleshooting/storage/#fixes","title":"Fixes","text":""},{"location":"troubleshooting/storage/#1-volume-stuck-on-old-node","title":"1. Volume Stuck on Old Node","text":"<pre><code># Check where volume is attached\nkubectl get volume &lt;volume-name&gt; -n longhorn-system -o jsonpath='{.status.currentNodeID}'\n\n# If node is down, force detach via Longhorn UI\n# Volume \u2192 &lt;volume&gt; \u2192 Detach \u2192 Force detach\n</code></pre>"},{"location":"troubleshooting/storage/#2-multiple-pods-trying-to-use-same-volume","title":"2. Multiple Pods Trying to Use Same Volume","text":"<p>Problem: Two pods on different nodes trying to use <code>ReadWriteOnce</code> volume</p> <p>Fix: <pre><code># Find pods using the volume\nkubectl get pods -A -o json | jq -r '.items[] | select(.spec.volumes[]?.persistentVolumeClaim.claimName==\"&lt;pvc-name&gt;\") | \"\\(.metadata.namespace)/\\(.metadata.name)\"'\n\n# Delete one of the pods\nkubectl delete pod &lt;pod-name&gt; -n &lt;namespace&gt;\n</code></pre></p>"},{"location":"troubleshooting/storage/#volume-performance-issues","title":"Volume Performance Issues","text":""},{"location":"troubleshooting/storage/#slow-io","title":"Slow I/O","text":"<p>Causes: - Node disk is slow (SD card vs NVMe) - Network bottleneck between replicas - Longhorn engine overloaded</p> <p>Diagnosis:</p> <pre><code># Check I/O performance on node\nssh &lt;node&gt;\nsudo iostat -x 5\n\n# Check Longhorn engine pods\nkubectl get pods -n longhorn-system | grep engine\nkubectl top pods -n longhorn-system | grep engine\n</code></pre> <p>Fixes:</p> <ol> <li>Use faster storage: Migrate to NVMe-backed nodes</li> <li>Reduce replica count (trade-off: less redundancy):    <pre><code>parameters:\n  numberOfReplicas: \"2\"  # Instead of 3\n</code></pre></li> <li>Use local-path storage for non-critical data</li> </ol>"},{"location":"troubleshooting/storage/#disk-space-issues","title":"Disk Space Issues","text":""},{"location":"troubleshooting/storage/#node-running-out-of-space","title":"Node Running Out of Space","text":"<p>Symptoms: - Pods evicted - \"No space left on device\" errors - Longhorn won't create new replicas</p> <p>Diagnosis:</p> <pre><code># Check node disk usage\nkubectl get nodes -o json | jq -r '.items[] | \"\\(.metadata.name): \\(.status.allocatable.ephemeralStorage)\"'\n\n# SSH to node and check\nssh &lt;node&gt;\ndf -h\ndu -sh /var/lib/longhorn/* | sort -h\n</code></pre> <p>Fixes:</p> <ol> <li> <p>Clean up old data:    <pre><code># Remove old container images\nsudo crictl rmi --prune\n\n# Remove old logs\nsudo journalctl --vacuum-time=7d\n</code></pre></p> </li> <li> <p>Delete old Longhorn backups via Longhorn UI</p> </li> <li> <p>Remove unused PVs:    <pre><code>kubectl get pv | grep Released\nkubectl delete pv &lt;pv-name&gt;\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/storage/#longhorn-exceeding-disk-reservation","title":"Longhorn Exceeding Disk Reservation","text":"<p>Error: \"Disk usage exceeded the threshold\"</p> <p>Fix: <pre><code># Via Longhorn UI: Node \u2192 &lt;node&gt; \u2192 Edit \u2192 Adjust Storage Reserved\n\n# Or via kubectl\nkubectl edit node.longhorn.io &lt;node-name&gt; -n longhorn-system\n# Adjust: spec.disks.default.storageReserved\n</code></pre></p>"},{"location":"troubleshooting/storage/#backup-issues","title":"Backup Issues","text":""},{"location":"troubleshooting/storage/#backup-failing","title":"Backup Failing","text":"<p>Symptoms: Longhorn backup shows failed status</p> <p>Diagnosis:</p> <pre><code># Check Longhorn backup target settings\nkubectl get settings.longhorn.io backup-target -n longhorn-system -o yaml\n\n# Check MinIO/S3 connectivity\nkubectl run -it --rm aws-cli --image=amazon/aws-cli --restart=Never -- s3 ls s3://&lt;bucket-name&gt;/\n</code></pre> <p>Common Causes:</p> <ol> <li>Invalid backup target URL: Verify S3/NFS URL</li> <li>Network connectivity: Can't reach backup target</li> <li>Insufficient permissions: S3 credentials lack permissions</li> </ol> <p>Fix:</p> <pre><code># Update backup target\nkubectl edit settings.longhorn.io backup-target -n longhorn-system\n\n# Update backup credentials\nkubectl edit secret longhorn-backup-target-credential -n longhorn-system\n</code></pre>"},{"location":"troubleshooting/storage/#restore-failing","title":"Restore Failing","text":"<p>Symptoms: Restore from backup fails</p> <p>Diagnosis:</p> <pre><code>kubectl get volumes.longhorn.io -A\n# Check volume status\n\n# Check Longhorn manager logs\nkubectl logs -n longhorn-system -l app=longhorn-manager --tail=200\n</code></pre> <p>Fix: Delete failed restore and retry via Longhorn UI</p>"},{"location":"troubleshooting/storage/#migrating-data-between-volumes","title":"Migrating Data Between Volumes","text":"<p>Use a Job to copy data:</p> <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: volume-migration\n  namespace: &lt;namespace&gt;\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: migration\n          image: ubuntu:latest\n          command: [\"/bin/sh\", \"-c\", \"cp -r /old/* /new/\"]\n          volumeMounts:\n            - name: old-vol\n              mountPath: /old\n            - name: new-vol\n              mountPath: /new\n      volumes:\n        - name: old-vol\n          persistentVolumeClaim:\n            claimName: old-pvc\n        - name: new-vol\n          persistentVolumeClaim:\n            claimName: new-pvc\n</code></pre>"},{"location":"troubleshooting/storage/#quick-reference","title":"Quick Reference","text":"<pre><code># Longhorn status\nkubectl get volumes.longhorn.io -A\nkubectl describe volume &lt;volume&gt; -n longhorn-system\n\n# Access Longhorn UI\nkubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80\n\n# Check all PVs\nkubectl get pv,pvc -A\n\n# Clean up Released PVs\nkubectl get pv | grep Released | awk '{print $1}' | xargs kubectl delete pv\n</code></pre> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"}]}