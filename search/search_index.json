{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Raspberry Pi GitOps Stack This document describes my current setup of my Rasperry Pi k8s cluster. Although everything should be reflected in code, usually my brain discards stuff which works ... now. The not-working is a problem for future brain \ud83d\ude04 So the text is mainly meant for me to keep track of how and why I did certain things. If somebody else finds value in it: great! Although the whole thing is a private project for educational purposes, I try to keep it as production ready as possible. Often the biggest learnings stem from corner cases. That means, at least for me, to stay true to the following points: Everything is automated, no manual kubectl commands Clear separation between public and private network Use secure connections All HTTPS connections have correct certificates from LetcEncrypt Disaster recovery is easy to do Critical parts of the system (like control-plane, networking, etc) are setup in HA","title":"Home"},{"location":"#raspberry-pi-gitops-stack","text":"This document describes my current setup of my Rasperry Pi k8s cluster. Although everything should be reflected in code, usually my brain discards stuff which works ... now. The not-working is a problem for future brain \ud83d\ude04 So the text is mainly meant for me to keep track of how and why I did certain things. If somebody else finds value in it: great! Although the whole thing is a private project for educational purposes, I try to keep it as production ready as possible. Often the biggest learnings stem from corner cases. That means, at least for me, to stay true to the following points: Everything is automated, no manual kubectl commands Clear separation between public and private network Use secure connections All HTTPS connections have correct certificates from LetcEncrypt Disaster recovery is easy to do Critical parts of the system (like control-plane, networking, etc) are setup in HA","title":"Raspberry Pi GitOps Stack"},{"location":"argocd/","text":"Argo CD Part Two: OIDC integration This part should follow after [Vault] and [Authentik] are up and running. OIDC Login First you have to create a provider and application in authentik to get a client id and secret. Afterwards the oidc credentials can be saved in the Vault and mapped over a SecretProviderClass . (Do not forget to mount the vault volumes for the secret to work [./secrets-csi.md#volumes-in-a-chart]) --- apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-argocd spec : provider : vault parameters : vaultAddress : \"http://vault.vault:8200\" roleName : \"argocd-app\" objects : | - objectName: \"oidc-id\" secretPath: \"kv-v2/data/framsburg/argocd/oidc\" secretKey: \"client-id\" - objectName: \"oidc-secret\" secretPath: \"kv-v2/data/framsburg/argocd/oidc\" secretKey: \"client-secret\" secretObjects : - data : - key : oidc.authentik.clientId objectName : oidc-id - key : oidc.authentik.clientSecret objectName : oidc-secret secretName : oidc type : Opaque labels : app.kubernetes.io/part-of : argocd # (1)! Without this label the secret reference in the argocd ConfigMap will not work and complain about that the secret key can not be found. argo-cd : server : ... config : url : https://argocd.framsburg.ch oidc.config : | name: Authentik issuer: \"https://authentik.framsburg.ch/application/o/argocd/\" clientID: \"c579d3195f85aeccaf1ecce35ef5501e023c2a6a\" clientSecret: \"$oidc:oidc.authentik.clientSecret\" requestedScopes: [\"openid\", \"profile\", \"email\"] logoutURL: \"https://authentik.framsburg.ch/if/session-end/argocd/\" rbacConfig : policy.default : role:readonly policy.csv : | g, 'authentik Admins', role:admin volumeMounts : - name : 'secrets-store-inline' mountPath : '/mnt/secrets-store' readOnly : true volumes : - name : secrets-store-inline csi : driver : 'secrets-store.csi.k8s.io' readOnly : true volumeAttributes : secretProviderClass : 'vault-argocd'","title":"ArgoCD"},{"location":"argocd/#argo-cd","text":"","title":"Argo CD"},{"location":"argocd/#part-two-oidc-integration","text":"This part should follow after [Vault] and [Authentik] are up and running.","title":"Part Two: OIDC integration"},{"location":"argocd/#oidc-login","text":"First you have to create a provider and application in authentik to get a client id and secret. Afterwards the oidc credentials can be saved in the Vault and mapped over a SecretProviderClass . (Do not forget to mount the vault volumes for the secret to work [./secrets-csi.md#volumes-in-a-chart]) --- apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-argocd spec : provider : vault parameters : vaultAddress : \"http://vault.vault:8200\" roleName : \"argocd-app\" objects : | - objectName: \"oidc-id\" secretPath: \"kv-v2/data/framsburg/argocd/oidc\" secretKey: \"client-id\" - objectName: \"oidc-secret\" secretPath: \"kv-v2/data/framsburg/argocd/oidc\" secretKey: \"client-secret\" secretObjects : - data : - key : oidc.authentik.clientId objectName : oidc-id - key : oidc.authentik.clientSecret objectName : oidc-secret secretName : oidc type : Opaque labels : app.kubernetes.io/part-of : argocd # (1)! Without this label the secret reference in the argocd ConfigMap will not work and complain about that the secret key can not be found. argo-cd : server : ... config : url : https://argocd.framsburg.ch oidc.config : | name: Authentik issuer: \"https://authentik.framsburg.ch/application/o/argocd/\" clientID: \"c579d3195f85aeccaf1ecce35ef5501e023c2a6a\" clientSecret: \"$oidc:oidc.authentik.clientSecret\" requestedScopes: [\"openid\", \"profile\", \"email\"] logoutURL: \"https://authentik.framsburg.ch/if/session-end/argocd/\" rbacConfig : policy.default : role:readonly policy.csv : | g, 'authentik Admins', role:admin volumeMounts : - name : 'secrets-store-inline' mountPath : '/mnt/secrets-store' readOnly : true volumes : - name : secrets-store-inline csi : driver : 'secrets-store.csi.k8s.io' readOnly : true volumeAttributes : secretProviderClass : 'vault-argocd'","title":"OIDC Login"},{"location":"authentik/","text":"Authentik","title":"Authentik"},{"location":"authentik/#authentik","text":"","title":"Authentik"},{"location":"bare-metal/","text":"Harware Setup of Raspberry PIs The initial setup is done with Ansible. (Optional) Rolling Update The initial ansible script is not very suitable for rolling updates as it assumes it is about to initialize a cluster which requires the order First master node which initializes (or restores) the etcd state All other master nodes which sync up to the first All worker nodes That is very efficient for setup and restore but would mean some outages if applied on a live cluster. Therefore we need a playbook which goes through every node sequentially (we have no special requirement on performance) and cares about draining nodes correctly. Ideally we can reuse roles from the cluster setup playbook.","title":"Bare-Metal"},{"location":"bare-metal/#harware-setup-of-raspberry-pis","text":"The initial setup is done with Ansible.","title":"Harware Setup of Raspberry PIs"},{"location":"bare-metal/#optional-rolling-update","text":"The initial ansible script is not very suitable for rolling updates as it assumes it is about to initialize a cluster which requires the order First master node which initializes (or restores) the etcd state All other master nodes which sync up to the first All worker nodes That is very efficient for setup and restore but would mean some outages if applied on a live cluster. Therefore we need a playbook which goes through every node sequentially (we have no special requirement on performance) and cares about draining nodes correctly. Ideally we can reuse roles from the cluster setup playbook.","title":"(Optional) Rolling Update"},{"location":"cert-manager/","text":"Cert-Manager","title":"Cert Manager"},{"location":"cert-manager/#cert-manager","text":"","title":"Cert-Manager"},{"location":"longhorn/","text":"Longhorn","title":"Longhorn"},{"location":"longhorn/#longhorn","text":"","title":"Longhorn"},{"location":"metallb/","text":"MetalLB","title":"MetalLB"},{"location":"metallb/#metallb","text":"","title":"MetalLB"},{"location":"monitoring/","text":"Monitoring with Prometheus Stack Part Two: OIDC Integration This part should follow after [Vault] and [Authentik] are up and running. Create Application in Authentik Create a new OIDC provider in authentik with Redirect URIs/Origin pointing to [https://grafana.framsburg.ch] Afterwards create a new application which uses the before created provider. Don't forget to create bindings for uses or groups. You will need the following three informations out of authentik in the next steps. OpenID Configuration Issuer Client ID Client Secret Add secrets to vault To use the vault CLI use the folling command: $ kubectl exec -it vault-0 -n vault -- /bin/sh Add the Client ID and the Client Secret as values to the vault: Create the secrets with: $ vault kv put kv-v2/framsburg/grafana/oidc client-id = \"someID\" client-secret = \"someSecret\" Create a policy to access the secrets: $ vault policy write grafana-app - <<EOF path \"kv-v2/data/framsburg/grafana/*\" { capabilities = [\"read\", \"list\"] } EOF Note Please be aware of the added /data/ . This is not a typo but something the Vault expects when referencing this secret. It is not displayed in the UI either. And as last step create a role which maps the k8s service account with the policy: $ vault write auth/kubernetes/role/grafana-app \\ bound_service_account_names = monitoring-stack-grafana \\ bound_service_account_namespaces = monitoring-stack \\ policies = grafana-app \\ ttl = 20m Secret Class Create a SecretProviderClass in the templates cluster-critical/monitoring-stack/templates/spc.yaml --- apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-grafana spec : provider : vault parameters : vaultAddress : \"http://vault.vault:8200\" roleName : \"grafana-app\" objects : | - objectName: \"oidc-id\" secretPath: \"kv-v2/data/framsburg/grafana/oidc\" secretKey: \"client-id\" - objectName: \"oidc-secret\" secretPath: \"kv-v2/data/framsburg/grafana/oidc\" secretKey: \"client-secret\" secretObjects : - data : - key : clientId objectName : oidc-id - key : clientSecret objectName : oidc-secret secretName : oidc type : Opaque Add Volumes in a Chart The Grafana-Chart doesn't allow CSI volumes to be added to the normal volume list. But it has a special value extraSecretMount for those volumes which thankfully even combines the volume and mount entry into one. cluster-critical/monitoring-stack/values.yaml kube-prometheus-stack : grafana : ... extraSecretMounts : - name : 'secrets-store-inline' mountPath : '/mnt/secrets-store' readOnly : true csi : driver : secrets-store.csi.k8s.io readOnly : true volumeAttributes : secretProviderClass : vault-grafana Set environment variables incl secrets cluster-critical/monitoring-stack/values.yaml kube-prometheus-stack : grafana : ... env : GF_AUTH_GENERIC_OAUTH_ENABLED : \"true\" GF_AUTH_GENERIC_OAUTH_NAME : \"authentik\" GF_AUTH_GENERIC_OAUTH_SCOPES : \"openid profile email\" GF_AUTH_GENERIC_OAUTH_AUTH_URL : \"https://authentik.framsburg.ch/application/o/authorize/\" GF_AUTH_GENERIC_OAUTH_TOKEN_URL : \"https://authentik.framsburg.ch/application/o/token/\" GF_AUTH_GENERIC_OAUTH_API_URL : \"https://authentik.framsburg.ch/application/o/userinfo/\" GF_AUTH_SIGNOUT_REDIRECT_URL : \"https://authentik.framsburg.ch/application/o/grafana/end-session/\" # Optionally enable auto-login (bypasses Grafana login screen) # GF_AUTH_OAUTH_AUTO_LOGIN: \"true\" # Optionally map user groups to Grafana roles # GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: \"contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'Grafana Editors') && 'Editor' || 'Viewer'\" envValueFrom : GF_AUTH_GENERIC_OAUTH_CLIENT_ID : secretKeyRef : name : oidc key : clientId GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET : secretKeyRef : name : oidc key : clientSecret","title":"Monitoring"},{"location":"monitoring/#monitoring-with-prometheus-stack","text":"","title":"Monitoring with Prometheus Stack"},{"location":"monitoring/#part-two-oidc-integration","text":"This part should follow after [Vault] and [Authentik] are up and running.","title":"Part Two: OIDC Integration"},{"location":"monitoring/#create-application-in-authentik","text":"Create a new OIDC provider in authentik with Redirect URIs/Origin pointing to [https://grafana.framsburg.ch] Afterwards create a new application which uses the before created provider. Don't forget to create bindings for uses or groups. You will need the following three informations out of authentik in the next steps. OpenID Configuration Issuer Client ID Client Secret","title":"Create Application in Authentik"},{"location":"monitoring/#add-secrets-to-vault","text":"To use the vault CLI use the folling command: $ kubectl exec -it vault-0 -n vault -- /bin/sh Add the Client ID and the Client Secret as values to the vault: Create the secrets with: $ vault kv put kv-v2/framsburg/grafana/oidc client-id = \"someID\" client-secret = \"someSecret\" Create a policy to access the secrets: $ vault policy write grafana-app - <<EOF path \"kv-v2/data/framsburg/grafana/*\" { capabilities = [\"read\", \"list\"] } EOF Note Please be aware of the added /data/ . This is not a typo but something the Vault expects when referencing this secret. It is not displayed in the UI either. And as last step create a role which maps the k8s service account with the policy: $ vault write auth/kubernetes/role/grafana-app \\ bound_service_account_names = monitoring-stack-grafana \\ bound_service_account_namespaces = monitoring-stack \\ policies = grafana-app \\ ttl = 20m","title":"Add secrets to vault"},{"location":"monitoring/#secret-class","text":"Create a SecretProviderClass in the templates cluster-critical/monitoring-stack/templates/spc.yaml --- apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-grafana spec : provider : vault parameters : vaultAddress : \"http://vault.vault:8200\" roleName : \"grafana-app\" objects : | - objectName: \"oidc-id\" secretPath: \"kv-v2/data/framsburg/grafana/oidc\" secretKey: \"client-id\" - objectName: \"oidc-secret\" secretPath: \"kv-v2/data/framsburg/grafana/oidc\" secretKey: \"client-secret\" secretObjects : - data : - key : clientId objectName : oidc-id - key : clientSecret objectName : oidc-secret secretName : oidc type : Opaque","title":"Secret Class"},{"location":"monitoring/#add-volumes-in-a-chart","text":"The Grafana-Chart doesn't allow CSI volumes to be added to the normal volume list. But it has a special value extraSecretMount for those volumes which thankfully even combines the volume and mount entry into one. cluster-critical/monitoring-stack/values.yaml kube-prometheus-stack : grafana : ... extraSecretMounts : - name : 'secrets-store-inline' mountPath : '/mnt/secrets-store' readOnly : true csi : driver : secrets-store.csi.k8s.io readOnly : true volumeAttributes : secretProviderClass : vault-grafana","title":"Add Volumes in a Chart"},{"location":"monitoring/#set-environment-variables-incl-secrets","text":"cluster-critical/monitoring-stack/values.yaml kube-prometheus-stack : grafana : ... env : GF_AUTH_GENERIC_OAUTH_ENABLED : \"true\" GF_AUTH_GENERIC_OAUTH_NAME : \"authentik\" GF_AUTH_GENERIC_OAUTH_SCOPES : \"openid profile email\" GF_AUTH_GENERIC_OAUTH_AUTH_URL : \"https://authentik.framsburg.ch/application/o/authorize/\" GF_AUTH_GENERIC_OAUTH_TOKEN_URL : \"https://authentik.framsburg.ch/application/o/token/\" GF_AUTH_GENERIC_OAUTH_API_URL : \"https://authentik.framsburg.ch/application/o/userinfo/\" GF_AUTH_SIGNOUT_REDIRECT_URL : \"https://authentik.framsburg.ch/application/o/grafana/end-session/\" # Optionally enable auto-login (bypasses Grafana login screen) # GF_AUTH_OAUTH_AUTO_LOGIN: \"true\" # Optionally map user groups to Grafana roles # GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: \"contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'Grafana Editors') && 'Editor' || 'Viewer'\" envValueFrom : GF_AUTH_GENERIC_OAUTH_CLIENT_ID : secretKeyRef : name : oidc key : clientId GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET : secretKeyRef : name : oidc key : clientSecret","title":"Set environment variables incl secrets"},{"location":"network/","text":"Network The Raspberries are in two networks. The internal network and a special DMZ for the external communication of the K8S Cluster. This is not done by actual multiple network interfaces but with vlan. Just by chance my firewall had a free network interface so I chose to actually use a seperate network interface for the DMZ there. I would have used vlan there as well. I could use pure port-forwarding to the MetalLB-IP of the Cluster. Opnsense On my Opnsense firewall I have to configure several things: WAN interface to accept inbound web (443) traffic Add NAT port-forward for the MetalLB-IP","title":"Network"},{"location":"network/#network","text":"The Raspberries are in two networks. The internal network and a special DMZ for the external communication of the K8S Cluster. This is not done by actual multiple network interfaces but with vlan. Just by chance my firewall had a free network interface so I chose to actually use a seperate network interface for the DMZ there. I would have used vlan there as well. I could use pure port-forwarding to the MetalLB-IP of the Cluster.","title":"Network"},{"location":"network/#opnsense","text":"On my Opnsense firewall I have to configure several things: WAN interface to accept inbound web (443) traffic Add NAT port-forward for the MetalLB-IP","title":"Opnsense"},{"location":"plex/","text":"PLEX Token Goto www.plex.tv/claim/ Set the env variable PLEX_CLAIM to the generated claim token. Remote Access Either add an additional entrypoint or use the custom port 443 for the remote port. It is important to adapt the custom hostname to the correct URL including protocol ( https:// ) and Path Prefix like /web . ReadWriteMany for storage To upload a lot of files it might be helpful to use ReadWriteMany volumes. This way the can be exposed for upload over samba, nfs or other setups. Upload kubectl cp <some-namespace>/<some-pod>:/tmp/foo /tmp/bar","title":"Plex"},{"location":"plex/#plex","text":"","title":"PLEX"},{"location":"plex/#token","text":"Goto www.plex.tv/claim/ Set the env variable PLEX_CLAIM to the generated claim token.","title":"Token"},{"location":"plex/#remote-access","text":"Either add an additional entrypoint or use the custom port 443 for the remote port. It is important to adapt the custom hostname to the correct URL including protocol ( https:// ) and Path Prefix like /web .","title":"Remote Access"},{"location":"plex/#readwritemany-for-storage","text":"To upload a lot of files it might be helpful to use ReadWriteMany volumes. This way the can be exposed for upload over samba, nfs or other setups.","title":"ReadWriteMany for storage"},{"location":"plex/#upload","text":"kubectl cp <some-namespace>/<some-pod>:/tmp/foo /tmp/bar","title":"Upload"},{"location":"secrets-csi/","text":"Secrets with CSI Vault Secret Injection with CSI One way to use credentials from the vault inside pods is with CSI. Vault post-start command to enable kubernetes auth-method Use Vault with CSI Install CSI Driver CRD with Chart Define a generic SecretProviderClass template as it is needed for each secret (quite a lot of boilerplate) In case you need the vault command you can easily log into the shell with: $ kubectl exec -it vault-0 -- /bin/sh Create the secrets with: $ vault kv put kv-v2/k8s/framsburg/dex client-id = \"someID\" client-secret = \"someSecret\" Enable and activate kubernetes auth method $ vault auth enable kubernetes $ vault write auth/kubernetes/config \\ issuer = \"https://kubernetes.default.svc.cluster.local\" \\ token_reviewer_jwt = \" $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) \" \\ kubernetes_host = \"https:// $KUBERNETES_PORT_443_TCP_ADDR :443\" \\ kubernetes_ca_cert = @/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Create a policy: $ vault policy write dex-app - <<EOF policy dex-app: path \"kv-v2/data/k8s/framsburg/dex\" { capabilities = [\"read\"] } EOF Write a role to map a service account with a policy $ vault write auth/kubernetes/role/dex-app \\ bound_service_account_names = dex \\ bound_service_account_namespaces = dex \\ policies = dex-app \\ ttl = 20m Success! Data written to: auth/kubernetes/role/dex-app Secret Class --- apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-dex spec : provider : vault parameters : vaultAddress : \"http://vault.vault:8200\" roleName : \"dex-app\" objects : | - objectName: \"oidc-id\" secretPath: \"kv-v2/data/k8s/framsburg/dex\" secretKey: \"client-id\" - objectName: \"oidc-secret\" secretPath: \"kv-v2/data/k8s/framsburg/dex\" secretKey: \"client-secret\" secretObjects : - data : - key : id objectName : oidc-id - key : secret objectName : oidc-secret secretName : oidc type : Opaque Volumes in a Chart ... env : - name : GITHUB_CLIENT_ID valueFrom : secretKeyRef : name : oidc key : id - name : GITHUB_CLIENT_SECRET valueFrom : secretKeyRef : name : oidc key : secret envFrom : - secretRef : name : oidc ... volumeMounts : - name : 'secrets-store-inline' mountPath : '/mnt/secrets-store' readOnly : true volumes : - name : secrets-store-inline csi : driver : secrets-store.csi.k8s.io readOnly : true volumeAttributes : secretProviderClass : vault-dex","title":"Secrets-CSI"},{"location":"secrets-csi/#secrets-with-csi","text":"","title":"Secrets with CSI"},{"location":"secrets-csi/#vault-secret-injection-with-csi","text":"One way to use credentials from the vault inside pods is with CSI. Vault post-start command to enable kubernetes auth-method Use Vault with CSI Install CSI Driver CRD with Chart Define a generic SecretProviderClass template as it is needed for each secret (quite a lot of boilerplate) In case you need the vault command you can easily log into the shell with: $ kubectl exec -it vault-0 -- /bin/sh Create the secrets with: $ vault kv put kv-v2/k8s/framsburg/dex client-id = \"someID\" client-secret = \"someSecret\" Enable and activate kubernetes auth method $ vault auth enable kubernetes $ vault write auth/kubernetes/config \\ issuer = \"https://kubernetes.default.svc.cluster.local\" \\ token_reviewer_jwt = \" $( cat /var/run/secrets/kubernetes.io/serviceaccount/token ) \" \\ kubernetes_host = \"https:// $KUBERNETES_PORT_443_TCP_ADDR :443\" \\ kubernetes_ca_cert = @/var/run/secrets/kubernetes.io/serviceaccount/ca.crt Create a policy: $ vault policy write dex-app - <<EOF policy dex-app: path \"kv-v2/data/k8s/framsburg/dex\" { capabilities = [\"read\"] } EOF Write a role to map a service account with a policy $ vault write auth/kubernetes/role/dex-app \\ bound_service_account_names = dex \\ bound_service_account_namespaces = dex \\ policies = dex-app \\ ttl = 20m Success! Data written to: auth/kubernetes/role/dex-app","title":"Vault Secret Injection with CSI"},{"location":"secrets-csi/#secret-class","text":"--- apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-dex spec : provider : vault parameters : vaultAddress : \"http://vault.vault:8200\" roleName : \"dex-app\" objects : | - objectName: \"oidc-id\" secretPath: \"kv-v2/data/k8s/framsburg/dex\" secretKey: \"client-id\" - objectName: \"oidc-secret\" secretPath: \"kv-v2/data/k8s/framsburg/dex\" secretKey: \"client-secret\" secretObjects : - data : - key : id objectName : oidc-id - key : secret objectName : oidc-secret secretName : oidc type : Opaque","title":"Secret Class"},{"location":"secrets-csi/#volumes-in-a-chart","text":"... env : - name : GITHUB_CLIENT_ID valueFrom : secretKeyRef : name : oidc key : id - name : GITHUB_CLIENT_SECRET valueFrom : secretKeyRef : name : oidc key : secret envFrom : - secretRef : name : oidc ... volumeMounts : - name : 'secrets-store-inline' mountPath : '/mnt/secrets-store' readOnly : true volumes : - name : secrets-store-inline csi : driver : secrets-store.csi.k8s.io readOnly : true volumeAttributes : secretProviderClass : vault-dex","title":"Volumes in a Chart"},{"location":"traefik/","text":"Traefik Use two traefik controllers for internal and external network. Issue with only one service api@internal with will mess up the WebUI: The web ui will display for both controllers all services and routes.","title":"Traefik"},{"location":"traefik/#traefik","text":"Use two traefik controllers for internal and external network. Issue with only one service api@internal with will mess up the WebUI: The web ui will display for both controllers all services and routes.","title":"Traefik"},{"location":"uptime-kuma/","text":"Uptime Kuma","title":"Uptime Kuma"},{"location":"uptime-kuma/#uptime-kuma","text":"","title":"Uptime Kuma"},{"location":"vault/","text":"Hashicorp Vault For operating the Vault inside K8S it is a good idea to use the Banzaicloud Vault-Operator . It automates some the integration and HA tasks. Install the operator Define a new Chart with a dependencies to the Vault-Operator Chart in the app of apps for the vault-operator with the following values: vault-operator : crdAnnotations : \"helm.sh/hook\" : crd-install Install Vault Define a new empty Chart with the following templates inside: ```yaml:rbac.yaml kind: ServiceAccount apiVersion: v1 metadata: name: vault kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: vault rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"*\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"update\", \"patch\"] kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: vault roleRef: kind: Role name: vault apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: vault This binding allows the deployed Vault instance to authenticate clients through Kubernetes ServiceAccounts (if configured so). apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: vault-auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault namespace: default ## Use secrets You always have to map the secrets in different ways. You can find a detailed description on [banzais website](https://banzaicloud.com/docs/bank-vaults/mutating-webhook/) ### As envrionment variable Pod should have the following annotations: ```yaml annotations: vault.security.banzaicloud.io/vault-addr: \"http://vault.vault.svc:8200\" vault.security.banzaicloud.io/vault-path: \"kubernetes\" vault.security.banzaicloud.io/vault-role: \"test\" vault.security.banzaicloud.io/vault-skip-verify: \"true\" You should adapt the role to the corresponding role you want to use. You can then use secrets in environment variables like this: env : - name : GITHUB_CLIENT_ID value : vault:secret/data/framsburg/test#github_token As secret The approach with secrets looks quite similar. The main difference is, that you have to provide the path to the secret base64 encoded. $ echo -n vault:secret/data/framsburg/test#github_token | base64 dmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu Then prepare the secret accodringly: apiVersion : v1 kind : Secret metadata : name : some-secret data : GITHUB_CLIENT_ID : dmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu type : Opaque Inline Instead of environment variables or secrets you can use the vault key reference anywhere in resources and the webhook will replace it with the secret.","title":"Hashicorp Vault"},{"location":"vault/#hashicorp-vault","text":"For operating the Vault inside K8S it is a good idea to use the Banzaicloud Vault-Operator . It automates some the integration and HA tasks.","title":"Hashicorp Vault"},{"location":"vault/#install-the-operator","text":"Define a new Chart with a dependencies to the Vault-Operator Chart in the app of apps for the vault-operator with the following values: vault-operator : crdAnnotations : \"helm.sh/hook\" : crd-install","title":"Install the operator"},{"location":"vault/#install-vault","text":"Define a new empty Chart with the following templates inside: ```yaml:rbac.yaml kind: ServiceAccount apiVersion: v1 metadata: name: vault kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: vault rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"*\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"update\", \"patch\"] kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: vault roleRef: kind: Role name: vault apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: vault","title":"Install Vault"},{"location":"vault/#this-binding-allows-the-deployed-vault-instance-to-authenticate-clients","text":"","title":"This binding allows the deployed Vault instance to authenticate clients"},{"location":"vault/#through-kubernetes-serviceaccounts-if-configured-so","text":"apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: vault-auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: vault namespace: default ## Use secrets You always have to map the secrets in different ways. You can find a detailed description on [banzais website](https://banzaicloud.com/docs/bank-vaults/mutating-webhook/) ### As envrionment variable Pod should have the following annotations: ```yaml annotations: vault.security.banzaicloud.io/vault-addr: \"http://vault.vault.svc:8200\" vault.security.banzaicloud.io/vault-path: \"kubernetes\" vault.security.banzaicloud.io/vault-role: \"test\" vault.security.banzaicloud.io/vault-skip-verify: \"true\" You should adapt the role to the corresponding role you want to use. You can then use secrets in environment variables like this: env : - name : GITHUB_CLIENT_ID value : vault:secret/data/framsburg/test#github_token","title":"through Kubernetes ServiceAccounts (if configured so)."},{"location":"vault/#as-secret","text":"The approach with secrets looks quite similar. The main difference is, that you have to provide the path to the secret base64 encoded. $ echo -n vault:secret/data/framsburg/test#github_token | base64 dmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu Then prepare the secret accodringly: apiVersion : v1 kind : Secret metadata : name : some-secret data : GITHUB_CLIENT_ID : dmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu type : Opaque","title":"As secret"},{"location":"vault/#inline","text":"Instead of environment variables or secrets you can use the vault key reference anywhere in resources and the webhook will replace it with the secret.","title":"Inline"}]}