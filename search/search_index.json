{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Raspberry Pi GitOps Stack","text":"<p>This document describes my current setup of my Rasperry Pi k8s cluster. Although everything should be reflected in code, usually my brain discards stuff which works ... now. The not-working is a problem for future brain \ud83d\ude04</p> <p>So the text is mainly meant for me to keep track of how and why I did certain things. If somebody else finds value in it: great!</p> <p>Although the whole thing is a private project for educational purposes, I try to keep it as production ready as possible. Often the biggest learnings stem from corner cases.  That means, at least for me, to stay true to the following points:</p> <ul> <li>Everything is automated, no manual kubectl commands</li> <li>Clear separation between public and private network</li> <li>Use secure connections<ul> <li>All HTTPS connections have correct certificates from LetcEncrypt</li> </ul> </li> <li>Disaster recovery is easy to do</li> <li>Critical parts of the system (like control-plane, networking, etc) are setup in HA</li> </ul>"},{"location":"#hardware","title":"Hardware","text":"<p>I use the following harware setup for this:</p> <ul> <li>3 Raspberry PI 4s with 8GB Ram and Corsair GTX USB Stick as Disk</li> <li>4 Raspberry PI 4s with 8GB Ram, Corsair GTX USB Stick as Boot Disk and an   external USB/NVME Drive (which has to be powered separatly)</li> </ul> <p>The first are used for the control plane, the other fours are used as worker nodes. Some of them are powered with PoE hats. The faster external drives are used for longhorn.</p> <ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"adguard/","title":"AdGuard","text":""},{"location":"adguard/#setup-adguard-container","title":"Setup AdGuard container","text":"<p>Use the AdGuard container from K8S@Home</p>"},{"location":"adguard/#config-in-opnsense","title":"Config in Opnsense","text":"<p>On my Opnsense firewall I have to configure several things:</p>"},{"location":"ansible/","title":"Pi Farm @ the river","text":""},{"location":"ansible/#first-time","title":"First Time","text":"<p>Create credentials file in devops home, named <code>.vault-credentials</code> with permission 0600. This file contains the passphrase for the ansible vault.</p>"},{"location":"ansible/#download-roles-from-the-ansible-galixy","title":"Download roles from the ansible galixy","text":"<p><code>ansible-galaxy install -r roles/requirements.yml</code></p>"},{"location":"ansible/#new-servers","title":"New servers","text":"<p>To provision new (or existing systems) with the correct users and keys do the following:</p> <ul> <li>Add system to the hosts file</li> <li>Ensure the default user exists in <code>group_vars/&lt;distribution-name&gt;.yaml</code>   Otherwise add a file with the variable <code>ansible_user_first_run</code></li> <li>Run ansible with <code>ansible-playbook add-user-ssh.yaml --limit k3sworker05</code></li> </ul>"},{"location":"ansible/#deploy-the-site","title":"Deploy the site","text":"<p><code>ansible-playbook site.yml</code></p>"},{"location":"ansible/#upgrade-all-servers-to-latest-versions","title":"Upgrade all servers to latest versions","text":"<p><code>ansible-playbook upgrade.yml</code></p>"},{"location":"ansible/#references","title":"References","text":"<ul> <li>https://github.com/prometheus/demo-site</li> </ul>"},{"location":"ansible/#raspberry-firmware-update","title":"Raspberry Firmware Update","text":"<pre><code>$ sudo rpi-eeprom-update\n$ sudo apt-get install rpi-eeprom\n$ sudo rpi-eeprom-update -a\n\n$ sudo raspi-config\n</code></pre> <p>TODO:</p>"},{"location":"ansible/#setup-requirements","title":"setup requirements","text":"<ul> <li> <p>update hostname: <code>sudo hostnamectl set-hostname k3s...</code></p> </li> <li> <p>cgroupt in /boot/firmware/cmdline.txt: (on ubuntu /boot/fir) append <code>cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1</code></p> </li> </ul>"},{"location":"ansible/#install-k3s-server","title":"install k3s server","text":"<pre><code>$ export K3S_DATASTORE_ENDPOINT='postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable'\n\n$ curl -sfL https://get.k3s.io | sh -s - server --node-taint CriticalAddonsOnly=true:NoExecute --tls-san 192.168.42.60\n\n--datastore-endpoint postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable\n--tls-san k3s.framsburg.ch\n</code></pre> <pre><code>$ sudo journalctl -u k3s.service\n</code></pre> <p>get token <code>sudo cat /var/lib/rancher/k3s/server/node-token</code></p>"},{"location":"ansible/#add-second-server","title":"add second server","text":"<p>export database export Token <pre><code>$ export K3S_DATASTORE_ENDPOINT='postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable'\n$ export K3S_TOKEN=K10c72f4e5f9fd862f0a1e91f9d1f91b16b2580621273705ee76528a66ed45f9819::server:5401d36937aa612639acb4d1083c2800\n\ncurl -sfL https://get.k3s.io | sh -s - server \\\n--datastore-endpoint postgres://k3s:12345678@192.168.42.221:5432/k3s?sslmode=disable \\\n--tls-san k3s.framsburg.ch\n</code></pre></p>"},{"location":"ansible/#install-agent","title":"install agent","text":"<p>log into agent node <pre><code>export K3S_URL=https://k3s.framsburg.ch:6443\nexport K3S_TOKEN=K10c72f4e5f9fd862f0a1e91f9d1f91b16b2580621273705ee76528a66ed45f9819::server:5401d36937aa612639acb4d1083c2800\ncurl -sfL https://get.k3s.io | sh -\n</code></pre></p>"},{"location":"ansible/#metallb","title":"MetalLB","text":"<p><code>kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\"$(openssl rand -base64 128)\" --dry-run -o yaml | kubectl apply -f -</code></p> <p>use ansible vault for secretkey.</p>"},{"location":"ansible/#dual-stack-ingress","title":"Dual Stack Ingress","text":"<p>Ideas from Digitalis</p>"},{"location":"ansible/#install-longhorn","title":"Install Longhorn","text":"<p>https://www.ekervhen.xyz/posts/2021-02/troubleshooting-longhorn-and-dns-networking/</p>"},{"location":"ansible/#new-disk","title":"New disk","text":"<p>find out device <code>lsblk -f</code> on new devices <code>wipefs -a /dev/{{ var_disk }}</code></p> <pre><code>$ sudo fdisk -l\n$ sudo fdisk /dev/sdb\n\nCommand: n\nPartition number: 1 (default)\nFirst sector: (default)\nLast sector: (default)\nCommand: w\n\n$ sudo fdisk -l\n</code></pre> <pre><code>$ sudo mkfs -t ext4 /dev/sdb1\n</code></pre>"},{"location":"ansible/#install-cert-manager","title":"install cert-manager","text":"<p>log into localhost or commander <pre><code>$ export KUBECONFIG='~/.kube/config-k3s'\n\n# If you have installed the CRDs manually instead of with the `--set installCRDs=true` option added to your Helm install command, you should upgrade your CRD resources before upgrading the Helm chart:\n\n$ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.5.1/cert-manager.crds.yaml\n\n# Add the Jetstack Helm repository\nhelm repo add jetstack https://charts.jetstack.io\n\n# Update your local Helm chart repository cache\n$ helm repo update\n\n# Install the cert-manager Helm chart\n$ helm install cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.5.4 \\\n  --set installCRDs=true \\\n  --debug\n</code></pre></p>"},{"location":"ansible/#install-rancher","title":"install rancher","text":"<p>log into localhost or commander <pre><code>$ export KUBECONFIG='~/.kube/config-k3s'\n$ helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n\n$ kubectl create namespace cattle-system\n\n# Update your local Helm chart repository cache\n$ helm repo update\n\n$ helm install rancher rancher-latest/rancher \\\n  --namespace cattle-system \\\n  --create-namespace \\\n  --set hostname=rancher.framsburg.ch \\\n  --set bootstrapPassword=admin \\\n  --set ingress.tls.source=rancher  --wait --debug --timeout 10m0s\n\n$ kubectl -n cattle-system rollout status deploy/rancher\n</code></pre></p> <p>uninstall cert-manager: - helm uninstall cert-manager -n cert-manager - kubectl delete -f https://github.com/jetstack/cert-manager/releases/download/v1.5.4/cert-manager.crds.yaml - kubectl delete  job.batch/cert-manager-startupapicheck -n cert-manager - kubectl delete rolebinding.rbac.authorization.k8s.io/cert-manager-startupapicheck:create-cert -n cert-manager - kubectl delete role.rbac.authorization.k8s.io/cert-manager-startupapicheck:create-cert -n cert-manager</p>"},{"location":"ansible/#argo-cd","title":"argo cd","text":""},{"location":"ansible/#prerequisites","title":"prerequisites","text":"<p>Install Go * [https://golang.org/doc/install]</p> <p>Install Docker * [https://docs.docker.com/engine/install/ubuntu/]</p> <p>Clone and build argo-cd according to ^argocdarm * <code>git clone https://github.com/argoproj/argo-cd.git</code> * <code>cd argo-cd</code> * <code>make armimage</code></p>"},{"location":"ansible/#plex","title":"Plex","text":"<p>Plex image after k8s-at-home/plex has an initial issue with recognising itself as plex media server. The reason is the claim token (https://raw.githubusercontent.com/uglymagoo/plex-claim-server/master/plex-claim-server.sh)</p> <p>if [ ! -z \"${PLEX_CLAIM}\" ] &amp;&amp; [ -z \"${token}\" ]; then   echo \"Attempting to obtain server token from claim token\"   loginInfo=\"$(curl -X POST \\         -H 'X-Plex-Client-Identifier: '${clientId} \\         -H 'X-Plex-Product: Plex Media Server'\\         -H 'X-Plex-Version: 1.1' \\         -H 'X-Plex-Provides: server' \\         -H 'X-Plex-Platform: Linux' \\         -H 'X-Plex-Platform-Version: 1.0' \\         -H 'X-Plex-Device-Name: PlexMediaServer' \\         -H 'X-Plex-Device: Linux' \\         \"https://plex.tv/api/claim/exchange?token=${PLEX_CLAIM}\")\"   token=\"$(echo \"$loginInfo\" | sed -n 's/.(.)&lt;\\/authentication-token&gt;.*/\\1/p')\" <p>if [ \"$token\" ]; then     setPref \"PlexOnlineToken\" \"${token}\"     echo \"Plex Media Server successfully claimed\"   fi fi</p>"},{"location":"ansible/#tips-tricks","title":"Tips &amp; Tricks","text":""},{"location":"ansible/#get-rid-of-node","title":"Get rid of node:","text":"<p>kubectl drain  kubectl delete"},{"location":"ansible/#list-all-helm-installations","title":"list all helm installations:","text":"<p>helm list -a  -A</p>"},{"location":"ansible/#uninstall-helm-installation","title":"uninstall helm installation:","text":"<p>helm uninstall  -n"},{"location":"ansible/#remove-dangling-namespaces","title":"Remove dangling namespaces:","text":"<p>( NAMESPACE=your-rogue-namespace kubectl proxy &amp; kubectl get namespace $NAMESPACE -o json |jq '.spec = {\"finalizers\":[]}' &gt;temp.json curl -k -H \"Content-Type: application/json\" -X PUT --data-binary @temp.json 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE/finalize )</p>"},{"location":"ansible/#remove-old-logs","title":"Remove old logs:","text":"<p>sudo journalctl --rotate --vacuum-time=5s sudo journalctl --rotate --vacuum-size=500M</p>"},{"location":"ansible/#remove-ssh-keys","title":"Remove SSH keys","text":"<p><code>sudo ssh-keygen -f \"/root/.ssh/known_hosts\" -R \"k3sworker02\"</code> <code>ssh-keygen -f \"/home/devops/.ssh/known_hosts\" -R \"k3sworker02\"</code></p>"},{"location":"ansible/#replace-master-node","title":"Replace master node","text":"<ul> <li>Drain master node</li> <li>Move master node to end of ansible master group list (if it is at the top, it will be initialized as new cluster)</li> <li></li> </ul>"},{"location":"ansible/#install-etcdctl","title":"Install etcdctl","text":"<p><pre><code>$ VERSION=\"v3.5.4\"\n$ curl -L https://github.com/etcd-io/etcd/releases/download/${VERSION}/etcd-${VERSION}-linux-arm64.tar.gz --output etcdctl-linux-arm64.tar.gz\n$ sudo tar -zxvf etcdctl-linux-arm64.tar.gz --strip-components=1 -C /usr/local/bin etcd-${VERSION}-linux-arm64/etcdctl\n</code></pre> <pre><code>$ sudo etcdctl --cacert=/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt --cert=/var/lib/rancher/k3s/server/tls/etcd/client.crt --key=/var/lib/rancher/k3s/server/tls/etcd/client.key version\n</code></pre></p>"},{"location":"ansible/#unix-utils","title":"Unix Utils","text":"<p>Find listening ports: <pre><code>$ sudo lsof -i -P -n | grep LISTEN\n</code></pre></p>"},{"location":"ansible/#next-steps","title":"Next Steps:","text":"<ul> <li>https://greg.jeanmart.me/2020/04/13/deploy-prometheus-and-grafana-to-monitor-a-k/</li> <li>https://www.civo.com/learn/monitoring-k3s-with-the-prometheus-operator-and-custom-email-alerts</li> <li>https://github.com/atoy3731/k8s-tools-app</li> </ul>"},{"location":"ansible/#references_1","title":"References:","text":"<ul> <li>https://rpi4cluster.com/k3s/k3s-hardware/</li> <li></li> <li>argocd selfdeploy: [https://www.arthurkoziel.com/setting-up-argocd-with-helm/]</li> </ul>"},{"location":"argocd/","title":"Argo CD","text":""},{"location":"argocd/#applicationset","title":"ApplicationSet","text":""},{"location":"argocd/#ignore-diffs","title":"Ignore Diffs","text":"diff in applicationset<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\nname: cluster-init\nnamespace: argocd-system\nspec:\ngenerators:\n...\ntemplate:\nspec:\nproject: default\nignoreDifferences:\n- group: \"\"\nkind: ConfigMap\njsonPointers:\n- /data/oidc.config\n- group: \"\"\nkind: Secret\njsonPointers:\n- /data/oidc.authentik.clientSecret\n...\n</code></pre>"},{"location":"argocd/#part-two-oidc-integration","title":"Part Two: OIDC integration","text":"<p>This part should follow after [Vault] and [Authentik] are up and running.</p>"},{"location":"argocd/#oidc-login","title":"OIDC Login","text":"<p>First you have to create a provider and application in authentik to get a client id and secret. Afterwards the oidc credentials can be saved in the Vault and mapped over a <code>SecretProviderClass</code>. (Do not forget to mount the vault volumes for the secret to work [./secrets-csi.md#volumes-in-a-chart])</p> <pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\nname: vault-argocd\nspec:\nprovider: vault\nparameters:\nvaultAddress: \"http://vault.vault:8200\"\nroleName: \"argocd-app\"\nobjects: |\n- objectName: \"oidc-id\"\nsecretPath: \"kv-v2/data/framsburg/argocd/oidc\"\nsecretKey: \"client-id\"\n- objectName: \"oidc-secret\"\nsecretPath: \"kv-v2/data/framsburg/argocd/oidc\"\nsecretKey: \"client-secret\"\nsecretObjects:\n- data:\n- key: oidc.authentik.clientId\nobjectName: oidc-id\n- key: oidc.authentik.clientSecret\nobjectName: oidc-secret\nsecretName: oidc\ntype: Opaque\nlabels:\napp.kubernetes.io/part-of: argocd # (1)!\n</code></pre> <ol> <li>Without this label the secret reference in the argocd ConfigMap will not work and complain about that the secret key can not be found.</li> </ol> <pre><code>argo-cd:\nserver:\n...\nconfig:\nurl: https://argocd.framsburg.ch\noidc.config: | name: Authentik\nissuer: \"https://authentik.framsburg.ch/application/o/argocd/\"\nclientID: \"c579d3195f85aeccaf1ecce35ef5501e023c2a6a\"\nclientSecret: \"$oidc:oidc.authentik.clientSecret\"\nrequestedScopes: [\"openid\", \"profile\", \"email\"]\nlogoutURL: \"https://authentik.framsburg.ch/if/session-end/argocd/\"\nrbacConfig:\npolicy.default: role:readonly\npolicy.csv: |\ng, 'authentik Admins', role:admin\nvolumeMounts:\n- name: 'secrets-store-inline'\nmountPath: '/mnt/secrets-store'\nreadOnly: true\nvolumes:\n- name: secrets-store-inline\ncsi:\ndriver: 'secrets-store.csi.k8s.io'\nreadOnly: true\nvolumeAttributes:\nsecretProviderClass: 'vault-argocd'\n</code></pre>"},{"location":"argocd/#rename-applicationset","title":"Rename Applicationset","text":"<p>Important: Remove Applicationset without cascade delete with the following two options.</p>"},{"location":"argocd/#variant-a-delete-over-cli","title":"Variant A: Delete over CLI","text":"<pre><code>$ kubectl delete ApplicationSet (NAME) --cascade=false\n</code></pre>"},{"location":"argocd/#variant-b-delete-over-gitops","title":"Variant B: Delete over GitOps","text":"<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\nname: cluster-apps\nnamespace: argocd-system\nspec:\ngoTemplate: true\nsyncPolicy:\npreserveResourcesOnDeletion: true\n</code></pre> <p>--&gt; Warning should occur of applicationsets are part of two applications (it's actually wrong, but apparently applicationsets are identified only by name)</p> <p>Remove old application set -</p> <p>Go through the following steps</p> <ol> <li>Add app for new applicationset</li> <li>Add preserve resources in old and new applicationset</li> <li>Remove old applicationset</li> <li>Add app to new applicationset one by one</li> </ol>"},{"location":"argocd/#switch-from-applicationset-to-app-of-apps","title":"Switch from Applicationset to App of Apps","text":"<p>To switch from applicationsets to an App of Apps setup we want to delete the superstructure of applicationsets and applications without removing the underlying resources like Pods or VolumeClaims. We do this in multiple steps</p>"},{"location":"argocd/#disable-cascading","title":"Disable cascading","text":"<p>Configure all application sets to preserve their resources. Otherwise the remove of the application set will trigger a deletion cascade to applications and pods, etc.</p> <p>You can do this by defining the following option on the applicationset spec (not the template!!):</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\nname: cluster-apps\nnamespace: argocd-system\nspec:\nsyncPolicy:\npreserveResourcesOnDeletion: true\n</code></pre>"},{"location":"argocd/#remove-applicationset","title":"Remove applicationset","text":"<p>Next step is to delete the applicationset. Make sure to remove them from the initial bootstrap setup otherwise they will be recreated again. This means remove the applicationset from <code>cluster-init/root</code>.</p> <p>After the sync the applicationset should be removed as well as the applications. If this is not the case you can manually remove them with the following command:</p> <pre><code>$ kubectl delete ApplicationSet &lt;NAME&gt; -n argocd-system --cascade=false\n</code></pre>"},{"location":"argocd/#add-new-app-of-apps","title":"Add new app of apps","text":"<p>This can be done anytime even before you start, if your new app of apps does not share the name with an existing application.</p> <p>This is quite straight forward. Add new app of apps to the bootstrap start under <code>apps-root-config/bootstrap/values.yaml</code>.</p>"},{"location":"argocd/#add-app-to-app-of-apps","title":"Add app to app of apps","text":"<p>First make sure the application you want to add is in the right folder. Don't move aka remove it from the old folder as the old application still exists and  would sync your change aka remove the app!</p> <p>Add your app to the new app of apps for example under <code>apps-root-config/applications/cluster-utility-apps.yaml</code></p> <p>This assumes the application name has not changed!. You can now remove the app files from the old folder.</p>"},{"location":"argocd/#github-webhook","title":"GitHub Webhook","text":"<p>This documentation describes how to setup a git webhook which notifies Argo CD on any source changes immedietly. Otherwise Argo CD will pull changes on a three minutes intervall. This follows the Argo CD documentation</p>"},{"location":"argocd/#create-secret-in-vault","title":"Create Secret in Vault","text":"<p>First create an arbitrary secret in the vault for GitHub to use and Argo CD to verify. This is important: As the callback URL must be publicly accessible (because GitHub is public) it opens an attack vector for a DDoS attack. As it not be changed frequently, go for something long ... 50 chars?</p> <p>Add it under <code>framsburg/argocd/github</code> and under key <code>webhook-secret</code>.</p> <p>Create the following external secret which provides this vault secret under the correct secret name. Remeber the k8s secret must be named <code>webhook.github.secret</code></p>"},{"location":"authentik/","title":"Authentik","text":""},{"location":"bare-metal/","title":"Harware Setup of Raspberry PIs","text":"<p>The initial setup is done with Ansible.</p>"},{"location":"bare-metal/#setup-minio-bucket-for-backup","title":"Setup Minio Bucket for Backup","text":"Create minio bucket<pre><code>$ mc mb myminio/k3s\n$ mc mb myminio/k3s/etcd-snapshot\n</code></pre> Create user with policy<pre><code>$ mc admin user add myminio k3s k3sk3sk3s\n\n$ cat &gt; /tmp/etcd-backups-policy.json &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n      \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:PutBucketPolicy\",\n        \"s3:GetBucketPolicy\",\n        \"s3:DeleteBucketPolicy\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3s\"\n      ],\n      \"Sid\": \"\"\n    },\n    {\n      \"Action\": [\n        \"s3:AbortMultipartUpload\",\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:PutObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3s/*\"\n      ],\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n$ mc admin policy add myminio etcd-backups-policy /tmp/etcd-backups-policy.json\n\n$ mc admin policy set myminio etcd-backups-policy user=k3s\n</code></pre>"},{"location":"bare-metal/#define-k3s-backup-target","title":"Define K3S backup target","text":"k3s-server.services<pre><code>[Service]\nExecStart={{ k3s_binary_path }}/k3s server \\\n...\n{% if backup_s3_enabled %}\n    --etcd-s3 \\\n    --etcd-snapshot-schedule-cron='{{ backup_schedule_cron }}' \\\n    --etcd-s3-endpoint='{{ backup_s3_endpoint }}' \\\n    --etcd-s3-endpoint-ca='{{ systemd_dir }}/k3s-server.service.crt' \\\n    --etcd-s3-bucket='{{ backup_s3_bucket }}' \\\n    --etcd-s3-folder='{{ backup_s3_folder }}' \\\n    --etcd-s3-access-key='{{ backup_s3_access_key }}' \\\n    --etcd-s3-secret-key='{{ backup_s3_secret_key }}' \\\n{% endif %}\n</code></pre> <p>Define in ansible vault <code>ansible-vault edit group_vars/all.yaml</code> the four coordinates: vault<pre><code>backup_s3_access_key: k3s\nbackup_s3_secret_key: k3sk3sk3s\n</code></pre></p> hosts<pre><code>backup_schedule_cron: '0 */6 * * *'\nbackup_s3_bucket: k3s\nbackup_s3_folder: etcd-snapshot\nbackup_s3_endpoint_ca: |\n-----BEGIN CERTIFICATE-----\nMIIDgTCCAmmgAwIBAgIJAJ85e+K5ngFRMA0GCSqGSIb3DQEBCwUAMGsxCzAJBgNV\n</code></pre>"},{"location":"bare-metal/#optional-rolling-update","title":"(Optional) Rolling Update","text":"<p>The initial ansible script is not very suitable for rolling updates as it assumes it is about to initialize a cluster which requires the order</p> <ol> <li>First master node which initializes (or restores) the etcd state</li> <li>All other master nodes which sync up to the first</li> <li>All worker nodes</li> </ol> <p>That is very efficient for setup and restore but would mean some outages if applied on a live cluster. Therefore we need a playbook which goes through every node sequentially (we have no special requirement on performance) and cares about draining nodes correctly.</p> <p>Ideally we can reuse roles from the cluster setup playbook.</p>"},{"location":"bare-metal/#update-k3s-version","title":"Update K3S Version","text":"<p>Update version in hosts or manifest</p> <p>Use playbook <code>07_k3s_update</code> and start with the master nodes individually. Be aware that Longhorn and other systems need time to recover from the reboots otherwise they will block a shutdown or might loose data.</p> <pre><code>ansible-playbook playbooks/07_k3s_update.yml --limit k3smaster1\n</code></pre>"},{"location":"bare-metal/#lenovo-nic-e1000","title":"Lenovo nic e1000","text":"<p>Some tiny servers might have an issue with their network card specifically with the offloading. In case the network adapter hangs itself up the following command might help:</p> <pre><code>ethtool -K &lt;ADAPTER&gt; gso off gro off tso off </code></pre> <p>This should be added as startup command to <code>/etc/network/if-up.d/</code></p>"},{"location":"cert-manager/","title":"Cert-Manager","text":""},{"location":"custom-images/","title":"Custom Docker Images","text":"<p>For the task of creating my own images I got inspired by the setup from k8s-at-home and of it's contributers like onedr0p</p>"},{"location":"custom-images/#testing","title":"Testing","text":"<p>Use GOSS.</p> <p>Use <code>dgoss</code> locally which is just a docker wrapper for goss.</p>"},{"location":"custom-images/#references","title":"References","text":""},{"location":"external-secrets/","title":"External Secrets","text":"<p>After some hickups with the vault-secrets-webhook I want to give external secrets a try. With the webhook I had issues with the env variables as not all Helm Charts made it so easy to use like the kube-monitoring-stack: Grafana was using some complicated mapping setup which made it necessary to use Secrets and <code>secret-refs</code>. And thats where my issues started because the vault-secrets-webhook did not reliably replace the placeholders ....</p>"},{"location":"external-secrets/#install-operator","title":"Install Operator","text":"<p>I created a new Helm Chart App for external secrets</p> Chart.yaml<pre><code>apiVersion: v2\nname: external-secrets\nversion: 0.0.0\ndependencies:\n- name: external-secrets\nversion: 0.5.9\nrepository: https://charts.external-secrets.io\n</code></pre> values.yaml<pre><code>external-secrets:\nserviceMonitor:\nenabled: true\n</code></pre> <p>That will create the very basic setup for External Secrets.</p>"},{"location":"external-secrets/#setup-store","title":"Setup Store","text":"<p>The next and most important part is the Secret Store. This is the actual connection to the vault and has the vault coordinates as well as access keys. To make it an easy setup, this is part of the external secrets application.</p> templates/secret-store.yaml<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\nname: vault-backend\nspec:\nprovider:\nvault:\nserver: \"http://vault.vault.svc:8200\"\npath: \"kubernetes\"\nversion: \"v2\"\nauth:\n# Authenticate against Vault using a Kubernetes ServiceAccount\n# token stored in a Secret.\n# https://www.vaultproject.io/docs/auth/kubernetes\nkubernetes:\n# Path where the Kubernetes authentication backend is mounted in Vault\nmountPath: \"kubernetes\"\n# A required field containing the Vault Role to assume.\nrole: \"externalSecret\" # (1)\n# Optional service account field containing the name\n# of a kubernetes ServiceAccount\nserviceAccountRef:\nname: \"external-secrets\"\n</code></pre> <ol> <li>This role name has to be created in the Vault or configured for the operator     to create it.</li> </ol>"},{"location":"external-secrets/#usecreate-external-secret","title":"Use/Create external secret","text":"<p>Last step: create a secret over the external secrets operator. Because the store is clusterwide available, the only missing part is the <code>ExternalSecret</code> definition.</p> <p>As an example, the following is the OIDC client id and secret for grafana. I stored all the authentik OIDC clients in the vault. The external secret manifest resides in the templates folder of the monitoring stack.</p> <p>templates/grafana-oidc-secret.yaml<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\nname: grafana-oidc-secret\nnamespace: monitoring-stack\nspec:\nrefreshInterval: 1m\nsecretStoreRef:\nname: vault-backend # (1)\nkind: ClusterSecretStore\nnamespace: external-secrets\ntarget:\nname: grafana-oidc-secret\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/part-of: monitoring-stack\ndata:\n- secretKey: oidc-id\nremoteRef:\nkey: secret/data/framsburg/grafana/oidc # (2)\nproperty: client-id # (3)\n- secretKey: oidc-secret\nremoteRef:\nkey: secret/data/framsburg/grafana/oidc\nproperty: client-secret\n</code></pre> 1.  Name of the configured Secret Store (Make sure the kind is in sync with your     Secret Store) 2.  Path of the secret in Vault. <code>secret</code> is the name of the secret engine,     <code>data</code> is part of the path although not visible in Vault ... some Hashicorp     Vault shenanigan 3.  Key of the secret</p> <p>The generated secret will look like this:</p> <pre><code>\n</code></pre>"},{"location":"external-secrets/#reference","title":"Reference","text":""},{"location":"longhorn/","title":"Longhorn","text":""},{"location":"longhorn/#setup-minio-for-backup","title":"Setup Minio for Backup","text":"<p>Use the minio cli <code>mc</code> which has an alias called <code>myminio</code></p> Create minio bucket<pre><code>$ mc mb myminio/k3sbackups\n$ mc mb myminio/k3sbackups/longhorn\n</code></pre> Create user with policy<pre><code>$ mc admin user add myminio longhorn mypass\n$ cat &gt; /tmp/k3s-backups-policy.json &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n      \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:PutBucketPolicy\",\n        \"s3:GetBucketPolicy\",\n        \"s3:DeleteBucketPolicy\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3sbackups\"\n      ],\n      \"Sid\": \"\"\n    },\n    {\n      \"Action\": [\n        \"s3:AbortMultipartUpload\",\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:PutObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3sbackups/*\"\n      ],\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n$ mc admin policy add myminio k3s-backups-policy /tmp/k3s-backups-policy.json\n\n$ mc admin policy set myminio k3s-backups-policy user=longhorn\n</code></pre>"},{"location":"longhorn/#define-backup-target-in-longhorn","title":"Define backup target in longhorn","text":"longhorn values.yaml<pre><code>longhorn:\ndefaultSettings:\nbackupTarget: 's3://k3sbackups@us-east-1/longhorn'\nbackupTargetCredentialSecret: minio-secret\n...\n</code></pre>"},{"location":"metallb/","title":"MetalLB","text":""},{"location":"monitoring/","title":"Monitoring with Prometheus Stack","text":""},{"location":"monitoring/#part-two-oidc-integration","title":"Part Two: OIDC Integration","text":"<p>This part should follow after [Vault] and [Authentik] are up and running.</p>"},{"location":"monitoring/#create-application-in-authentik","title":"Create Application in Authentik","text":"<p>Create a new OIDC provider in authentik with Redirect URIs/Origin pointing to [https://grafana.framsburg.ch]</p> <p>Afterwards create a new application which uses the before created provider. Don't forget to create bindings for uses or groups. You will need the following three informations out of authentik in the next steps.</p> <ul> <li>OpenID Configuration Issuer</li> <li>Client ID</li> <li>Client Secret</li> </ul>"},{"location":"monitoring/#add-secrets-to-vault","title":"Add secrets to vault","text":"<p>To use the vault CLI use the folling command:</p> <pre><code>$ kubectl exec -it vault-0 -n vault -- /bin/sh\n</code></pre> <p>Add the Client ID and the Client Secret as values to the vault:</p> <p>Create the secrets with:</p> <pre><code>$ vault kv put kv-v2/framsburg/grafana/oidc client-id=\"someID\" client-secret=\"someSecret\"\n</code></pre> <p>Create a policy to access the secrets: <pre><code>$ vault policy write grafana-app - &lt;&lt;EOF\npath \"kv-v2/data/framsburg/grafana/*\" {\n  capabilities = [\"read\", \"list\"]\n}\nEOF\n</code></pre></p> <p>Note</p> <p>Please be aware of the added <code>/data/</code>. This is not a typo but something the Vault expects when referencing this secret. It is not displayed in the UI either.</p> <p>And as last step create a role which maps the k8s service account with the policy:</p> <pre><code>$ vault write auth/kubernetes/role/grafana-app \\\nbound_service_account_names=monitoring-stack-grafana \\\nbound_service_account_namespaces=monitoring-stack \\\npolicies=grafana-app \\\nttl=20m\n</code></pre>"},{"location":"monitoring/#secret-class","title":"Secret Class","text":"<p>Create a SecretProviderClass in the templates</p> cluster-critical/monitoring-stack/templates/spc.yaml<pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\nname: vault-grafana\nspec:\nprovider: vault\nparameters:\nvaultAddress: \"http://vault.vault:8200\"\nroleName: \"grafana-app\"\nobjects: |\n- objectName: \"oidc-id\"\nsecretPath: \"kv-v2/data/framsburg/grafana/oidc\"\nsecretKey: \"client-id\"\n- objectName: \"oidc-secret\"\nsecretPath: \"kv-v2/data/framsburg/grafana/oidc\"\nsecretKey: \"client-secret\"\nsecretObjects:\n- data:\n- key: clientId\nobjectName: oidc-id\n- key: clientSecret\nobjectName: oidc-secret\nsecretName: oidc\ntype: Opaque\n</code></pre>"},{"location":"monitoring/#add-volumes-in-a-chart","title":"Add Volumes in a Chart","text":"<p>The Grafana-Chart doesn't allow CSI volumes to be added to the normal volume list. But it has a special value <code>extraSecretMount</code> for those volumes which thankfully even combines the volume and mount entry into one.</p> cluster-critical/monitoring-stack/values.yaml<pre><code>kube-prometheus-stack:\ngrafana:\n...\nextraSecretMounts:\n- name: 'secrets-store-inline'\nmountPath: '/mnt/secrets-store'\nreadOnly: true\ncsi:\ndriver: secrets-store.csi.k8s.io\nreadOnly: true\nvolumeAttributes:\nsecretProviderClass: vault-grafana\n</code></pre>"},{"location":"monitoring/#set-environment-variables-incl-secrets","title":"Set environment variables incl secrets","text":"cluster-critical/monitoring-stack/values.yaml<pre><code>kube-prometheus-stack:\ngrafana:\n...\nenv:\nGF_AUTH_GENERIC_OAUTH_ENABLED: \"true\"\nGF_AUTH_GENERIC_OAUTH_NAME: \"authentik\"\nGF_AUTH_GENERIC_OAUTH_SCOPES: \"openid profile email\"\nGF_AUTH_GENERIC_OAUTH_AUTH_URL: \"https://authentik.framsburg.ch/application/o/authorize/\"\nGF_AUTH_GENERIC_OAUTH_TOKEN_URL: \"https://authentik.framsburg.ch/application/o/token/\"\nGF_AUTH_GENERIC_OAUTH_API_URL: \"https://authentik.framsburg.ch/application/o/userinfo/\"\nGF_AUTH_SIGNOUT_REDIRECT_URL: \"https://authentik.framsburg.ch/application/o/grafana/end-session/\"\n# Optionally enable auto-login (bypasses Grafana login screen)\n# GF_AUTH_OAUTH_AUTO_LOGIN: \"true\"\n# Optionally map user groups to Grafana roles\n# GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: \"contains(groups[*], 'Grafana Admins') &amp;&amp; 'Admin' || contains(groups[*], 'Grafana Editors') &amp;&amp; 'Editor' || 'Viewer'\"\nenvValueFrom:\nGF_AUTH_GENERIC_OAUTH_CLIENT_ID:\nsecretKeyRef:\nname: oidc\nkey: clientId\nGF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:\nsecretKeyRef:\nname: oidc\nkey: clientSecret\n</code></pre>"},{"location":"network/","title":"Network","text":"<p>The Raspberries are in two networks. The internal network and a special DMZ for the external communication of the K8S Cluster. This is not done by actual multiple network interfaces but with vlan.</p> <p>Just by chance my firewall had a free network interface so I chose to actually use a seperate network interface for the DMZ there. I would have used vlan there as well.</p> <p>I could use pure port-forwarding to the MetalLB-IP of the Cluster.</p>"},{"location":"network/#opnsense","title":"Opnsense","text":"<p>On my Opnsense firewall I have to configure several things:</p> <ol> <li>WAN interface to accept inbound web (443) traffic</li> <li>Add NAT port-forward for the MetalLB-IP</li> </ol>"},{"location":"network/#vpn","title":"VPN","text":"<p>The firewall has OpenVPN installed. The users are configured on the firewall and the authentication uses OTPT.</p> <p>The OTP from the Google Authenticator is entered with the password in the following form: <code>&lt;password&gt;&lt;otp&gt;</code>.</p>"},{"location":"plex/","title":"PLEX","text":""},{"location":"plex/#token","title":"Token","text":"<p>Goto www.plex.tv/claim/</p> <p>Set the env variable <code>PLEX_CLAIM</code> to the generated claim token.</p>"},{"location":"plex/#remote-access","title":"Remote Access","text":"<p>Either add an additional entrypoint or use the custom port 443 for the remote port.</p> <p>It is important to adapt the custom hostname to the correct URL including protocol (<code>https://</code>) and Path Prefix like <code>/web</code>.</p>"},{"location":"plex/#readwritemany-for-storage","title":"ReadWriteMany for storage","text":"<p>To upload a lot of files it might be helpful to use <code>ReadWriteMany</code> volumes. This way the can be exposed for upload over samba, nfs or other setups.</p>"},{"location":"plex/#upload","title":"Upload","text":"<pre><code>kubectl cp &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/foo /tmp/bar\n</code></pre>"},{"location":"plex/#change-video-containers-with-ffmpeg","title":"Change video containers with ffmpeg","text":"<pre><code>ffmpeg -i example.mkv -c copy -tag:v hvc1 example.mp4\n</code></pre> <pre><code>for f in *.mkv; do ffmpeg -i \"$f\" -c copy -tag:v hvc1 \"${f%.mkv}.mp4\"; rm \"$f\"; done\n</code></pre>"},{"location":"postgres/","title":"Database Cluster","text":"<p>The database setup is based on the zalando-operator. With one cluster for all productive applications and one or multipe for test purposes.</p> <p>The productive cluster is created as HA cluster with pg_bouncer but without a standby database.</p>"},{"location":"postgres/#operator-setup","title":"Operator setup","text":""},{"location":"postgres/#backup","title":"Backup","text":"<p>Use own Minio for Backup.</p> <p>Create new Policy with the name <code>SpiloS3Access</code> and the poliy configuration: <pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"s3:*\"\n],\n\"Resource\": [\n\"arn:aws:s3:::postgres-backups\"\n\"arn:aws:s3:::postgres-backups/*\"\n]\n}\n]\n}\n</code></pre></p> <p>Create new identity with <code>postgres-pod-role</code> with password and assign the previously created policy. Store both in the Vault under <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code></p>"},{"location":"postgres/#setup-cluster","title":"Setup Cluster","text":""},{"location":"proxmox-setup/","title":"Setup Proxmox","text":"<p>Proxmox is an OpenSource virtualization software. The largere nodes are setup with it and the nodes on it virtualized.</p>"},{"location":"proxmox-setup/#install-proxmox","title":"Install Proxmox","text":"<p>First create a boot USB stick with Proxmox VE Installer ISO.</p> <p>Select ZFS as filesystem as it will set the bootloader to systemd-boot instead of GRUB.</p>"},{"location":"proxmox-setup/#first-steps-on-a-new-proxmox-server","title":"First steps on a new Proxmox server","text":"<p>After setting up a new Proxmox server, there are a few things to do before they can be used. Idealy those steps would be automated but ...</p> <p>Those steps are heavily inspired by techno tim setup article</p>"},{"location":"proxmox-setup/#updates","title":"Updates","text":"<p>Edit <code>/etc/apt/sources.list</code></p> <pre><code>deb http://ftp.debian.org/debian bullseye main contrib\n\ndeb http://ftp.debian.org/debian bullseye-updates main contrib\n\n# security updates\ndeb http://security.debian.org/debian-security bullseye-security main contrib\n\n# PVE pve-no-subscription repository provided by proxmox.com,\n# NOT recommended for production use\ndeb http://download.proxmox.com/debian/pve bullseye pve-no-subscription\n</code></pre> <p>Edit <code>/etc/apt/sources.list.d/pve-enterprise.list</code></p> <pre><code># deb https://enterprise.proxmox.com/debian/pve buster pve-enterprise\n</code></pre> <p>Run</p> <pre><code>apt-get update\n\napt full-upgrade\n\nreboot\n</code></pre>"},{"location":"proxmox-setup/#storage","title":"Storage","text":"<p>BE CAREFUL. This is meant for the storage/longhorn disks as it will wipe it</p> <p>```shell caption=\"select the correct disk device\" fdisk /dev/sda <pre><code>Then P for partition, then D for delete and W for write.\n\n### IOMMU  (PCI Passthrough)\n\nSee [Proxmox PCI Passthrough](https://pve.proxmox.com/wiki/Pci_passthrough)\n\nFirst make sure IOMMU is enabled in the BIOS.\n\n`nano /etc/kernel/cmdline`\n\nAdd `intel_iommu=on iommu=pt` to the end of this line without line breaks. (for\nAMD processors add `amd_iommu=on ...`)\n\n```shell\nroot=ZFS=rpool/ROOT/pve-1 boot=zfs intel_iommu=on iommu=pt\n</code></pre></p> <p>Edit <code>/etc/modules</code> and add</p> <pre><code>vfio\nvfio_iommu_type1\nvfio_pci\nvfio_virqfd\n</code></pre> <p>run</p> <pre><code>update-initramfs -u -k all\n# or proxmox-boot-tool refresh\nreboot\n</code></pre>"},{"location":"proxmox-setup/#vlan-aware","title":"VLAN Aware","text":"<p>Setup the network interface to handle VLANs.</p> <pre><code>nano /etc/network/interfaces\n</code></pre> <p>Add the following lines to the main network interface</p> <pre><code>bridge-vlan-aware yes\nbridge-vids 2-4094\n</code></pre>"},{"location":"proxmox-setup/#isos","title":"ISOs","text":"<p>Define images:</p> <ul> <li>https://releases.ubuntu.com/22.10/ubuntu-22.10-live-server-amd64.iso</li> </ul>"},{"location":"proxmox-setup/#setup-worker-node","title":"Setup worker node","text":"<p>Depending on the usage the parameters may vary. But following is a node which is part of the longhorn storage and has an interface to the internet.</p> <ul> <li>General:</li> <li>Node: pve1-x</li> <li>Name: k3sworker</li> <li>Start at boot: true</li> <li>OS:</li> <li>ISO Image: ubuntu server installer</li> <li>System:</li> <li>Qemu Agent: true</li> <li>Disks:</li> <li>disk 1:<ul> <li>Storage: virtual</li> <li>Size: 64 GB</li> <li>Backup: yes</li> <li>Skip replication: no</li> <li>Discard: yes</li> <li>SSD emulation: yes</li> </ul> </li> <li>disk 2:<ul> <li>Storage: virtual</li> <li>Size: &gt;512 GB</li> <li>Backup: no</li> <li>Skip replication: yes</li> <li>Discard: yes</li> <li>SSD emulation: yes</li> </ul> </li> <li>CPU:</li> <li>Sockets: 1</li> <li>Cores: 2-12</li> <li>Memory:</li> <li>min 32 GB</li> <li>max 64 GB</li> <li>Network:</li> <li>net0<ul> <li>Bridge: vmbr0</li> </ul> </li> <li>net1<ul> <li>Bridge: vmbr0</li> <li>vlan tag: 99</li> </ul> </li> </ul> <p>Don't forget to define the generated mac address in DHCP</p>"},{"location":"proxmox-setup/#install-qemu-agent","title":"Install Qemu Agent","text":"<pre><code>ssh devops@k3sworker&lt;xy&gt;\nsudo apt-get install qemu-guest-agent\n</code></pre>"},{"location":"proxmox-setup/#references","title":"References","text":"<ul> <li>[CSI]: Container Storage Interface</li> <li>[IOMMU]: Input-Output Memory Management Unit. Used to virualize memory access for devices. See Wikipedia</li> </ul>"},{"location":"rails-app/","title":"Rails app on Kubernetes","text":""},{"location":"rails-app/#reference","title":"Reference","text":"<p>some was copied from https://kubernetes-rails.com</p>"},{"location":"secrets-csi/","title":"Secrets with CSI","text":""},{"location":"secrets-csi/#vault-secret-injection-with-csi","title":"Vault Secret Injection with CSI","text":"<p>One way to use credentials from the vault inside pods is with CSI.</p> <ul> <li>Vault post-start command to enable kubernetes auth-method</li> <li>Use Vault with CSI</li> <li>Install CSI Driver CRD with Chart</li> <li>Define a generic SecretProviderClass template as it is needed for each secret (quite a lot of boilerplate)</li> </ul> <p>In case you need the vault command you can easily log into the shell with:</p> <pre><code>$ kubectl exec -it vault-0 -- /bin/sh\n</code></pre> <p>Create the secrets with:</p> <pre><code>$ vault kv put kv-v2/k8s/framsburg/dex client-id=\"someID\" client-secret=\"someSecret\"\n</code></pre> <p>Enable and activate kubernetes auth method <pre><code>$ vault auth enable kubernetes\n$ vault write auth/kubernetes/config \\\nissuer=\"https://kubernetes.default.svc.cluster.local\" \\\ntoken_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\nkubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\\nkubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n</code></pre></p> <p>Create a policy: <pre><code>$ vault policy write dex-app - &lt;&lt;EOF\npolicy dex-app:\npath \"kv-v2/data/k8s/framsburg/dex\" {\n  capabilities = [\"read\"]\n}\nEOF\n</code></pre></p> <p>Write a role to map a service account with a policy</p> <pre><code>$ vault write auth/kubernetes/role/dex-app \\\nbound_service_account_names=dex \\\nbound_service_account_namespaces=dex \\\npolicies=dex-app \\\nttl=20m\nSuccess! Data written to: auth/kubernetes/role/dex-app\n</code></pre>"},{"location":"secrets-csi/#secret-class","title":"Secret Class","text":"<pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\nname: vault-dex\nspec:\nprovider: vault\nparameters:\nvaultAddress: \"http://vault.vault:8200\"\nroleName: \"dex-app\"\nobjects: |\n- objectName: \"oidc-id\"\nsecretPath: \"kv-v2/data/k8s/framsburg/dex\"\nsecretKey: \"client-id\"\n- objectName: \"oidc-secret\"\nsecretPath: \"kv-v2/data/k8s/framsburg/dex\"\nsecretKey: \"client-secret\"\nsecretObjects:\n- data:\n- key: id\nobjectName: oidc-id\n- key: secret\nobjectName: oidc-secret\nsecretName: oidc\ntype: Opaque\n</code></pre>"},{"location":"secrets-csi/#volumes-in-a-chart","title":"Volumes in a Chart","text":"<pre><code>...\nenv:\n- name: GITHUB_CLIENT_ID\nvalueFrom:\nsecretKeyRef:\nname: oidc\nkey: id\n- name: GITHUB_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: oidc\nkey: secret\nenvFrom:\n- secretRef:\nname: oidc\n...\nvolumeMounts:\n- name: 'secrets-store-inline'\nmountPath: '/mnt/secrets-store'\nreadOnly: true\nvolumes:\n- name: secrets-store-inline\ncsi:\ndriver: secrets-store.csi.k8s.io\nreadOnly: true\nvolumeAttributes:\nsecretProviderClass: vault-dex\n</code></pre>"},{"location":"securitycam/","title":"Security Cam","text":""},{"location":"securitycam/#config-for-pi0w-cam","title":"Config for PI0w Cam","text":"<pre><code>dtparam=audio=on\ndtoverlay=vc4-kms-v3d\nmax_framebuffers=2\ncamera_auto_detect=1\n</code></pre> <p>sudo modprobe bcm2835-v4l2</p>"},{"location":"securitycam/#stream","title":"Stream","text":"<p>motion</p> <p>libcamerify motion</p> <p>sudo service motion status</p> <p>libcamera-vid -n --level 4.2 --denoise cdn_off -t 0 --inline --autofocus-mode continuous  --width 1920 --height 1080 --framerate 30 -o - | cvlc -vvv stream:///dev/stdin :demux=h264 --no-audio --sout '#rtp{sdp=http://:8554/x}'</p> <pre><code>$ libcamera-vid -t 0 -n --inline -o - | gst-launch-1.0 fdsrc fd=0 ! h264parse ! rtph264pay ! udpsink host=overseer port=5000\n</code></pre> <p>with cvlc</p> <pre><code>$ libcamera-vid -t 0 --inline -o - | cvlc stream:///dev/stdin --sout '#rtp{sdp=rtsp://:8554/stream1}' :demux=h264\n</code></pre>"},{"location":"traefik/","title":"Traefik","text":"<p>Use two traefik controllers for internal and external network.</p> <p>Issue with only one service <code>api@internal</code> with will mess up the WebUI: The web ui will display for both controllers all services and routes.</p>"},{"location":"uptime-kuma/","title":"Uptime Kuma","text":""},{"location":"vault/","title":"Hashicorp Vault","text":"<p>For operating the Vault inside K8S it is a good idea to use the Banzaicloud Vault-Operator. It automates some the integration and HA tasks.</p>"},{"location":"vault/#install-the-operator","title":"Install the operator","text":"<p>Define a new Chart with a dependencies to the Vault-Operator Chart in the app of apps for the vault-operator with the following values:</p> <pre><code>vault-operator:\ncrdAnnotations:\n\"helm.sh/hook\": crd-install\n</code></pre>"},{"location":"vault/#install-vault","title":"Install Vault","text":"<p>Define a new empty Chart with the following templates inside:</p> <p>```yaml:rbac.yaml kind: ServiceAccount apiVersion: v1 metadata:   name: vault</p> <p>kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:   name: vault rules:   - apiGroups: [\"\"]     resources: [\"secrets\"]     verbs: [\"*\"]   - apiGroups: [\"\"]     resources: [\"pods\"]     verbs: [\"get\", \"update\", \"patch\"]</p> <p>kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:   name: vault roleRef:   kind: Role   name: vault   apiGroup: rbac.authorization.k8s.io subjects:   - kind: ServiceAccount     name: vault</p>"},{"location":"vault/#this-binding-allows-the-deployed-vault-instance-to-authenticate-clients","title":"This binding allows the deployed Vault instance to authenticate clients","text":""},{"location":"vault/#through-kubernetes-serviceaccounts-if-configured-so","title":"through Kubernetes ServiceAccounts (if configured so).","text":"<p>apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: vault-auth-delegator roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: system:auth-delegator subjects:   - kind: ServiceAccount     name: vault     namespace: default <pre><code>## Use secrets\n\nYou always have to map the secrets in different ways. You can find a detailed\ndescription on [banzais website](https://banzaicloud.com/docs/bank-vaults/mutating-webhook/)\n\n### As envrionment variable\n\nPod should have the following annotations:\n\n```yaml\nannotations:\n  vault.security.banzaicloud.io/vault-addr: \"http://vault.vault.svc:8200\"\n  vault.security.banzaicloud.io/vault-path: \"kubernetes\"\n  vault.security.banzaicloud.io/vault-role: \"test\"\n  vault.security.banzaicloud.io/vault-skip-verify: \"true\"\n</code></pre></p> <p>You should adapt the role to the corresponding role you want to use. You can then use secrets in environment variables like this:</p> <pre><code>env:\n- name: GITHUB_CLIENT_ID\nvalue: vault:secret/data/framsburg/test#github_token\n</code></pre>"},{"location":"vault/#as-secret","title":"As secret","text":"<p>The approach with secrets looks quite similar. The main difference is, that you have to provide the path to the secret base64 encoded.</p> <pre><code>$ echo -n vault:secret/data/framsburg/test#github_token | base64\ndmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu\n</code></pre> <p>Then prepare the secret accodringly:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: some-secret\ndata:\nGITHUB_CLIENT_ID: dmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu\ntype: Opaque\n</code></pre>"},{"location":"vault/#inline","title":"Inline","text":"<p>Instead of environment variables or secrets you can use the vault key reference anywhere in resources and the webhook will replace it with the secret.</p>"},{"location":"vpn/","title":"VPN","text":""},{"location":"vpn/#setup-client","title":"Setup Client","text":"<p>https://docs.opnsense.org/manual/how-tos/sslvpn_client.html#create-a-server-certificate</p>"},{"location":"vpn/#troubleshoot","title":"Troubleshoot","text":""},{"location":"vpn/#renew-openvpn-certificate","title":"Renew OpenVPN Certificate","text":"<p>System &gt;&gt; Trust &gt;&gt; Certificates</p> <ul> <li>Create Certificate</li> <li>Create internal certificate</li> <li>Descriptive name: SSL VPN Server Certificate  <li>CA: SSL VPN CA (newest)</li> <li>Type: Server Certificate</li> <li>Length: 4096</li> <li>Digest: SHA512</li> <li>Common Name: SSL VPN Server Certificate"},{"location":"vpn/#renew-client-certificate","title":"Renew Client Certificate","text":"<p>System &gt;&gt; Trust &gt;&gt; Certificates</p> <ul> <li>Create Certificate</li> <li>Create internal certificate</li> </ul> <p>System &gt;&gt; Access &gt;&gt; Users</p> <ul> <li>Edit user</li> <li>Select new Certificate</li> </ul> <p>VPN &gt;&gt; OpenVPN &gt;&gt; Client Export</p> <ul> <li>Download certifcate for user</li> <li>Use new config in OpenVPN Client</li> </ul>"}]}