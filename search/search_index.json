{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Raspberry Pi GitOps Stack","text":"<p>This document describes my current setup of my Rasperry Pi k8s cluster. Although everything should be reflected in code, usually my brain discards stuff which works ... now. The not-working is a problem for future brain \ud83d\ude04</p> <p>So the text is mainly meant for me to keep track of how and why I did certain things. If somebody else finds value in it: great!</p> <p>Although the whole thing is a private project for educational purposes, I try to keep it as production ready as possible. Often the biggest learnings stem from corner cases.  That means, at least for me, to stay true to the following points:</p> <ul> <li>Everything is automated, no manual kubectl commands</li> <li>Clear separation between public and private network</li> <li>Use secure connections<ul> <li>All HTTPS connections have correct certificates from LetcEncrypt</li> </ul> </li> <li>Disaster recovery is easy to do</li> <li>Critical parts of the system (like control-plane, networking, etc) are setup in HA</li> </ul>"},{"location":"adguard/","title":"AdGuard","text":""},{"location":"adguard/#setup-adguard-container","title":"Setup AdGuard container","text":"<p>Use the AdGuard container from K8S@Home</p>"},{"location":"adguard/#config-in-opnsense","title":"Config in Opnsense","text":"<p>On my Opnsense firewall I have to configure several things:</p> <ol>   </ol>"},{"location":"argocd/","title":"Argo CD","text":""},{"location":"argocd/#applicationset","title":"ApplicationSet","text":""},{"location":"argocd/#ignore-diffs","title":"Ignore Diffs","text":"diff in applicationset<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: cluster-init\n  namespace: argocd-system\nspec:\n  generators:\n    ...\n  template:\n    spec:\n      project: default\n      ignoreDifferences:\n        - group: \"\"\n          kind: ConfigMap\n          jsonPointers:\n            - /data/oidc.config\n        - group: \"\"\n          kind: Secret\n          jsonPointers:\n            - /data/oidc.authentik.clientSecret\n      ...\n</code></pre>"},{"location":"argocd/#part-two-oidc-integration","title":"Part Two: OIDC integration","text":"<p>This part should follow after [Vault] and [Authentik] are up and running.</p>"},{"location":"argocd/#oidc-login","title":"OIDC Login","text":"<p>First you have to create a provider and application in authentik to get a client id and secret. Afterwards the oidc credentials can be saved in the Vault and mapped over a <code>SecretProviderClass</code>. (Do not forget to mount the vault volumes for the secret to work [./secrets-csi.md#volumes-in-a-chart])</p> <pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: vault-argocd\nspec:\n  provider: vault\n  parameters:\n    vaultAddress: \"http://vault.vault:8200\"\n    roleName: \"argocd-app\"\n    objects: |\n      - objectName: \"oidc-id\"\n        secretPath: \"kv-v2/data/framsburg/argocd/oidc\"\n        secretKey: \"client-id\"\n      - objectName: \"oidc-secret\"\n        secretPath: \"kv-v2/data/framsburg/argocd/oidc\"\n        secretKey: \"client-secret\"\n  secretObjects:\n    - data:\n        - key: oidc.authentik.clientId\n          objectName: oidc-id\n        - key: oidc.authentik.clientSecret\n          objectName: oidc-secret\n      secretName: oidc\n      type: Opaque\n      labels:\n        app.kubernetes.io/part-of: argocd # (1)!\n</code></pre> <ol> <li>Without this label the secret reference in the argocd ConfigMap will not work and complain about that the secret key can not be found.</li> </ol> <pre><code>argo-cd:\n  server:\n    ...\n    config:\n      url: https://argocd.framsburg.ch\n      oidc.config: | \n          name: Authentik\n          issuer: \"https://authentik.framsburg.ch/application/o/argocd/\"\n          clientID: \"c579d3195f85aeccaf1ecce35ef5501e023c2a6a\"\n          clientSecret: \"$oidc:oidc.authentik.clientSecret\"\n          requestedScopes: [\"openid\", \"profile\", \"email\"]\n          logoutURL: \"https://authentik.framsburg.ch/if/session-end/argocd/\"\n    rbacConfig:\n      policy.default: role:readonly\n      policy.csv: |\n          g, 'authentik Admins', role:admin\n    volumeMounts:\n      - name: 'secrets-store-inline'\n        mountPath: '/mnt/secrets-store'\n        readOnly: true\n    volumes:\n      - name: secrets-store-inline\n        csi:\n          driver: 'secrets-store.csi.k8s.io'\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: 'vault-argocd'\n</code></pre>"},{"location":"argocd/#rename-applicationset","title":"Rename Applicationset","text":"<p>kubectl delete ApplicationSet (NAME) --cascade=false</p> <p>on new and old applicationset, with identical names!!! .spec.syncPolicy.preserveResourcesOnDeletion</p> <p>--&gt; Warning should occur of applicationsets are part of two applications (it's actually wrong, but apparently applicationsets are identified only by name)</p> <p>Remove old application set -</p> <ul> <li>Add app for new applicationset</li> <li>Add preserve resources in old and new applicationset</li> <li>Remove old applicationset</li> <li>Add app to new applicationset one by one</li> </ul>"},{"location":"authentik/","title":"Authentik","text":""},{"location":"bare-metal/","title":"Harware Setup of Raspberry PIs","text":"<p>The initial setup is done with Ansible.</p>"},{"location":"bare-metal/#setup-minio-bucket-for-backup","title":"Setup Minio Bucket for Backup","text":"Create minio bucket<pre><code>$ mc mb myminio/k3s\n$ mc mb myminio/k3s/etcd-snapshot\n</code></pre> Create user with policy<pre><code>$ mc admin user add myminio k3s k3sk3sk3s\n\n$ cat &gt; /tmp/etcd-backups-policy.json &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n      \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:PutBucketPolicy\",\n        \"s3:GetBucketPolicy\",\n        \"s3:DeleteBucketPolicy\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3s\"\n      ],\n      \"Sid\": \"\"\n    },\n    {\n      \"Action\": [\n        \"s3:AbortMultipartUpload\",\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:PutObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3s/*\"\n      ],\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n\n$ mc admin policy add myminio etcd-backups-policy /tmp/etcd-backups-policy.json\n\n$ mc admin policy set myminio etcd-backups-policy user=k3s\n</code></pre>"},{"location":"bare-metal/#define-k3s-backup-target","title":"Define K3S backup target","text":"k3s-server.services<pre><code>[Service]\nExecStart={{ k3s_binary_path }}/k3s server \\\n...\n{% if backup_s3_enabled %}\n    --etcd-s3 \\\n    --etcd-snapshot-schedule-cron='{{ backup_schedule_cron }}' \\\n    --etcd-s3-endpoint='{{ backup_s3_endpoint }}' \\\n    --etcd-s3-endpoint-ca='{{ systemd_dir }}/k3s-server.service.crt' \\\n    --etcd-s3-bucket='{{ backup_s3_bucket }}' \\\n    --etcd-s3-folder='{{ backup_s3_folder }}' \\\n    --etcd-s3-access-key='{{ backup_s3_access_key }}' \\\n    --etcd-s3-secret-key='{{ backup_s3_secret_key }}' \\\n{% endif %}\n</code></pre> <p>Define in ansible vault <code>ansible-vault edit group_vars/all.yaml</code> the four coordinates: vault<pre><code>backup_s3_access_key: k3s\nbackup_s3_secret_key: k3sk3sk3s\n</code></pre></p> hosts<pre><code>backup_schedule_cron: '0 */6 * * *'\nbackup_s3_bucket: k3s\nbackup_s3_folder: etcd-snapshot\nbackup_s3_endpoint_ca: |\n          -----BEGIN CERTIFICATE-----\n          MIIDgTCCAmmgAwIBAgIJAJ85e+K5ngFRMA0GCSqGSIb3DQEBCwUAMGsxCzAJBgNV\n</code></pre>"},{"location":"bare-metal/#optional-rolling-update","title":"(Optional) Rolling Update","text":"<p>The initial ansible script is not very suitable for rolling updates as it assumes it is about to initialize a cluster which requires the order</p> <ol> <li>First master node which initializes (or restores) the etcd state</li> <li>All other master nodes which sync up to the first</li> <li>All worker nodes</li> </ol> <p>That is very efficient for setup and restore but would mean some outages if applied on a live cluster. Therefore we need a playbook which goes through every node sequentially (we have no special requirement on performance) and cares about draining nodes correctly.</p> <p>Ideally we can reuse roles from the cluster setup playbook.</p>"},{"location":"cert-manager/","title":"Cert-Manager","text":""},{"location":"custom-images/","title":"Custom Docker Images","text":"<p>For the task of creating my own images I got inspired by the setup from k8s-at-home and of it's contributers like onedr0p</p>"},{"location":"custom-images/#testing","title":"Testing","text":"<p>Use GOSS.</p> <p>Use <code>dgoss</code> locally which is just a docker wrapper for goss.</p>"},{"location":"custom-images/#references","title":"References","text":""},{"location":"external-secrets/","title":"External Secrets","text":"<p>After some hickups with the vault-secrets-webhook I want to give external secrets a try. With the webhook I had issues with the env variables as not all Helm Charts made it so easy to use like the kube-monitoring-stack: Grafana was using some complicated mapping setup which made it necessary to use Secrets and <code>secret-refs</code>. And thats where my issues started because the vault-secrets-webhook did not reliably replace the placeholders ....</p>"},{"location":"external-secrets/#install-operator","title":"Install Operator","text":"<p>I created a new Helm Chart App for external secrets</p> Chart.yaml<pre><code>apiVersion: v2\nname: external-secrets\nversion: 0.0.0\ndependencies:\n  - name: external-secrets\n    version: 0.5.9\n    repository: https://charts.external-secrets.io\n</code></pre> values.yaml<pre><code>external-secrets:\n  serviceMonitor:\n    enabled: true\n</code></pre> <p>That will create the very basic setup for External Secrets.</p>"},{"location":"external-secrets/#setup-store","title":"Setup Store","text":"<p>The next and most important part is the Secret Store. This is the actual connection to the vault and has the vault coordinates as well as access keys. To make it an easy setup, this is part of the external secrets application.</p> templates/secret-store.yaml<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: vault-backend\nspec:\n  provider:\n    vault:\n      server: \"http://vault.vault.svc:8200\"\n      path: \"kubernetes\"\n      version: \"v2\"\n      auth:\n        # Authenticate against Vault using a Kubernetes ServiceAccount\n        # token stored in a Secret.\n        # https://www.vaultproject.io/docs/auth/kubernetes\n        kubernetes:\n          # Path where the Kubernetes authentication backend is mounted in Vault\n          mountPath: \"kubernetes\"\n          # A required field containing the Vault Role to assume.\n          role: \"externalSecret\" # (1)\n          # Optional service account field containing the name\n          # of a kubernetes ServiceAccount\n          serviceAccountRef:\n            name: \"external-secrets\"\n</code></pre> <ol> <li>This role name has to be created in the Vault or configured for the operator     to create it.</li> </ol>"},{"location":"external-secrets/#usecreate-external-secret","title":"Use/Create external secret","text":"<p>Last step: create a secret over the external secrets operator. Because the store is clusterwide available, the only missing part is the <code>ExternalSecret</code> definition.</p> <p>As an example, the following is the OIDC client id and secret for grafana. I stored all the authentik OIDC clients in the vault. The external secret manifest resides in the templates folder of the monitoring stack.</p> <p>templates/grafana-oidc-secret.yaml<pre><code>apiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: grafana-oidc-secret\n  namespace: monitoring-stack\nspec:\n  refreshInterval: 1m\n  secretStoreRef:\n    name: vault-backend # (1)\n    kind: ClusterSecretStore\n    namespace: external-secrets\n  target:\n    name: grafana-oidc-secret\n    template:\n      metadata:\n        labels:\n          app.kubernetes.io/part-of: monitoring-stack\n      data:\n        - secretKey: oidc-id\n          remoteRef:\n            key: secret/data/framsburg/grafana/oidc # (2)\n            property: client-id # (3)\n        - secretKey: oidc-secret\n          remoteRef:\n            key: secret/data/framsburg/grafana/oidc\n            property: client-secret\n</code></pre> 1.  Name of the configured Secret Store (Make sure the kind is in sync with your     Secret Store) 2.  Path of the secret in Vault. <code>secret</code> is the name of the secret engine,     <code>data</code> is part of the path although not visible in Vault ... some Hashicorp     Vault shenanigan 3.  Key of the secret</p> <p>The generated secret will look like this:</p>"},{"location":"external-secrets/#reference","title":"Reference","text":""},{"location":"longhorn/","title":"Longhorn","text":""},{"location":"longhorn/#setup-minio-for-backup","title":"Setup Minio for Backup","text":"<p>Use the minio cli <code>mc</code> which has an alias called <code>myminio</code></p> Create minio bucket<pre><code>$ mc mb myminio/k3sbackups\n$ mc mb myminio/k3sbackups/longhorn\n</code></pre> Create user with policy<pre><code>$ mc admin user add myminio longhorn mypass\n$ cat &gt; /tmp/k3s-backups-policy.json &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n      \"Statement\": [\n    {\n      \"Action\": [\n        \"s3:PutBucketPolicy\",\n        \"s3:GetBucketPolicy\",\n        \"s3:DeleteBucketPolicy\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3sbackups\"\n      ],\n      \"Sid\": \"\"\n    },\n    {\n      \"Action\": [\n        \"s3:AbortMultipartUpload\",\n        \"s3:DeleteObject\",\n        \"s3:GetObject\",\n        \"s3:ListMultipartUploadParts\",\n        \"s3:PutObject\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": [\n        \"arn:aws:s3:::k3sbackups/*\"\n      ],\n      \"Sid\": \"\"\n    }\n  ]\n}\nEOF\n\n$ mc admin policy add myminio k3s-backups-policy /tmp/k3s-backups-policy.json\n\n$ mc admin policy set myminio k3s-backups-policy user=longhorn\n</code></pre>"},{"location":"longhorn/#define-backup-target-in-longhorn","title":"Define backup target in longhorn","text":"longhorn values.yaml<pre><code>longhorn:\n  defaultSettings:\n    backupTarget: 's3://k3sbackups@us-east-1/longhorn'\n    backupTargetCredentialSecret: minio-secret\n...\n</code></pre>"},{"location":"metallb/","title":"MetalLB","text":""},{"location":"monitoring/","title":"Monitoring with Prometheus Stack","text":""},{"location":"monitoring/#part-two-oidc-integration","title":"Part Two: OIDC Integration","text":"<p>This part should follow after [Vault] and [Authentik] are up and running.</p>"},{"location":"monitoring/#create-application-in-authentik","title":"Create Application in Authentik","text":"<p>Create a new OIDC provider in authentik with Redirect URIs/Origin pointing to [https://grafana.framsburg.ch]</p> <p>Afterwards create a new application which uses the before created provider. Don't forget to create bindings for uses or groups. You will need the following three informations out of authentik in the next steps.</p> <ul> <li>OpenID Configuration Issuer</li> <li>Client ID</li> <li>Client Secret</li> </ul>"},{"location":"monitoring/#add-secrets-to-vault","title":"Add secrets to vault","text":"<p>To use the vault CLI use the folling command:</p> <pre><code>$ kubectl exec -it vault-0 -n vault -- /bin/sh\n</code></pre> <p>Add the Client ID and the Client Secret as values to the vault:</p> <p>Create the secrets with:</p> <pre><code>$ vault kv put kv-v2/framsburg/grafana/oidc client-id=\"someID\" client-secret=\"someSecret\"\n</code></pre> <p>Create a policy to access the secrets: <pre><code>$ vault policy write grafana-app - &lt;&lt;EOF\npath \"kv-v2/data/framsburg/grafana/*\" {\n  capabilities = [\"read\", \"list\"]\n}\nEOF\n</code></pre></p>  <p>Note</p> <p>Please be aware of the added <code>/data/</code>. This is not a typo but something the Vault expects when referencing this secret. It is not displayed in the UI either.</p>  <p>And as last step create a role which maps the k8s service account with the policy:</p> <pre><code>$ vault write auth/kubernetes/role/grafana-app \\\n    bound_service_account_names=monitoring-stack-grafana \\\n    bound_service_account_namespaces=monitoring-stack \\\n    policies=grafana-app \\\n    ttl=20m\n</code></pre>"},{"location":"monitoring/#secret-class","title":"Secret Class","text":"<p>Create a SecretProviderClass in the templates</p> cluster-critical/monitoring-stack/templates/spc.yaml<pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: vault-grafana\nspec:\n  provider: vault\n  parameters:\n    vaultAddress: \"http://vault.vault:8200\"\n    roleName: \"grafana-app\"\n    objects: |\n      - objectName: \"oidc-id\"\n        secretPath: \"kv-v2/data/framsburg/grafana/oidc\"\n        secretKey: \"client-id\"\n      - objectName: \"oidc-secret\"\n        secretPath: \"kv-v2/data/framsburg/grafana/oidc\"\n        secretKey: \"client-secret\"\n  secretObjects:\n    - data:\n        - key: clientId\n          objectName: oidc-id\n        - key: clientSecret\n          objectName: oidc-secret\n      secretName: oidc\n      type: Opaque\n</code></pre>"},{"location":"monitoring/#add-volumes-in-a-chart","title":"Add Volumes in a Chart","text":"<p>The Grafana-Chart doesn't allow CSI volumes to be added to the normal volume list. But it has a special value <code>extraSecretMount</code> for those volumes which thankfully even combines the volume and mount entry into one.</p> cluster-critical/monitoring-stack/values.yaml<pre><code>kube-prometheus-stack:\n  grafana:\n...\n    extraSecretMounts:\n      - name: 'secrets-store-inline'\n        mountPath: '/mnt/secrets-store'\n        readOnly: true\n        csi:\n          driver: secrets-store.csi.k8s.io\n          readOnly: true\n          volumeAttributes:\n            secretProviderClass: vault-grafana\n</code></pre>"},{"location":"monitoring/#set-environment-variables-incl-secrets","title":"Set environment variables incl secrets","text":"cluster-critical/monitoring-stack/values.yaml<pre><code>kube-prometheus-stack:\n  grafana:\n...\n    env:\n      GF_AUTH_GENERIC_OAUTH_ENABLED: \"true\"\n      GF_AUTH_GENERIC_OAUTH_NAME: \"authentik\"\n      GF_AUTH_GENERIC_OAUTH_SCOPES: \"openid profile email\"\n      GF_AUTH_GENERIC_OAUTH_AUTH_URL: \"https://authentik.framsburg.ch/application/o/authorize/\"\n      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: \"https://authentik.framsburg.ch/application/o/token/\"\n      GF_AUTH_GENERIC_OAUTH_API_URL: \"https://authentik.framsburg.ch/application/o/userinfo/\"\n      GF_AUTH_SIGNOUT_REDIRECT_URL: \"https://authentik.framsburg.ch/application/o/grafana/end-session/\"\n      # Optionally enable auto-login (bypasses Grafana login screen)\n      # GF_AUTH_OAUTH_AUTO_LOGIN: \"true\"\n      # Optionally map user groups to Grafana roles\n      # GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: \"contains(groups[*], 'Grafana Admins') &amp;&amp; 'Admin' || contains(groups[*], 'Grafana Editors') &amp;&amp; 'Editor' || 'Viewer'\"\n\n    envValueFrom:\n      GF_AUTH_GENERIC_OAUTH_CLIENT_ID:\n        secretKeyRef:\n          name: oidc\n          key: clientId\n\n      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:\n        secretKeyRef:\n          name: oidc\n          key: clientSecret\n</code></pre>"},{"location":"network/","title":"Network","text":"<p>The Raspberries are in two networks. The internal network and a special DMZ for the external communication of the K8S Cluster. This is not done by actual multiple network interfaces but with vlan.</p> <p>Just by chance my firewall had a free network interface so I chose to actually use a seperate network interface for the DMZ there. I would have used vlan there as well.</p> <p>I could use pure port-forwarding to the MetalLB-IP of the Cluster.</p>"},{"location":"network/#opnsense","title":"Opnsense","text":"<p>On my Opnsense firewall I have to configure several things:</p> <ol> <li>WAN interface to accept inbound web (443) traffic</li> <li>Add NAT port-forward for the MetalLB-IP</li> </ol>"},{"location":"network/#vpn","title":"VPN","text":"<p>The firewall has OpenVPN installed. The users are configured on the firewall and the authentication uses OTPT.</p> <p>The OTP from the Google Authenticator is entered with the password in the following form: <code>&lt;password&gt;&lt;otp&gt;</code>.</p>"},{"location":"plex/","title":"PLEX","text":""},{"location":"plex/#token","title":"Token","text":"<p>Goto www.plex.tv/claim/</p> <p>Set the env variable <code>PLEX_CLAIM</code> to the generated claim token.</p>"},{"location":"plex/#remote-access","title":"Remote Access","text":"<p>Either add an additional entrypoint or use the custom port 443 for the remote port.</p> <p>It is important to adapt the custom hostname to the correct URL including protocol (<code>https://</code>) and Path Prefix like <code>/web</code>.</p>"},{"location":"plex/#readwritemany-for-storage","title":"ReadWriteMany for storage","text":"<p>To upload a lot of files it might be helpful to use <code>ReadWriteMany</code> volumes. This way the can be exposed for upload over samba, nfs or other setups.</p>"},{"location":"plex/#upload","title":"Upload","text":"<pre><code>kubectl cp &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/foo /tmp/bar\n</code></pre>"},{"location":"plex/#change-video-containers-with-ffmpeg","title":"Change video containers with ffmpeg","text":"<pre><code>ffmpeg -i example.mkv -c copy -tag:v hvc1 example.mp4\n</code></pre> <pre><code>for f in *.mkv; do ffmpeg -i \"$f\" -c copy -tag:v hvc1 \"${f%.mkv}.mp4\"; rm \"$f\"; done\n</code></pre>"},{"location":"rails-app/","title":"Rails app on Kubernetes","text":""},{"location":"rails-app/#reference","title":"Reference","text":"<p>some was copied from https://kubernetes-rails.com</p>"},{"location":"secrets-csi/","title":"Secrets with CSI","text":""},{"location":"secrets-csi/#vault-secret-injection-with-csi","title":"Vault Secret Injection with CSI","text":"<p>One way to use credentials from the vault inside pods is with CSI.</p> <ul> <li>Vault post-start command to enable kubernetes auth-method</li> <li>Use Vault with CSI</li> <li>Install CSI Driver CRD with Chart</li> <li>Define a generic SecretProviderClass template as it is needed for each secret (quite a lot of boilerplate)</li> </ul> <p>In case you need the vault command you can easily log into the shell with:</p> <pre><code>$ kubectl exec -it vault-0 -- /bin/sh\n</code></pre> <p>Create the secrets with:</p> <pre><code>$ vault kv put kv-v2/k8s/framsburg/dex client-id=\"someID\" client-secret=\"someSecret\"\n</code></pre> <p>Enable and activate kubernetes auth method <pre><code>$ vault auth enable kubernetes\n$ vault write auth/kubernetes/config \\\n    issuer=\"https://kubernetes.default.svc.cluster.local\" \\\n    token_reviewer_jwt=\"$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n    kubernetes_host=\"https://$KUBERNETES_PORT_443_TCP_ADDR:443\" \\\n    kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n</code></pre></p> <p>Create a policy: <pre><code>$ vault policy write dex-app - &lt;&lt;EOF\npolicy dex-app:\npath \"kv-v2/data/k8s/framsburg/dex\" {\n  capabilities = [\"read\"]\n}\nEOF\n</code></pre></p> <p>Write a role to map a service account with a policy</p> <pre><code>$ vault write auth/kubernetes/role/dex-app \\\n    bound_service_account_names=dex \\\n    bound_service_account_namespaces=dex \\\n    policies=dex-app \\\n    ttl=20m\nSuccess! Data written to: auth/kubernetes/role/dex-app\n</code></pre>"},{"location":"secrets-csi/#secret-class","title":"Secret Class","text":"<pre><code>---\napiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: vault-dex\nspec:\n  provider: vault\n  parameters:\n    vaultAddress: \"http://vault.vault:8200\"\n    roleName: \"dex-app\"\n    objects: |\n      - objectName: \"oidc-id\"\n        secretPath: \"kv-v2/data/k8s/framsburg/dex\"\n        secretKey: \"client-id\"\n      - objectName: \"oidc-secret\"\n        secretPath: \"kv-v2/data/k8s/framsburg/dex\"\n        secretKey: \"client-secret\"\n  secretObjects:\n    - data:\n        - key: id\n          objectName: oidc-id\n        - key: secret\n          objectName: oidc-secret\n      secretName: oidc\n      type: Opaque\n</code></pre>"},{"location":"secrets-csi/#volumes-in-a-chart","title":"Volumes in a Chart","text":"<pre><code>...\n  env:\n    - name: GITHUB_CLIENT_ID\n      valueFrom:\n        secretKeyRef:\n          name: oidc\n          key: id\n    - name: GITHUB_CLIENT_SECRET\n      valueFrom:\n        secretKeyRef:\n          name: oidc\n          key: secret\n  envFrom:\n    - secretRef:\n        name: oidc\n...\n  volumeMounts:\n    - name: 'secrets-store-inline'\n      mountPath: '/mnt/secrets-store'\n      readOnly: true\n  volumes:\n    - name: secrets-store-inline\n      csi:\n        driver: secrets-store.csi.k8s.io\n        readOnly: true\n        volumeAttributes:\n          secretProviderClass: vault-dex\n</code></pre>"},{"location":"traefik/","title":"Traefik","text":"<p>Use two traefik controllers for internal and external network.</p> <p>Issue with only one service <code>api@internal</code> with will mess up the WebUI: The web ui will display for both controllers all services and routes.</p>"},{"location":"uptime-kuma/","title":"Uptime Kuma","text":""},{"location":"vault/","title":"Hashicorp Vault","text":"<p>For operating the Vault inside K8S it is a good idea to use the Banzaicloud Vault-Operator. It automates some the integration and HA tasks.</p>"},{"location":"vault/#install-the-operator","title":"Install the operator","text":"<p>Define a new Chart with a dependencies to the Vault-Operator Chart in the app of apps for the vault-operator with the following values:</p> <pre><code>vault-operator:\n  crdAnnotations:\n      \"helm.sh/hook\": crd-install\n</code></pre>"},{"location":"vault/#install-vault","title":"Install Vault","text":"<p>Define a new empty Chart with the following templates inside:</p> <p>```yaml:rbac.yaml kind: ServiceAccount apiVersion: v1 metadata:   name: vault</p>  <p>kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:   name: vault rules:   - apiGroups: [\"\"]     resources: [\"secrets\"]     verbs: [\"*\"]   - apiGroups: [\"\"]     resources: [\"pods\"]     verbs: [\"get\", \"update\", \"patch\"]</p>  <p>kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:   name: vault roleRef:   kind: Role   name: vault   apiGroup: rbac.authorization.k8s.io subjects:   - kind: ServiceAccount     name: vault</p>"},{"location":"vault/#this-binding-allows-the-deployed-vault-instance-to-authenticate-clients","title":"This binding allows the deployed Vault instance to authenticate clients","text":""},{"location":"vault/#through-kubernetes-serviceaccounts-if-configured-so","title":"through Kubernetes ServiceAccounts (if configured so).","text":"<p>apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:   name: vault-auth-delegator roleRef:   apiGroup: rbac.authorization.k8s.io   kind: ClusterRole   name: system:auth-delegator subjects:   - kind: ServiceAccount     name: vault     namespace: default <pre><code>## Use secrets\n\nYou always have to map the secrets in different ways. You can find a detailed\ndescription on [banzais website](https://banzaicloud.com/docs/bank-vaults/mutating-webhook/)\n\n### As envrionment variable\n\nPod should have the following annotations:\n\n```yaml\nannotations:\n  vault.security.banzaicloud.io/vault-addr: \"http://vault.vault.svc:8200\"\n  vault.security.banzaicloud.io/vault-path: \"kubernetes\"\n  vault.security.banzaicloud.io/vault-role: \"test\"\n  vault.security.banzaicloud.io/vault-skip-verify: \"true\"\n</code></pre></p> <p>You should adapt the role to the corresponding role you want to use. You can then use secrets in environment variables like this:</p> <pre><code>env:\n  - name: GITHUB_CLIENT_ID\n    value: vault:secret/data/framsburg/test#github_token\n</code></pre>"},{"location":"vault/#as-secret","title":"As secret","text":"<p>The approach with secrets looks quite similar. The main difference is, that you have to provide the path to the secret base64 encoded.</p> <pre><code>$ echo -n vault:secret/data/framsburg/test#github_token | base64\ndmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu\n</code></pre> <p>Then prepare the secret accodringly:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: some-secret\ndata:\n  GITHUB_CLIENT_ID: dmF1bHQ6c2VjcmV0L2RhdGEvZnJhbXNidXJnL3Rlc3QjZ2l0aHViX3Rva2Vu\ntype: Opaque\n</code></pre>"},{"location":"vault/#inline","title":"Inline","text":"<p>Instead of environment variables or secrets you can use the vault key reference anywhere in resources and the webhook will replace it with the secret.</p>"}]}